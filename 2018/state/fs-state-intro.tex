%%% fs-state-intro - Introduction

\label {fs-intro-seciton}

% Currently, distributed stream processing is a hot area of research that has a lot of practical applications in IoT, short-term personalization, fraud detection, etc. Unlike batch processing systems like Google's MapReduce~\cite{Dean:2008:MSD:1327452.1327492}, Apache Hadoop~\cite{hadoop2009hadoop}, and Apache Spark~\cite{Zaharia:2016:ASU:3013530.2934664}, streaming systems provide for low latency, early termination, retrieval of approximate results, etc~\cite{Doulkeridis:2014:SLA:2628707.2628782}. Despite the fact that stream processing is mostly used in the fields, where interactive or near real-time processing is required, popular kappa architecture pattern postulates that stream processing fits for any data processing scenario~\cite{lin2017lambda}.

% However, modern stream processing is hardly can be claimed as silver bullet. Most of batch processing systems assure consistent and deterministic results even in case of failures. In state-of-the-art stream processing systems~\cite{carbone2015apache, apache:storm, Zaharia:2016:ASU:3013530.2934664} a contract with end-user regarding ~{\em which data} will be eventually processed and released in case of failures is usually described in terms of so-called~{\em delivery guarantees}. These guarantees include {\em at most once}, {\em at least once}, and {\em exactly once}. Unfortunately, these notions are rather intuitive than formal. {\it At most once} states that each input event is processed once or not processed at all. {\it At least once} guarantees that each input item is processed, but possibly multiple times. {\it Exactly once} assures that each input event is processed exactly one time. Only exactly once guarantees the same level of data consistency as batch processing. These definitions are seemingly simple, but have several main pitfalls:

% \begin{itemize}
%     \item Any stream processing system has a {\em state}. Each output item depends not only on the corresponding input item but also on the system state. This fact implies that a system that {\em technically} supports at most once or exactly once delivery guarantee, in practice can release completely invalid results, because of inconsistencies in the state.
%     \item Multiple elements can be generated from one within streaming system. If these elements are applied to state or released non-atomically, it can lead to inconsistencies as well.
% \end{itemize}

% State-of-the-art stream processing systems avoid these types of inconsistencies using complex fault tolerant mechanisms. These mechanisms have high overhead and reduce the applicability of stream processing systems. For example, {\it micro-batching} technique, that is used in Spark Streaming~\cite{Zaharia:2012:DSE:2342763.2342773} and in Storm Trident~\cite{apache:storm:trident}, inherits strong consistency properties from batch systems. More precisely, input items are collected or divided at system entry into small static data blocks called {\it micro-batches}, and then these blocks are processed as ordinary batches. It is possible to achieve the latency of several seconds using this approach~\cite{7530084, 7474816}, but it is still too high for many streaming applications. Another example is Apache Flink~\cite{Carbone:2017:SMA:3137765.3137777}, that is able to provide latency in terms of milliseconds for at least once. However, if exactly once guarantee enabled, the latency in the worst case cannot be lower than the duration of the global state snapshotting. Therefore, in terms of streaming systems, delivery guarantees are not only about delivery but is also about {\em consistency}. We believe that such meaning is much more reasonable, and further in this paper, we will call them {\em consistency guarantees}.

% The main idea of these methods is to ensure that an input element together with all elements generated from it are applied to the system state and outputted atomically.
%  The lack of formalization of streaming consistency guarantees is constantly causing debates and misunderstandings~\cite{JerryPengStreamIO, PaperTrail}. Furthermore, informal definitions do not give a key why modern stream processing systems experience difficulties with providing of consistency guarantees.

% In this paper, we aim at three goals. The first one is to formalize stream processing consistency guarantees. The second is to identify the way to reduce overhead using obtained formalization. The third one is to show the practical feasibility of cheap guarantees within real-life problem. Within the formal model, it is shown that consistency guarantees  We recognize that the main reason, why the overhead for exactly-once is so huge in state-of-the-art systems, is a need for atomically taking state snapshot and releasing corresponding output elements. On the other hand, we show that the property of {\em determinism} can relax this requirement. 

Distributed batch processing systems, such as Google's MapReduce~\cite{Dean:2008:MSD:1327452.1327492}, Apache Hadoop~\cite{hadoop2009hadoop}, and Apache Spark~\cite{Zaharia:2016:ASU:3013530.2934664}, address a need to process vast amounts of data (e.g., Internet-scale). The fundamental idea behind them is to independently process large data blocks that are collected from static datasets. These systems are able to run in a massively parallel fashion on clusters consisting of thousands of commodity computational units. The main advantages of this approach are consistency, fault-recovery, and practically unlimited scalability~\cite{borthakur2011apache}.

However, there are a lot of scenarios where data is most valuable at its time of arrival, for example, IoT, news processing, financial analysis, anti-fraud, network monitoring, etc. Such problems cannot be directly addressed by classical MapReduce~\cite{Doulkeridis:2014:SLA:2628707.2628782}. State-of-the-art stream processing systems, such as Flink \cite{carbone2015apache}, Samza \cite{Noghabi:2017:SSS:3137765.3137770}, Storm \cite{apache:storm}, Spark Streaming~\cite{Zaharia:2012:DSE:2342763.2342773}, aim at filling this gap by introducing another computational model. According to this model, computational units receive a record or set of records, update internal state if any, and send out new records. This technique allows releasing results before all computations are completed and achieving significantly lower latency than batching. 

One of the main difficulties, that stream processing systems experience, is providing guarantees on data. A contract with end-user regarding ~{\em which data} will be eventually processed and released in case of failures is usually described in terms of so-called~{\em delivery guarantees}. They include {\em at most once}, {\em at least once}, and {\em exactly once}. {\it At most once} states that each input event is processed once or not processed at all. {\it At least once} guarantees that each input item is processed, but possibly multiple times. {\it Exactly once} assures that each input event is processed exactly one time. These notions are seemingly simple, but have important pitfalls:

\begin{itemize}
    \item The lack of formalization is constantly causing debates and misunderstandings~\cite{JerryPengStreamIO, PaperTrail}. It is often unclear which guarantees are {\em exactly} provided by a stream processing system, especially for inexperienced users. The term {\em delivery} also raises misleadings.  
    \item Although, exactly once behavior is natural for batch processing systems, it is challenging for streaming systems. For instance, Samza and Storm do not support such a guarantee at all. Micro-batching approaches like Storm Trident~\cite{apache:storm:trident} and Spark Streaming~\cite{Zaharia:2012:DSE:2342763.2342773} provide quite high latency, about a few seconds~\cite{7530084, 7474816}. Flink can achieve latency in terms of milliseconds for at least once guarantee but requires a protocol based on distributed transactions for exactly once, that leads to significant latency increasing. Such a difference in latency between distinct types of guarantees is commonly not obvious and reduce the applicability of stream processing systems.
\end{itemize}

A tricky thing in achieving delivery guarantees is that the output item depends not only on the corresponding input item but also on the system state. This fact implies that a system can {\em technically} support exactly once delivery guarantee, but in practice can release completely invalid results, because of inconsistencies in the state or in-flight elements. Another vague point is the possibility of the generation of multiple elements from one inside a system. If these elements are applied to state or released non-atomically, it can lead to inconsistencies as well. One other issue is the fact that stream processing systems are usually non-deterministic and can produce other results in case of failure and recovery. 

In this work, we formalize the definitions of delivery guarantees in the form as they are implemented in state-of-the-art stream processing systems. It is shown that delivery guarantees are not only about delivery but is also about {\em consistency}. We believe that such meaning is much more reasonable, and further in this paper, we will call them {\em consistency guarantees} as well. Using our formal model, we demonstrate that the main reason of high overhead on exactly once is the lack of determinism. State-of-the-art stream processing systems handle the absence of determinism, while, to the best of our knowledge, there are no pure streaming solutions, which try to implement consistency guarantees on top of the deterministic engine.

In order to prove the feasibility of cheap exactly once, we design fault tolerance protocols on top of {\em drifting state} model~\cite{we2018adbis} that provides for determinism with almost no overhead. The key idea of this model is to technically make the user state an ordinary streaming element. We also show that such state model allows us to take snapshots without special preparations. Eventually, it is demonstrated that the proposed solution can significantly outperform state-of-the-art stream processing system in terms of latency.

Therefore, the contributions of this paper are the following: 
\begin{itemize}
    \item Formalization of streaming consistency guarantees 
    \item Identification of the key reasons behind high overhead on exactly once in modern solutions 
    \item Proposals on how to make exactly once cheaper
    \item Demonstration of practical feasibility of the proposed approaches
\end{itemize}

The rest of the paper is structured as follows: 

% However, there are a lot of scenarios where data is most valuable at its time of arrival, for example, news processing, financial analysis, anti-fraud, network monitoring, etc. Such problems cannot be directly addressed by classical MapReduce~\cite{Doulkeridis:2014:SLA:2628707.2628782}. Therefore, there is a need for systems that provide low latency between event occurrence and its processing under a fixed load. {\it Micro-batching} technique, that is used in Spark Streaming~\cite{Zaharia:2012:DSE:2342763.2342773} and in Storm Trident~\cite{apache:storm:trident}, inherits strong consistency properties from batch systems. More precisely, input items are collected or divided at system entry into small static data blocks called {\it micro-batches}, and then these blocks are processed as ordinary batches. It is possible to achieve the latency of several seconds using this approach~\cite{7530084, 7474816}, but it is still too high for many applications.

% State-of-the-art industrial distributed stream processing systems, such as Flink \cite{carbone2015apache}, Samza \cite{Noghabi:2017:SSS:3137765.3137770}, Storm \cite{apache:storm}, address this issue by providing {\it record-at-a-time} processing model. According to this model, computational units receive each record, update internal state if any, and send out new records. This technique avoids buffering before processing and allows achieving significantly lower latency than micro-batching~\cite{7530084}.

% The main difficulty, that stream processing systems with record-at-a-time model experience, is providing strong consistency. Regarding stream processing, the strongest consistency semantics is {\it exactly once}. Exactly once semantics requires that every input data item is processed only once in spite of the failures. Although, such behavior is natural for batch processing systems, it is challenging for streaming systems. For instance, Samza and Storm do not support such guarantee at all, while Flink requires a protocol based on distributed transactions, that can lead to latency increasing.

% The key problem here is that unlike batch processing, distributed stream processing is inherently non-deterministic. In particular, there is no guarantee that the messages will be processed in the same order between any two runs. Therefore, it can be unclear how to determine which elements have been already released from the system after recovery to avoid duplicates. Finding the right resuming input point can be tricky as well. Additionally, if the states of operations are replicated, there is a need to determine that they are consistent at the moment of taking the snapshot. 

% In this paper, we propose a technique to provide consistency guarantees for~\FlameStream\ (the name is changed due to ongoing blind review) - stream processing model that hides non-deterministic nature of stream processing by strong ordering assumptions. Out-of-order items are handled in an optimistic manner to avoid extra buffering~\cite{hiddenSeim}. Invalid items that can be generated by the optimistic approach are filtered out before they leave the system. The evaluations demonstrated that this approach has low overhead and can outperform alternative industrial solution under moderate load conditions. While opportunities for achieving exactly-once semantics using the proposed model are briefly discussed in~\cite{hiddenBeyondMR}, in this paper, we concentrate on the exact protocols and performance evaluations.

% The order between data items is provided by meta-information that is assigned to each item. Because processing model is deterministic, meta-information allows us to uniquely identify which data items have already left the system, from which point computations should start after recovery, and which version of the operations' state should be loaded. Furthermore, the properties of the proposed model allow the process of releasing output items to be independent of other processes. These features make exactly once semantics low-cost in terms of performance, that is shown in our experiments. 

% Therefore, the contributions of this paper are the following: 
% \begin{itemize}
%     \item Definition of deterministic stateful streaming computational model 
%     \item Description of method for providing strong consistency properties within proposed model 
%     \item Demonstration the performance competitiveness of this technique
% \end{itemize}

% The rest of the paper is structured as follows: in section~\ref{fs-related-seciton} we consider relevant research papers and industrial projects, low-overhead deterministic model and its implementation are detailed in~\ref{fs-model-section}, in section~\ref{fs-consistency-seciton} we define lightweight protocols for exactly-once, performance comparison against state-of-the-art industrial solution is demonstrated in~\ref{fs-experiments-seciton}, and conclusions and future plans are discussed in~\ref{fs-conclusion-seciton}.

