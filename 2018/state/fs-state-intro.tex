%%% fs-state-intro - Introduction

\label {fs-intro-seciton}

Distributed batch processing systems, such as Google's MapReduce~\cite{Dean:2008:MSD:1327452.1327492}, Apache Hadoop~\cite{hadoop2009hadoop}, and Apache Spark~\cite{Zaharia:2016:ASU:3013530.2934664}, address a need to process vast amounts of data (e.g., Internet-scale). The fundamental idea behind them is to independently process large data blocks that are collected from static datasets. These engines are able to run in a massively parallel fashion on clusters consisting of thousands of commodity computational units. The main advantages of these systems are consistency, fault tolerance, and scalability~\cite{borthakur2011apache}.

However, there are a plenty of scenarios where results are most valuable at the time of data arrival, for example, IoT, news processing, financial analysis, anti-fraud, network monitoring, etc. Such problems cannot be directly addressed by classical MapReduce~\cite{Doulkeridis:2014:SLA:2628707.2628782}. State-of-the-art stream processing systems, such as Flink \cite{carbone2015apache}, Samza \cite{Noghabi:2017:SSS:3137765.3137770}, Storm \cite{apache:storm}, Spark Streaming~\cite{Zaharia:2012:DSE:2342763.2342773}, aim at filling this gap by introducing another computational model. According to this model, a system receives a record or a set of records, update internal state if any, and send out new records. 

One of the most challenging task for streaming systems is to provide guarantees on data processing. Unlike batch processing, streaming systems must release output elements before processing has finished, because input data is assumed to be unbounded. This requirement makes failure recovery mechanisms more complex. Streaming systems face a need to recover computations consistently with previous input data, the current system state, and with the elements which have been already delivered to end-user. 

A contract regarding {\em which data} will be eventually processed and released in case of failures is usually described in terms of so-called {\em delivery guarantees}. They include {\em at most once}, {\em at least once}, and {\em exactly once}. Exactly once is the strongest and the most valuable guarantee from the user perspective because it mitigates efforts on output consistency enforcement. These notions are seemingly simple but have important difficulties. Typically, an output item depends not only on the corresponding input item but also on the system state. This fact implies that a system can process each element exactly once, but in practice can release completely invalid results, because of inconsistencies in the state or in-flight elements:

\begin{itemize}
    \item A simple issue arises when a system just restores invalid state after a failure. For instance, if the state is lost or corrupted while computing the sum of all input elements, a system may continue to process input with exactly-once delivery, but the resulted sum becomes wrong. 
    \item Much more sophisticated problem can be faced if a data flow contains a non-commutative operation. As an example, let us consider a data flow with an operation that concatenates input strings and delivers the result of each iteration. After a failure, the system must restore its state, in this case, a concatenation of strings. A popular approach to restoring state is to replay missing input elements. However, these elements can be reordered in an asynchronous distributed environment. This behavior may lead to the concatenation that is affected by several input elements exactly once but is inconsistent with elements that have been released before the failure.    
\end{itemize}

These examples illustrate that naive definitions of delivery guarantees are not sufficient to provide output consistency. However, most existing implementations prevent anomalies illustrated by the examples above. Flink ensures atomicity between state updates and output elements delivery using a protocol based on distributed transactions. This protocol prevents inconsistencies but leads to a significant increase of latency. Google MillWheel~\cite{Akidau:2013:MFS:2536222.2536229} enforces consistency between state and output elements by writing results of each operation to persistent external storage. The lower bound of latency, in this case, is a duration of all external writes within routes of an input element and its descendants. Micro-batching engines like Storm Trident~\cite{apache:storm:trident} and Spark Streaming~\cite{Zaharia:2012:DSE:2342763.2342773} process data in small-sized blocks. Each block is atomically processed on each stage of a data flow that gives similar to batch processing properties. The main downside of this approach is high latency, about a few seconds~\cite{7530084, 7474816}.

The huge gap between the notion of exactly once and the properties of its implementations indicates the lack of formalization. Misunderstandings of streaming delivery guarantees frequently cause debates and discussions~\cite{JerryPengStreamIO, PaperTrail}. Without a formal model, it is hard to observe similarities and distinctions between existing solutions and to recognize their limitations.

% different - погрепать

Another property that is inherent for batch processing systems, but is hard to achieve in streaming engines is {\em determinism}. Determinism means that each run of the system on the same data produces the same results. Determinism is commonly considered as a challenging task~\cite{Zacheilas:2017:MDS:3093742.3093921}. On the other hand, this property is desirable, because it implies reproducibility and predictability. Intuitively, determinism is connected with consistency~\cite{Stonebraker:2005:RRS:1107499.1107504}, but, to the best of our knowledge, this relation has not been deeply investigated. 

In this work, we introduce a formal model of stream processing that captures delivery guarantees existing in most state-of-the-art systems. We show that the concepts of delivery guarantees are closely related to output data {\em consistency}. We demonstrate that determinism is tightly connected with both delivery and consistency. 

An important consequence of our formal framework is that determinism mitigates a need for synchronization between state updates and output elements delivery for exactly once. One reason behind it is that a deterministic system produces the same results on the same input data. Hence, in a deterministic system, a state of a non-commutative operation can be reprocessed consistently with the previous output elements. This property opens a wide range for performance optimizations.

In order to prove the feasibility of efficient exactly once over determinism, we design fault tolerance protocols on top of {\em drifting state} model~\cite{we2018adbis}. Unlike naive expectations, this optimistic technique provides determinism with low overhead. We show that lightweight determinism together with the results of the formal inference allows achieving exactly once with almost no extra cost. It is verified by the experiments on a realistic problem.

The contributions of this paper are the following: 
\begin{itemize}
    \item Formalization of streaming delivery guarantees 
    \item Demonstration that the property of determinism is tightly related to exactly once
    \item Techniques for lightweight implementation of exactly once guarantee on top of the deterministic engine
    \item Study of practical feasibility of the proposed approaches
\end{itemize}

The rest of the paper is structured as follows: section~\ref{fs-preliminaries} recalls basic concepts of stream processing, we introduce our formal framework in section~\ref{fs-formalism}, lightweight deterministic model is explained in section~\ref{fs-model-section}, implementation details of exactly once over determinism are mentioned in section~\ref{fs-consistency-section}, experiments that demonstrate feasibility of the proposed concept are detailed in section~\ref{fs-experiments-seciton}, and we discuss prior works on the topic in section~\ref{fs-related-seciton}. 