%%% fs-state-intro - Introduction

\label {fs-intro-seciton}

Distributed batch processing systems, such as Google's MapReduce~\cite{Dean:2008:MSD:1327452.1327492}, Apache Hadoop~\cite{hadoop2009hadoop}, and Apache Spark~\cite{Zaharia:2016:ASU:3013530.2934664}, address a need to process vast amounts of data (e.g., Internet-scale). The fundamental idea behind them is to independently process large data blocks that are collected from static datasets. These systems are able to run in a massively parallel fashion on clusters consisting of thousands of commodity computational units. The main advantages of this approach are consistency, fault-recovery, and practically unlimited scalability~\cite{borthakur2011apache}.

However, there are a lot of scenarios where data is most valuable at its time of arrival, for example, news processing, financial analysis, anti-fraud, network monitoring, etc. Such problems cannot be directly addressed by classical MapReduce~\cite{Doulkeridis:2014:SLA:2628707.2628782}. Therefore, there is a need for systems that provide low latency between event occurrence and its processing under a fixed load. {\it Micro-batching} technique, that is used in Spark Streaming~\cite{Zaharia:2012:DSE:2342763.2342773} and in Storm Trident~\cite{apache:storm:trident}, inherits strong consistency properties from batch systems. More precisely, input items are collected or divided at system entry into small static data blocks called {\it micro-batches}, and then these blocks are processed as ordinary batches. It is possible to achieve the latency of several seconds using this approach~\cite{7530084, 7474816}, but it is still too high for many applications.

State-of-the-art industrial distributed stream processing systems, such as Flink \cite{carbone2015apache}, Samza \cite{Noghabi:2017:SSS:3137765.3137770}, Storm \cite{apache:storm}, address this issue by providing {\it record-at-a-time} processing model. According to this model, computational units receive each record, update internal state if any, and send out new records. This technique avoids buffering before processing and allows achieving significantly lower latency than micro-batching~\cite{7530084}.

The main difficulty, that stream processing systems with record-at-a-time model experience, is providing strong consistency. Regarding stream processing, the strongest consistency semantics is {\it exactly once}. Exactly once semantics requires that every input data item is processed only once in spite of the failures. Although, such behavior is natural for batch processing systems, it is challenging for streaming systems. For instance, Samza and Storm do not support such guarantee at all, while Flink requires a protocol based on distributed transactions, that can lead to latency increasing.

The key problem here is that unlike batch processing, distributed stream processing is inherently non-deterministic. In particular, there is no guarantee that the messages will be processed in the same order between any two runs. Therefore, it can be unclear how to determine which elements have been already released from the system after recovery to avoid duplicates. Finding the right resuming input point can be tricky as well. Additionally, if the states of operations are replicated, there is a need to determine that they are consistent at the moment of taking the snapshot. 

In this paper, we propose a technique to provide consistency guarantees for~\FlameStream\ (the name is changed due to ongoing blind review) - stream processing model that hides non-deterministic nature of stream processing by strong ordering assumptions. Out-of-order items are handled in an optimistic manner to avoid extra buffering~\cite{hiddenSeim}. Invalid items that can be generated by the optimistic approach are filtered out before they leave the system. The evaluations demonstrated that this approach has low overhead and can outperform alternative industrial solution under moderate load conditions. While opportunities for achieving exactly-once semantics using the proposed model are briefly discussed in~\cite{hiddenBeyondMR}, in this paper, we concentrate on the exact protocols and performance evaluations.

The order between data items is provided by meta-information that is assigned to each item. Because processing model is deterministic, meta-information allows us to uniquely identify which data items have already left the system, from which point computations should start after recovery, and which version of the operations' state should be loaded. Furthermore, the properties of the proposed model allow the process of releasing output items to be independent of other processes. These features make exactly once semantics low-cost in terms of performance, that is shown in our experiments. 

Therefore, the contributions of this paper are the following: 
\begin{itemize}
    \item Definition of deterministic stateful streaming computational model 
    \item Description of method for providing strong consistency properties within proposed model 
    \item Demonstration the performance competitiveness of this technique
\end{itemize}

The rest of the paper is structured as follows: in section~\ref{fs-related-seciton} we consider relevant research papers and industrial projects, low-overhead deterministic model and its implementation are detailed in~\ref{fs-model-section}, in section~\ref{fs-consistency-seciton} we define lightweight protocols for exactly-once, performance comparison against state-of-the-art industrial solution is demonstrated in~\ref{fs-experiments-seciton}, and conclusions and future plans are discussed in~\ref{fs-conclusion-seciton}.

