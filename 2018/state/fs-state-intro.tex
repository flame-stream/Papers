%%% fs-state-intro - Introduction

\label {fs-intro-seciton}

Distributed batch processing systems, such as Google's MapReduce~\cite{Dean:2008:MSD:1327452.1327492}, Apache Hadoop~\cite{hadoop2009hadoop}, and Apache Spark~\cite{Zaharia:2016:ASU:3013530.2934664}, address a need to process vast amounts of data (e.g., Internet-scale). The fundamental idea behind them is to independently process large data blocks that are collected from static datasets. These systems are able to run in a massively parallel fashion on clusters consisting of thousands of commodity computational units. The main advantages of these techniques are consistency, fault-recovery, and practically unlimited scalability~\cite{borthakur2011apache}.

However, there are a lot of scenarios where data is most valuable at its time of arrival, for example, IoT, news processing, financial analysis, anti-fraud, network monitoring, etc. Such problems cannot be directly addressed by classical MapReduce~\cite{Doulkeridis:2014:SLA:2628707.2628782}. State-of-the-art stream processing systems, such as Flink \cite{carbone2015apache}, Samza \cite{Noghabi:2017:SSS:3137765.3137770}, Storm \cite{apache:storm}, Spark Streaming~\cite{Zaharia:2012:DSE:2342763.2342773}, aim at filling this gap by introducing another computational model. According to this model, computational units receive a record or set of records, update internal state if any, and send out new records. This technique allows end user to obtain results before all computations are completed and achieving significantly lower latency than batch processing. 

One of the main difficulties, that stream processing systems experience, is providing guarantees on data quality. 
% Не в этом дело
Unlike batch systems, streaming ones cannot restart processing from the very beginning after recovery because input data is assumed to be infinite. A contract with end-user regarding {\em which data} will be eventually processed and released in case of failures is usually described in terms of so-called {\em delivery guarantees}. They include {\em at most once}, {\em at least once}, and {\em exactly once}. {\it At most once} states that each input event is processed once or not processed at all. {\it At least once} guarantees that each input item is processed, but possibly multiple times. {\it Exactly once} assures that each input event is processed exactly one time. These notions are seemingly simple, but have important difficulties:

\begin{itemize}
    \item Typically, an output item depends not only on the corresponding input item but also on the system state. This fact implies that a system can {\em technically} support exactly once delivery guarantee, but in practice can release completely invalid results, because of inconsistencies in the state or in-flight elements. For example, if the state is lost while computing the sum of all input elements, a system may continue to process input with exactly-once delivery, but the resulted sum becomes less than expected. 
    \item Multiple elements can be generated from one inside a system. If these elements are applied to state or released non-atomically, it can lead to inconsistencies as well. Therefore, there is a need to ensure that each input element is processed completely with all its derivatives. It can be illustrated as follows: assume that the system computes word counts and input elements are texts. Inside a system, each input text is mapped into the multiple word elements. If at least one word element is lost or is not applied to system state, while others are succeeded, a system does not support any kind of delivery guarantee. 
\end{itemize}

Only exactly once provides similar to batch systems guarantees on data. However, the mentioned difficulties make exactly once enforcement challenging for streaming systems. For instance, Samza and Storm do not support such a guarantee at all. Micro-batching approaches like Storm Trident~\cite{apache:storm:trident} and Spark Streaming~\cite{Zaharia:2012:DSE:2342763.2342773} provide quite high latency, about a few seconds~\cite{7530084, 7474816}. Flink can achieve latency in terms of milliseconds for at least once guarantee but requires a protocol based on distributed transactions for exactly once, that leads to a significant increase of latency. Such a difference in latency between distinct types of guarantees is commonly not obvious and reduce the applicability of stream processing systems.

Another property that is inherent for batch processing systems, but is hard to achieve in streaming engines is {\em determinism}. Determinism means that each run of the system on the same data produces the same results. Determinism is commonly considered as a challenging task because streaming systems must produce results before all computations are done~\cite{Zacheilas:2017:MDS:3093742.3093921}. On the other hand, this property is desirable, because implies reproducibility and predictability. Intuitively, determinism is connected with consistency~\cite{Stonebraker:2005:RRS:1107499.1107504}, but, to the best of our knowledge, this relation has not been deeply investigated. 

Besides implementation complexity, a major pitfall that system developers and users face regarding streaming guarantees is the lack of its formalization. This fact constantly causing debates and misunderstandings~\cite{JerryPengStreamIO, PaperTrail}. It is often unclear which guarantees are {\em exactly} provided by a stream processing system, especially for inexperienced users. Without a formal model, it is also hard to observe similarities and distinctions between existing solutions and to recognize their limitations.

In this work, we introduce a formal model of stream processing that captures delivery guarantees existing in several state-of-the-art systems. We show that the concepts of delivery guarantees are closely related to {\em consistency}. We believe that such meaning is much more reasonable, and further in this paper, we will call them {\em consistency guarantees} as well. We demonstrate that determinism is tightly connected with the consistency guarantees. One reason behind it is that deterministic system always produces the same results in case of failure and subsequent partial input replay and these results can be simply deduplicated.

In order to prove the feasibility of exactly once over determinism, we design fault tolerance protocols on top of {\em drifting state} model~\cite{we2018adbis}. Unlike naive expectations, this optimistic technique provides for determinism with only a low overhead. We show that having determinism, it is possible to achieve exactly once almost without extra cost. Our experiments demonstrate that the proposed solution can significantly outperform state-of-the-art stream processing system in terms of latency.

The contributions of this paper are the following: 
\begin{itemize}
    \item Formalization of streaming consistency guarantees 
    \item Demonstration that the property of determinism implies exactly once
    \item Techniques for low-cost implementation of exactly once on top of the deterministic engine
    \item Demonstration of practical feasibility of the proposed approaches
\end{itemize}

The rest of the paper is structured as follows: 