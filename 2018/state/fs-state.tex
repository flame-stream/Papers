\documentclass{llncs}
% packages should be added as needed 
\usepackage{graphicx}

\usepackage{algorithm} % for algorithms
\usepackage{algpseudocode}

\usepackage{booktabs} % For formal tables
\usepackage{cite} %for multiple refs

%\usepackage{amsthm} % For claims
%\theoremstyle{remark}

%\settopmatter{printacmref=false, printccs=false, printfolios=false}
\pagestyle{plain} % removes running headers

\newcommand{\PicScale}{0.5}
\newcommand {\FlameStream} {FlameStream}
\begin{document}

\title {Consistency guarantees for deterministic stateful stream processing}

\author{Igor E. Kuralenok \and Artem Trofimov \and Nikita Marshalkin \and Boris Novikov}
\institute{JetBrains Research, St. Petersburg, Russia}

\maketitle

\email{\string{ikuralenok, trofimov9artem, marnikitta\string}@gmail.com, borisnov@acm.org}

\begin{abstract}
\end{abstract}

\section {Introduction}
Distributed batch processing systems, such as Google's MapReduce~\cite{Dean:2008:MSD:1327452.1327492} and Apache Hadoop~\cite{hadoop2009hadoop}, address a need to process huge amounts of data (e.g. Internet scale). The key idea behind them is to process data blocks that have already been stored over a period of time. These systems are able to run in a massively parallel fashion on clusters consisting of thousands of commodity computational units. The main advantages of this approach are consistency, fault-recovery, and practically unlimited scalability.

However, there are a lot of scenarios where data is most valuable at its time of arrival, for example, news processing, financial analysis, network monitoring, etc. Therefore, there is a need for systems that provide low latency between event occurrence and its processing under a fixed load. Micro-batching technique, that is used in Apache Spark~\cite{Zaharia:2012:DSE:2342763.2342773} and in Storm Trident~\cite{apache:storm:trident}, inherits strong consistency properties from batch systems. More precisely, input items are collected or divided at systems' entry into small data blocks called {\it micro-batches} and then these blocks are processed as ordinary batches. It is possible to achieve the latency of several second using this approach~\cite{7530084, 7474816}, but it is still too high for many applications.

State-of-the-art industrial distributed stream processing systems, such as Flink \cite{carbone2015apache}, Samza \cite{Noghabi:2017:SSS:3137765.3137770}, Storm \cite{apache:storm}, address this issue by providing {\it record-at-a-time} processing model. According to this model, computational units receive each record, update internal state if any, and send out new records. This technique avoids buffering data items before processing and allows to achieve significantly lower latency than micro-batching~\cite{7530084}.

The main difficulty, that stream processing systems experience, is providing strong consistency. Regarding stream processing, the strongest consistency semantics is {\it exactly once}. Exactly once semantics requires that every input data item is processed only once, with or without the existence of failures. Although, such behavior is natural for batch processing systems, it is challenging for streaming systems. For instance, Samza and Storm do not support such guarantee at all, while Flink requires a complex protocol that can lead to latency increasing.

The key problem here is that unlike batch processing, stream processing is inherently non-deterministic. Particularly, there is no guarantee that the messages will be processed in the same order between any two runs. Therefore, it can be unclear how to determine which elements have been already released from the system after recovery to avoid duplicates. Finding the right resuming input point can be tricky as well. Additionally, if the states of operations are replicated, there is a need to determine that they are in a consistent state at the moment of taking the snapshot. 

In this paper we propose a technique to provide consistency guarantees for~\FlameStream\ - stream processing model that we designed and implemented. \FlameStream\ hides non-deterministic nature of stream processing by strong ordering assumptions. Out-of-order items are handled in an optimistic manner to avoid extra buffering. Invalid items, that can be generated by the optimistic approach, are filtered out before they leave the system. Our evaluation demonstrates that this approach has low overhead and can outperform alternative industrial solution under normal load conditions.

The order between data items is provided by meta-information that is assigned to each item. Because of the fact that processing model is deterministic, meta-information allows us to uniquely identify which data items have already left the system, from which point computations should start after recovery, and which version of the operations' state should be loaded. These features make exactly once semantics low-cost in terms of performance, that is shown in our experiments.

Therefore, the contributions of this paper are the following: 

\begin {itemize}
\item Definition of deterministic stateful streaming computational model 
\item Description of method for providing strong consistency properties within proposed model 
\item Demonstration the competitiveness of this technique
\end {itemize}

The rest of the paper is structured as follows: ...

\section {Related work}
Micro-batching. Flink's async snapshots + 2PC. Both provide high overhead. 

\section {Efficient deterministic model}
This section contains the high-level view of our model and a few implementation details.

\subsection{Data flow}
Define stream and data items.

\subsection{Computational flow}
Flow is represented in the form of a graph.

\subsection{Ordering assumptions}
The main idea: we define a total ordering on items to achieve determinism.

\subsection{Physical deployment}
The graph is deployed on each node. Hash-units, partitioning, etc.

\subsection{Supported operations}
Map/grouping and their properties.

\subsection{Stateful transformations}
Pipeline for any stateful transformation using only map and grouping. 

\subsection{Implementation notes}
In this section we describe how proposed model can be efficiently implemented without diving into details.

\subsection{Consistency guarantees}
At most once, at least once, exactly once. Exactly once is achieved if failures do not occur.

\section{Fault tolerance}

Our deterministic model is able to naturally provide some consistency guarantees. There are three main techniques: input replay, barrier flushing, and state snapshotting.

\subsection{Input replay}
Fronts should replay in terms of our meta-information.

\subsection{Barrier flushing}
To achieve exactly once, we should know which elements have been released. Because of the deterministic model, we should know only the meta-information of the last released item. Therefore, we require that user is able to return last received item. Contracts with rear are different for at least once and exactly once. Flushing process is independent from main processing.

\subsection{State snapshotting}
Snapshotting is needed to not replay all input. Acker starts snapshotting. All nodes store snapshot asynchronously.

\subsection{Recovery protocols}
We can handle package loss and node failure. Network partitioning is out of the scope of stream processing.

\subsubsection{Package loss}
Acker determines. Replay.

\subsubsection{Node failure}
State can be restored. Replay.

\subsubsection{Barrier failure}
Barrier's state can be restored. Replay.

\section {Experiments}

\subsection{Overhead and scalability}
Show that our model is scalable and provides low overhead.

\subsection{Comparison against Apache Flink}
Show that our prototype outperforms Flink with at least once/exactly once mode on. Explain such behavior.

\section {Conclusion and future work}

\bibliographystyle{splncs03}
\bibliography{../../bibliography/flame-stream}
\end {document}


\endinput
you can put whatever here
