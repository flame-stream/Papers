%%% fs-state-consistency - Fault tolerance

\label {fs-consistency-seciton}

In this section we describe how consistency guarantees can be provided within the proposed model in case of failures. Initially, we define three mechanisms which are necessary both for consistent processing and for fast and reliable recovery. After that, we demonstrate how they behave in recovery processes and why consistency semantics is preserved.

It is worth to mention that we rely strongly on the deterministic properties of the proposed model. Particularly, we expect that several independent runs with the same input data produce exactly the same result. This property is guaranteed by the fact that all supported operations produce deterministic results up to the order of input items. These features allow us to build efficient techniques for barrier flushing and operations' state management, which almost do not depend on each other.  

\subsection{Input replay}
Input replay functionality is commonly used to restore computational process after a failure in stream processing systems [?]. The key idea here is that in case of failure, previously released input data can be produced again. Usually, replay is started not from the beginning, but from some determined point. Like other stream processing solutions, our system requires from producers an ability to replay input data. The only difference is that the initial point of replay is defined in terms of the global time. Practically, the role of global times can be played by any monotonically increasing sequence, e.g. offsets in Apache Kafka or the values of a logical clock. Therefore, this requirement is not a strong limitation for real-life deployments.

\subsection{Barrier flushing}
Notifications of the new minimal time within the stream are sent by the acker with monotonically increasing global times. Barrier receives these notifications and releases output items with monotonically increasing global times as well. Hence, if barrier knows the global time of the last released item $GT_{last}$, it can process data in idempotent fashion simply by filtering out any items with a global time less than or equal to $GT_{last}$. 

Therefore, the barrier has its own state - $GT_{last}$, and this state is applied for avoiding duplicates in case of failure and subsequent input replay. Exactly-once semantics is possible only if there are no any inconsistencies between released items and $GT_{last}$, so there is a need to atomically output items and update $GT_{last}$. To solve this problem, we require the following output protocol with data consumer:

\begin{itemize}
    \item When minimal time is received, barrier send special output bundle to the data consumer. This bundle contains all corresponding output items and $GT_{last}$. The consumer must acknowledge that it received the bundle
    \item Barrier does not send new output bundle until the previous one is not acknowledged
    \item Consumer must return last received bundle on barrier's request 
\end{itemize}

This protocol guarantees that $GT_{last}$ and released items are always consistent with each other. It implies that barrier can request the last released bundle and fetch $GT_{last}$ after recovery to avoid duplicates and preserve exactly-once semantics. It should be noted that for at least once semantics, contract with the consumer can be relaxed: it is not required to store the last received bundle.

\subsection{State snapshotting}
Actually, in case of failures, a producer can replay all previously sent data items on recovery without loss of exactly-once semantics. Such behavior is achieved because processing results are deterministic and all possible duplicates are filtered out at the barrier. However, this approach can be memory and time demanding. 

Therefore, there is a need to periodically take a snapshot of operations' state and to replay on recovery from the global time that corresponds to the previous snapshot. In order to start processing after recovery from global time $GT$, two conditions must be satisfied:
\begin{enumerate}
    \item All stateful operations can restore the state that could be reached after consuming all data items with global times in range $[0..GT)$ 
    \item All bundles that contain items with global times in range $[0..GT)$ have been already acknowledged by data consumer 
\end{enumerate}

It is convenient to piggyback on the acker's notifications of the minimal time within the stream for triggering state saving. Let $GT_{min}$ be the last received minimal time within the stream. Grouping's buckets in practice are lists sorted by global time. Because of the guarantees provided by minimal time notification, these lists cannot be changed at any position before the item that corresponds to $GT_{min}$. At the same time, because of the grouping semantics, state that could be reached after consuming all data items with global times in range $[0..GT_{min})$ is just a window-sized sublist that is located in the immutable part of the bucket. Thereby, state snapshotting can be done asynchronously not only with state snapshotting of other operations but even with the computational process of the current operation.

Considering the properties mentioned above, the protocol of state snapshotting is the following:

\begin{itemize}
    \item On some minimal time event, acker decides to initiate state snapshotting. The decision is made by its internal logic, i.e. by the elapsed time since the last snapshot. Acker sends a request for the new snapshot along with minimal time notification
    \item When grouping operation receives the request, it asynchronously saves the state and sends back the acceptance message   
    \item When barrier receives the request, it waits until producer acknowledges all in-flight bundles, and then sends back the acceptance message to acker
    \item When acker receives all acceptance messages, it writes the global time of the snapshot to ZooKeeper 
\end{itemize}

It is worth to note that any persistent key-value storage can be applied as a storage for a state. Hash unit of the corresponding grouping operation concatenated with received minimal time can be used as a key. Waiting until barrier sends out all in-flight bundles is the only dependency between state snapshotting and barrier flushing mechanisms, that does not practically influence end-to-end processing latency. 

This protocol satisfies two proposed above conditions regarding the global time that is written to ZooKeeper. Hence, this global time can be safely used as a resuming point after recovery.

\subsection{Recovery protocols}
We can handle package loss and node failure. Network partitioning is out of the scope of stream processing.

\subsubsection{Package loss}
Acker determines. Replay.

\subsubsection{Node failure}
The state can be restored. Replay.

\subsubsection{Barrier failure}
Barrier's state can be restored. Replay.