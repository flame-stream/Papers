%%% fs-state-consistency - Fault tolerance

\label {fs-consistency-seciton}

In this section we describe how consistency guarantees can be provided within the proposed model in case of failures. Initially, we define three mechanisms which are necessary both for consistent processing and for fast and reliable recovery. After that, we demonstrate how they behave in recovery processes and why consistency semantics is preserved.

It is worth to mention that we rely strongly on the properties of the proposed model and its implementation. Firstly, we expect that processing is deterministic, i.e. several independent runs with the same input data produce exactly the same result. This property is guaranteed by the fact that all supported operations produce deterministic results up to the order of input items. Secondly, we suppose that there is a single per cluster agent that can coordinate protocols for fault tolerance. The role of such coordinator is played by the acker. Thirdly, we use the fact that acker broadcasts minimal time notifications when it is guaranteed that there are no in-flight data items with particular global time and such items cannot be generated further. Additionally, we exploit the simple structure of the grouping's state, which is actually a list sorted by the global time. These features allow us to build efficient techniques for barrier flushing and operations' state management, which preserve exactly-once semantics and almost do not depend on each other.

It is important to notice that at most once semantics is preserved even in case of failures without any additional logic. Such behavior is achieved because we assume that data producers do not replay input data if it is not requested and within our system, data can only be lost, but not duplicated. Therefore, we mainly focus on providing at least once and exactly-once semantics.

\subsection{Input replay}
Input replay functionality is commonly used to restore computational process after a failure in stream processing systems [?]. The key idea here is that in case of failure, previously released input data can be retrieved again. Usually, replay is started not from the beginning, but from some determined point. Like other stream processing solutions, our system requires from producers an ability to replay input data. The only difference is that the initial point of replay is defined in terms of the global time. Practically, the role of global times can be played by any monotonically increasing sequence, e.g. offsets in Apache Kafka or the values of a logical clock. Therefore, this requirement is not a strong limitation for real-life deployments.

\subsection{Barrier flushing}
Notifications of the new minimal time within the stream are sent by the acker with monotonically increasing global times. Barrier receives these notifications and releases output items with monotonically increasing global times as well. Hence, if barrier knows the global time of the last released item $GT_{last}$, it can process data in idempotent fashion simply by filtering out any items with a global time less than or equal to $GT_{last}$. 

Therefore, the barrier has its own state - $GT_{last}$, and this state is applied for avoiding duplicates in case of failure and subsequent input replay. Exactly-once semantics is possible only if there are no inconsistencies between released items and $GT_{last}$, so there is a need to atomically output items and update $GT_{last}$. To solve this problem, we require the following output protocol with data consumer: \ref{consumer-contract}

\begin{itemize}
    \item When minimal time is received, barrier send special output bundle to the data consumer. This bundle contains all corresponding output items and $GT_{last}$. The consumer must acknowledge that it received the bundle
    \item Barrier does not send new output bundle until the previous one is not acknowledged
    \item Consumer must return last received bundle on barrier's request 
\end{itemize}

This protocol guarantees that $GT_{last}$ and released items are always consistent with each other. It implies that barrier can request the last released bundle and fetch $GT_{last}$ after recovery to avoid duplicates and preserve exactly-once semantics. It should be noted that for at least once semantics, contract with the consumer can be relaxed: it is not required to store the last received bundle.

\subsection{State snapshotting}
In case of failures, a producer can replay all previously sent data items on recovery without loss of exactly-once semantics. Such behavior is achieved because processing results are deterministic and all possible duplicates are filtered out at the barrier. However, this approach can be memory and time demanding. 

Therefore, there is a need to periodically take a snapshot of operations' state and to replay on recovery from some determined point that corresponds to the previous snapshot. Global time is the most natural indicator of such points within our model because the grouping's state for the specified global time can be easily retrieved from the full state and the data producer supports replay in terms of global time.   

Hence, the global time for taking snapshot must be periodically chosen. It is convenient to piggyback on the acker's notifications of the minimal time within the stream for triggering state saving. Let $GT_{min}$ be the last received minimal time within the stream. Grouping's buckets in practice are lists sorted by global time. Because of the guarantees provided by minimal time notification, these lists cannot be changed at any position before the item that corresponds to $GT_{min}$. At the same time, because of the grouping semantics, state that could be reached after consuming all data items with global times in range $[0..GT_{min})$ is just a window-sized sublist that is located in the immutable part of the bucket. Thereby, state snapshotting can be done asynchronously not only with state snapshotting of other operations but even with the computational process of the current operation. 

Considering the properties mentioned above, the protocol of state snapshotting is the following:

\algblockdefx[Process]{Process}{EndProcess}[2][Unknown]{{\bf Process} {\it #2}}{}
\algblockdefx[Event]{Event}{EndEvent}[2][Unknown]{{\bf upon event #2 do}}{}

\begin{algorithm}
\caption{Snapshotting}
\label{consumer-contract}
  \begin{algorithmic}
    \Process {Acker}
      \State $lastCommit = -\infty$ \Comment{kept in persistent storage}
      \State $verteces \gets$ all physical groupings and barrier instances
      \State $committed \gets \varnothing$
      \State
      \Event{$\langle MinTime, t \rangle$}
        \If {$committed \neq verteces$} \Comment{There are no unfinnished commits}
          \For{{\bf all $v$} $\in verteces$}
            \State \Call{Send}{$v, \langle MinTime, t, Commit \rangle$}
          \EndFor
        \EndIf
      \EndEvent
      \Event{$\langle Committed, v, t\rangle$}
        \State $committed \gets committed \cup v$
        \If {$committed = verteces$}
          \State $committed \gets \varnothing$
          \State $lastCommit \gets t$
        \EndIf
      \EndEvent
    \EndProcess

    \Process {Grouping}
      \State $bucket[]$ \Comment{list of data items ordered by time}
      \State
      \Event{$\langle MinTime, t, Commit \rangle$}
        \State \Call{Save}{t, bucket[0, t]} \Comment{Save substate to persistent storage}
        \State \Call{ClearRange}{bucket, 0, t}
        \State \Call{Send}{$acker, \langle Committed, self, t \rangle$}
      \EndEvent
    \EndProcess

    \Process {Barrier}
      \State $lastAcknowledged \gets -\infty$ \Comment{time of the last acknowledged bundle}
      \State $postponedCommit \gets \infty$
      \State
      \Event{$\langle MinTime, t, Commit\rangle$}
        \If{$lastAcknowledged \geq t$}
          \State \Call{Send}{$acker, \langle Committed, self, t \rangle$}
        \Else
          \State $postponedCommit \gets t$
        \EndIf
      \EndEvent
      \Event{$\langle Ack, t\rangle$}
        \State $lastAcknowledged \gets t$
        \If {$postponedCommit < lastAcknowledged$}
          \State \Call{Send}{$acker, \langle Committed, self, postponedCommit \rangle$}
          \State $postponedCommit \gets \infty$
        \EndIf
      \EndEvent
    \EndProcess
  \end{algorithmic}
\end{algorithm}

\begin{itemize}
    \item On minimal time event, acker sends a request for the new snapshot along with minimal time notification
    \item When grouping operation receives the request, it asynchronously saves the state and sends back the acceptance message   
    \item When barrier receives the request, it waits until producer acknowledges all in-flight bundles, and then sends back the acceptance message to acker
    \item When acker receives all acceptance messages, it writes the global time of the snapshot to cluster manager (ZooKeeper) 
\end{itemize}

Acker can send requests for the new snapshot not for each minimal time event, e.g. it can skip request if the elapsed time since the last snapshot is less than some threshold. Parameters that influence the frequency of snapshots are in user's scope.

It is worth to note that any persistent key-value storage can be used as a storage for the state. Hash unit of the corresponding grouping operation concatenated with received minimal time can be used as a key. Waiting until barrier sends out all in-flight bundles is the only dependency between state snapshotting and barrier flushing mechanisms, that does not practically influence end-to-end processing latency. 

This protocol satisfies the following conditions regarding the global time $GT$ that is written to cluster manager:
\begin{enumerate}
    \item All stateful operations can restore the state that could be reached after consuming all data items with global times in range $[0..GT)$ 
    \item All bundles that contain items with global times in range $[0..GT)$ have been already acknowledged by data consumer 
\end{enumerate}

Hence, this global time can be used as a resuming point after recovery without loss of consistency.

\subsection{Failure detection and recovery}
Typically, distributed systems take into consideration the following types of failures:
\begin{itemize}
    \item Packet loss
    \item Node failure
    \item Network partitioning
\end{itemize}

Network partitioning is the special case of failure because it leads to an unavoidable loss of data. We believe that in this case, stream processing does not make sense. To the best of our knowledge, there are no open-source stream processing systems that tolerate network partitioning.

As it was described above, acker traces data items using the table of XORs grouped by global time. Therefore, packet loss can be determined by the acker because in this case, the corresponding value in acker's table will not be nullified. Node failure can be also observed by the acker because state snapshotting protocol requires waiting until all barriers accept that corresponding bundles have been sent and acknowledged. In fact, each node contains a barrier, so if an acceptance is not received during some user-defined timeout, the node can be declared as down. 

In case of packet loss or node failure, acker fully restarts the computations. The failure of the acker itself can be detected by special scripts, that trigger the same restart protocol. Restart protocol includes the following steps:

\begin{itemize}
    \item Acker reads the global time of the last snapshot from cluster manager. After that, it sends this global time to all grouping operations
    \item Grouping operations fetch their states from state backend using the corresponding hash unit and received global time as a key. After that, grouping operations acknowledge acker that they are ready for processing 
    \item Barriers request the last released bundle from data consumers and acknowledges acker that they are ready for processing
    \item When acker receives all acknowledgments from groupings and barriers, it sends the global time of the last snapshot to data producer, and the processing begins 
\end{itemize}

Proposed protocol guarantees the following properties:

\begin{itemize}
    \item Processing does not restart until all grouping operations obtain consistent states. The consistency of these states is guaranteed by the state snapshotting protocol
    \item Producing duplicates is avoided, because, at the moment when processing is restarted, it is ensured that barrier has obtained the last released global time and is able to filter out extra items
    \item The last released global time cannot be less than the global time of the last snapshot, therefore, data cannot be lost. This property is guaranteed in conjunction with state snapshotting protocol
\end{itemize}

Therefore, the proposed protocol can be used for system restart without loss of exactly once semantics. Regarding at least once semantics, all protocol actions are the same, except waiting until the barriers are ready.
