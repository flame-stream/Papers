%%% fs-phd-related - State of the art
\label{fs-phd-reliable}

Our main goal is to provide high-level user-defined guarantees on data. We start with a discussion about the possible way to define and work with such guarantees from a user perspective. After that, we touch on the initial thoughts about physical implementation of this feature.

The typical user of data processing system desire to fetch reliable results with minimal effort. Therefore, we believe that computational pipelines should be defined in a declarative fashion. However, existing declarative approaches, e.g. SQL, do not allow to set up custom high-level consistency requirements. It is convenient to think about data streams in terms of {\em semantic types} rather than particular operations and constraints. For example, we can describe the stream as "test data for experiment number 3" or "users of mobile application". Each semantic type can also require some high-level consistency constraints, e.g. the percent of users with an empty profile is less than 10 or all items satisfy some particular schema. In our work we wish to implement user-system interaction in terms of the semantic types.

An ultimate objective for us is to implement mechanisms that allow users to define streams in terms of semantic types and maintain high-level consistency guarantees relating to the types. In this case, administrators of the system can define basic conversions between types and high-level consistency requirements for each type. The system must automatically derive some defined semantic type from existing if it is possible. However there are several open problems regarding this feature, e.g., it is not clear how to implement type derivation subsystem that generates low-level execution plans, which are optimal regarding CPU and network usage, and, at the same time, provide minimal time for detecting inconsistencies in the stream. Besides, there is a need to design a mechanism that allows to determine which low-level consistency is needed for the particular high-level requirements.  

If a stream is defined in terms of semantic type, consistency requirements must be checked after each type conversion. On the physical level, type conversion is a sequence of low-level operations on the stream, so consistency requirements must be checked after some physical operations. However, operations can be partitioned into multiple computational units. Merging data from all partitions after each operation for consistency checking may provide high overhead on network and serialization. The only way to effectively implement consistency maintenance mechanisms is to check requirements independently on each partition. In order to have a strong statistical grounds for this approach, we need to ensure that data samples on all partitions are uniform. Hence, the system must ensure that hashes using for partitioning are fairly uniform.

We suppose that user can define distinct consistency requirements on distinct types. Therefore, different parts of the computational pipeline can require different consistency guarantees. Moreover, these requirements can depend on each other. Therefore, consistency checking mechanisms can generate additional streams which can go along execution graph and transfer meta-information about the consistency of some parts of the pipeline. These streams can also require some low-level consistency guarantees, so our model for low-overhead exactly-once, that was discussed in the previous section, can be successfully applied here. An idea of such additional stream is not a new, e.g. the idea of punctuations~\cite{Tucker:2003:EPS:776752.776780} is similar. Nevertheless, we plan to make these streams more complex and suitable for providing custom high-level guarantees.

Our model of meta-streams for high-level consistency faces a lot of complex statistical problems. For instance, even perfectly uniform hash functions guarantee only the uniformity of samples, but not the uniformity of the load. For example, the first partition can receive 90\% of the data for the first operation and 10\% of the data for the second operation, while the proportion of the data for the second partition can be the opposite. Hence, there is a need for statistical corrections for meta-information to achieve statistically significant results. Another example of the problem that we address is to statistically compensate the imperfectness of the hash functions if we know the probability of the collision.  