%%% fs-phd-related - State of the art
\label{fs-phd-reliable}

Our main goal is to provide high-level user-defined guarantees on data. We wish to bind data management and statistics to achieve it. We start with a discussion about physical implementation of this feature and then touch on the initial thoughts about the ways of defining high-level guarantees.

Let us assume that users are able to set up high-level guarantees in some way. We suppose that consistency requirements must be checked after each operation. However, operations can be partitioned into multiple computational units. Merging data from all partitions after each operation for consistency checking may provide high overhead on network and serialization. The only way to effectively implement consistency maintenance mechanisms is to check requirements independently on each partition. In order to have a strong statistical grounds for this approach, we need to ensure that data samples on all partitions are uniform. Hence, the system must ensure that hashes using for partitioning are fairly uniform.

We suppose that user can define distinct consistency requirements on distinct parts of the computational pipeline. In this case, these requirements can depend on each other. Therefore, consistency checking mechanisms can generate additional streams which can go along execution graph and transfer meta-information about the consistency of some parts of the pipeline. These streams can also require some low-level consistency guarantees, so our model for low-overhead exactly-once, that was discussed in the previous section, can be successfully applied here. An idea of such additional stream is not a new, e.g. the idea of punctuations~\cite{Tucker:2003:EPS:776752.776780} is similar. Nevertheless, we plan to make these streams more complex and suitable for providing custom high-level guarantees.

Our model of meta-streams for high-level consistency faces a lot of complex statistical problems. For instance, even perfectly uniform hash functions guarantee only the uniformity of samples, but not the uniformity of the load. For example, the first partition can receive 90\% of the data for the first operation and 10\% of the data for the second operation, while the proportion of the data for the second partition can be the opposite. Hence, there is a need for statistical corrections for meta-information to achieve statistically significant results. Another example of the problem that we address is to statistically compensate the imperfectness of the hash functions if we know the probability of the collision.  

The typical user of data processing system desire to fetch reliable results with minimal effort. Therefore, we believe that computational pipelines should be defined in a declarative fashion. However, existing declarative approaches, e.g. SQL, do not allow to set up custom high-level consistency requirements. It is convenient to think about data streams in terms of {\em semantical types} rather than particular operations and constraints. For example, we can describe the stream as "test data for experiment number 3" or "users of mobile application". Each semantical type can also require some high-level consistency constraints, e.g. the percent of users with an empty profile is less than 10 or all items satisfy some particular schema. In our work we wish to implement user-system interaction in terms of the semantical types.