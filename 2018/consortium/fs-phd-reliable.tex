%%% fs-phd-related - State of the art
\label{fs-phd-reliable}

Our primary goal is to provide high-level user-defined guarantees on data. We start with a discussion about the desired way to define and work with such guarantees from a user perspective. After that, the key problems, which we expect to address, are mentioned. Eventually, we touch on the initial thoughts about the implementation plan.

\subsection{Concepts}

The typical user of data processing system desire to fetch reliable results with minimal effort. Therefore, we believe that computational pipelines should be defined in a declarative fashion. However, existing declarative approaches, e.g., SQL, do not allow to set up custom high-level consistency requirements. It is convenient to think about data streams in terms of {\em semantic types} rather than particular operations and constraints. For example, we can describe a stream as "test data for experiment number 3" or "users of mobile application". Each semantic type can also require some high-level consistency constraints, e.g., the percent of users with an empty profile is less than 10 or all items satisfy some particular scheme. In our work, we wish to implement user-system interaction in terms of the semantic types.

Stream publishing is another area, where semantic types can provide a better user experience. Suppose, a lot of users work with a set of published streams. In this terms, {\em published} means available for other users. Each stream has its semantic type. If a user needs for a new stream to work with, they can verify its current state by checking its type consistency. Thus, users can have a reliable way to stream verification.

An ultimate objective for us is to implement mechanisms that allow users to define streams in terms of semantic types and maintain high-level consistency guarantees relating to the types. In this case, basic conversions between types and high-level consistency requirements for each type can be defined manually, but the system must automatically derive some defined semantic type from existing if it is possible. However there are several open problems regarding this feature, e.g., it is not clear how to implement type derivation subsystem that generates low-level execution plans, which are optimal regarding CPU and network usage, and, at the same time, provide minimal time for detecting inconsistencies in a stream. Besides, there is a need to design a mechanism that allows determining which low-level consistency is needed for the particular high-level requirements.  

If a stream is defined in terms of semantic type, consistency requirements must be checked after each type conversion. On the physical level type conversion is a sequence of low-level operations within a computational pipeline, so consistency requirements must be checked after some physical operations. However, operations can be partitioned into multiple computational units. Merging data from all partitions after each operation for consistency checking may provide high overhead on network and serialization. The only way to effectively implement consistency maintenance mechanisms is to check requirements independently on each partition. In order to have a strong statistical grounds for this approach, we need to ensure that data samples on all partitions are uniform. Hence, the system must ensure that hashes being used for partitioning are fairly uniform.

We suppose that user can define distinct consistency requirements on distinct types. Therefore, different parts of the computational pipeline can require different consistency guarantees. Moreover, these requirements can depend on each other. Thus, consistency checking mechanisms can generate additional streams which can go along execution graph and transfer meta-information about the consistency of some parts of the pipeline. These streams can also require some low-level consistency guarantees, so our model for low-overhead exactly-once, that was discussed in the previous section, can be successfully applied here. An idea of such additional stream is not a new, e.g., the concept of punctuations~\cite{Tucker:2003:EPS:776752.776780} is similar. Nevertheless, we plan to make these streams more complex and suitable for providing custom high-level guarantees.

Our model of meta-streams for high-level consistency faces a lot of complicated statistical problems. For instance, even perfectly uniform hash functions guarantee only the uniformity of samples, but not the uniformity of the load. For example, the first partition can receive 90\% of the data for the first operation and 10\% of the data for the second operation, while the proportion of the data for the second partition can be the opposite. Hence, there is a need for statistical corrections for meta-information to achieve statistically significant results. Another example of the problem that we address is to statistically compensate the imperfectness of the hash functions if we know the probability of the collision.  

\subsection{Implementation}

We plan to start the implementation with migration from typical API for manual configuration of execution graph to semantic types-based API within \FlameStream\ stream processing engine~\cite{we2018beyondmr}. We suppose that high-level consistency is most valuable in conjunction with a convenient declarative way to define the computational pipeline. As it was mentioned above, the complex technical task here is to implement efficient types derivation mechanisms, which utilize CPU and network usage.

After that, it is planned to design a suitable approach to declare custom high-level consistency guarantees for particular types. This mechanism should be easy to use for analysts, but flexible enough in order to allow declaring a wide range of constraints, e.g., constraints on data distribution, on a schema, on particular values, etc.  We wonder if it is possible to automatically determine required low-level consistency requirements from the high-level definition. If it is not, we plan to allow users to manually set up required low-level guarantees for custom high-level ones.

The next step is to build a subsystem for checking high-level consistency semantics within the whole computational pipeline. Initially, we plan to implement proof-of-concept without deep performance optimization. Merging data from all partitions after each operation for consistency checking is also acceptable for us on this level. The main goal of the step is to demonstrate that maintenance of high-level consistency semantics is possible within distributed stream processing.

The last step in our list is an accurate performance optimization. We aim at achieving minimal overhead on the proposed functionality. The key problem regarding this step is to design statistical methods for checking high-level consistency independently on each physical partition.
