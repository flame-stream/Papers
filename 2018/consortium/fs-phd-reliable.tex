%%% fs-phd-related - State of the art
\label{fs-phd-reliable}

Our primary goal is to provide high-level user-defined guarantees on data. We start with a discussion about the desired way to define and work with such guarantees from a user perspective. After that, the key problems, which we expect to address, are mentioned. Eventually, we touch on the initial thoughts about the implementation plan.

\subsection{Concepts}

The key purpose of the high-level consistency mechanisms within stream processing is to validate data in terms of business logic. High-level validation is not a novel approach, e.g., {\em acceptance tests} have been successfully applying in software engineering for a long time~\cite{hambling2013user}. Acceptance testing is also applied in data processing. As an example, we can mention uniformity checking in test and baseline samples within a popular A/B testing task. Besides, data validation is a key task regarding machine learning model deployment in TFX~\cite{Baylor:2017:TTP:3097983.3098021}. However, usage of acceptance testing for stream processing pipelines requires complex technical implementation, because modern engines do not provide native mechanisms for this feature.  

In general, acceptance tests validate some statistical or structural constraints on data. Let {\em semantic type} be a set of high-level consistency constraints with a label. Our goal is to build a system that maps semantic type to the part of the streaming computational pipeline. In this case, a user can define the full pipeline as a sequence of semantic type conversions. The key motivation behind this model is that it is declarative, but, unlike common declarative approaches, provides for defining high-level constraints by design.          

Regarding the A/B test example, we can define "uniform users" semantic type with corresponding constraints. Users data in the stream of this type is guaranteed to be uniform. If it is not, stream processing system alerts and stops processing or tries to fix the issue. A user also can define derivative streams, e.g., "uniform mobile users" or "uniform desktop users". Now, we assume that constraints are statistical or structural, but formulating the complete list of constraint types that we plan to support is an area of further research. 

Therefore, we aim at migration from the conservative low-level graph or SQL-based declaration of a computational pipeline to semantic types. It allows users to reason in terms of what to implement from the business perspective, not how to implement it. Hence, users can concentrate on business tasks, while concrete procedures for data transformation are hidden.

\subsection{Challenges}

Conversions between types can be expressed in the form of ordinary execution graphs. Basic conversions can be defined manually by administrators or defined natively, but the system must automatically infer some defined semantic types from existing ones if it is possible. However, there are several open problems regarding this feature, e.g., it is not clear how to implement type inference subsystem that generates low-level execution plans, which are optimal regarding CPU and network usage, and, at the same time, provide minimal time for detecting inconsistencies in a stream. Besides, there is a need to design a mechanism that allows determining which low-level consistency is needed for the particular high-level requirements. In the most cases, high-level consistency is hard to achieve without strong low-level guarantees. Therefore, our model for low-overhead exactly-once, that is discussed in the previous section, can be successfully applied here.

If a pipeline is defined in terms of semantic types, consistency requirements must be checked after each type conversion. On the execution level type conversion is a sequence of low-level operations, so consistency requirements must be checked after some physical operations. However, operations can be partitioned into multiple computational units. Merging data from all partitions after each operation for consistency checking may provide high overhead on network and serialization. The only way to effectively implement consistency maintenance mechanisms is to check requirements independently on each partition. In order to have a strong statistical grounds for this approach, we need to ensure that the distribution of data samples has the same properties on all partitions.

We suppose that user can define distinct consistency requirements on distinct types. Therefore, different parts of the computational pipeline can require different consistency guarantees. Moreover, these requirements can depend on each other. Thus, consistency checking mechanisms can generate additional streams which can go along the execution graph and transfer meta-information about the consistency of some parts of the pipeline. An idea of such additional stream is not a new, e.g., the concept of punctuations~\cite{Tucker:2003:EPS:776752.776780} is similar. Nevertheless, we plan to make these streams more complex and suitable for providing custom high-level guarantees.

Our model of meta-streams for high-level consistency faces a lot of complicated statistical problems. For instance, even perfectly uniform hash functions guarantee only the uniformity of samples, but not the uniformity of the load. For example, the first partition can receive 90\% of the data for the first operation and 10\% of the data for the second operation, while the proportion of the data for the second partition can be the opposite. Hence, there is a need for statistical corrections for meta-information to achieve statistically significant results. Another example of the problem that we address is to statistically compensate the imperfectness of the hash functions if we know the probability of the collision.  

\subsection{Implementation plan}

We plan to start the implementation with migration from typical API for manual configuration of execution graph to semantic types-based API within \FlameStream\ stream processing engine~\cite{we2018beyondmr}. At this point, we consider semantic type just as a label without constraints on data. As it was mentioned above, the complex technical task here is to implement efficient types inference mechanisms, which utilize CPU and network usage.

After that, it is planned to design a suitable approach to declare custom high-level consistency guarantees for particular types. This mechanism should be easy to use for analysts, but flexible enough in order to allow declaring a wide range of constraints, e.g., constraints on data distribution, on a schema, on particular values, etc.  We wonder if it is possible to automatically determine required low-level consistency requirements from the high-level definition. If it is not, we plan to allow users to manually set up required low-level guarantees for custom high-level ones.

The next step is to build a subsystem for checking high-level consistency semantics within the whole computational pipeline. Initially, we plan to implement a proof-of-concept without deep performance optimization. Merging data from all partitions after each operation for consistency checking is also acceptable for us on this level. The main goal of the step is to demonstrate that maintenance of high-level consistency semantics is possible within distributed stream processing.

The last step in our list is an accurate performance optimization. We aim at achieving minimal overhead on the proposed functionality. The key problem regarding this step is to design statistical methods for checking high-level consistency independently on each physical partition.
