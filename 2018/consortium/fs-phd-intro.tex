%%% fs-phd-intro - Introduction

\label {fs-phd-intro}

In recent years large-scale data analytics has become a hot area of research and a crucial task for industrial applications~\cite{Zou:2010:SRQ:1920841.1921012}. The main reasons behind this fact are the continuous growth of the amount of the data available for analysis and the lack of the appropriate tools. Distributed batch processing systems, e.g., Google MapReduce ~\cite{Dean:2008:MSD:1327452.1327492} and Apache Hadoop~\cite{hadoop2009hadoop}, address some issues of large-scale data analytics. They can provide high throughput while being fault-tolerant and strong consistent in some sense. However, several issues still remain. In particular, MapReduce model suffers from high latency between event arrival and its processing, the lack of iterative processing, the lack of early termination~\cite{Doulkeridis:2014:SLA:2628707.2628782}, etc. Nevertheless, there are scenarios which require one more of the mentioned properties. These tasks include news processing, fraud detection, short-term personalization, etc.

Distributed stream processing systems were designed to address the mentioned issues with prevention of some consistency guarantees. The central concept of stream processing is a stream. The stream is a potentially unlimited sequence of input items. Typically, stream processing system is a shared-nothing distributed runtime, that handles input items and processes them one-by-one according to user-provided logic. A computational pipeline is usually specified by {\em execution graph}, where vertices define operations and edges determine the order between them. Operations within the pipeline can be stateless and stateful. The typical system manages the state of operations, e.g., periodically takes snapshots. Flink \cite{carbone2015apache}, Samza \cite{Noghabi:2017:SSS:3137765.3137770}, and Storm \cite{apache:storm} are the examples of modern stream processing engines. There are three main types of consistency guarantees regarding stream processing, which is considered in the literature~\cite{Kulkarni:2015:THS:2723372.2742788, Akidau:2013:MFS:2536222.2536229, Carbone:2017:SMA:3137765.3137777}. {\em At most once} semantics guarantees that each input event is processed once or not processed at all. {\em At least once} states that each input item is processed, but possibly multiple times. {\em Exactly once} semantics guarantee that each input event is processed exactly one time. In this work we will call these guarantees {\em low-level}. 

Generally, the main purpose of the users of stream processing systems is to retrieve valuable insights from input data. However, the processing result can be consistent in terms of low-level consistency, but, at the same time, be entirely incorrect from the business perspective. As an example, we can note the scenario of migration to the new version of data processing pipeline that computes some metrics on a stream. In this case, the latest version of the algorithm for metrics can contain bugs. However, only low-level consistency guarantees do not allow to detect this kind of faults.

Therefore, it would be extremely useful for users of stream processing systems to have a mechanism that allows to define a custom consistency semantics. The system should maintain such user-defined consistency or alarm if it is not possible. We will call this kind of consistency {\em high-level}. As examples of high-level consistency semantics, we can mention requirements for the particular statistical criteria, suitability of data for making reasonable decisions, and the presence of some structure in data. Although such semantics-based consistency is not a new area of research~\cite{Garcia-Molina:1983:USK:319983.319985}, this task is not seemed to be solved. Some papers on this topic are pure theoretical~\cite{Rodriguez:2008:ITA:1463434.1463480, Guo:2010:CMS:1822018.1822052}, while others propose only some predefined higher-level guarantees~\cite{Mihaila:2008:AIO:1458082.1458132, Fischer:2010:SSP:1739041.1739068}.

Keeping in mind all the mentioned above, a lot of theoretical and practical questions are arising, e.g., How does low-level consistency correspond with high-level? How can it be implemented without high overhead? What is the most appropriate and convenient way to declare custom consistency for users? How to make preserving of high-level consistency suitable for applying in performance-sensitive scenarios? The main purpose of our research work is to try to answer these questions. We aim at designing and evaluating effective mechanisms for high-level consistency within distributed stream processing in order to make large-scale stream analytics more stable and reliable. We believe that this concept can influence the overall efficiency of the process of stream analytics. The following list briefly demonstrates the plan of our research:

\begin{enumerate}
    \item Analyze modern approaches for achieving low-level consistency and design a new one if needed
    \item Investigate existing models and implementations of high-level consistency
    \item Understand how low-level and high-level consistency levels are correlated
    \item Implement high-level consistency on the top of an efficient implementation of low-level guarantees
    \item Design convenient, possibly declarative, mechanisms for setting up high-level guarantees
    \item Make proposed approaches compatible with state-of-the-art stream processing engines in terms of performance
\end{enumerate}

Currently, we have been working on the first three steps. It has been realized that some high-level guarantees require low-level consistency, while current methods for achieving exactly-once are too ineffective. In~\cite{we2018seim} we introduced a novel stream processing model and approach for handling out-of-order items. We briefly showed how these concepts can provide for exactly-once semantics with significantly lower overhead compared to state-of-the-art industrial solutions in~\cite{we2018beyondmr}. In this paper we discuss the obtained results and share our initial ideas about the rest of our plan, particularly, about possible concepts of the high-level consistency. 

The rest of the paper is organized as follows: in section~\ref{fs-phd-related} we discuss existing approaches for low-level and high-level consistency and analyze their applicability, the results that have been already obtained are detailed in~\ref{fs-phd-optimistic}, our plans and prospects regarding high-level consistency are mentioned in~\ref{fs-phd-reliable}, and we summarize the full research in~\ref{fs-phd-summary}.

%Как соотносятся разные модели согласованности со стоимостью их достижения. Достоверность.
%Задача анализа потоков. На сегодняшних системах оборудования - распределенные. Ослабленные, низкоуровневые гарантии согласованности. План всего, что будет дальше. Хотим эффективно делать низкоуровневые гарантии, чтобы получать стабильные, достоверные результаты (протаскивать на верхний уровень).