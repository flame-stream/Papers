\label {fs-acker-impl}

In the previous section, we introduced a general schema of the \tracker. In this section, we deepen into its implementation details. In the first section, we formally define the \textit{global time} labeling and explore its properties. The second section is dedicated to the \tracker\ agent messaging structure. The next section focuses on the single node implementation of the \tracker\ agent and local \tracker\ optimization. Distributed implementation of the \tracker\ concludes this part.

\subsection{Global time}
In Section~\ref{sec:acker-analysis}, we have introduced the notation of \textit{global time}, but the details of its practical implementation remain behind the scene. We can use time oracle~\cite{10.14778/3055330.3055335}, but in this case, the system will have a single point of failure, and we decided to make our system tolerant to the time difference between font-end. In this case, the comparison between timestamps coming from different front nodes is limited, and the \textit{global time} nice properties are under threat. To overcome this difficulty, we systematically synchronize the system time of the front-nodes and assume that this measure prevents the system time difference more than some fixed $\delta$, which we reference as synchronization slack.

To work with the guarantees formally we need to define suitable notation. We use $\tau$ for precise physical time. If some data item $d$ comes to the system~\footnote{The moment when item is associated with its label by some front-end node} at time $\tau_0$ we write $\tau(d)=\tau_0$. The true order of events $\tau(d_1) > \tau(d_2)$ coming from different front-end nodes can be sometimes restored by their system timestamps $s(d_1)$ and $s(d_2)$. If these timestamps differ more than time synchronization slack then the order is clear: $s(d_1) > s(d_2) + \delta \Rightarrow \tau(d_1) > \tau(d_2)$.

This fact allows us to define \textit{global time} slots $t(d)$ such that $t(d) = [s(d) / \delta]$. This way we make global time less precise,  on one hand but this trick gives us ability to compare global time associated by different front-end nodes on the other. If global time of an item $d_1$ is greater by one than the global time of some other element $d_2$ then their order is defined despite the front-end nodes associated their labels:  $t(d_1) > t(d_2) + 1 \Rightarrow \tau(d_1) > \tau(d_2)$. This property forms a basis of our guarantees mechanism. Here is the summary of labeling mechanism:
\begin{enumerate}
    \item On receiving of input item, front-end node gets the system timestamp
    \item The system timestamp is shrunk up to synchronization slack (practically we use 10ms)
    \item The item is labeled by the result \textit{global time} and this label is sent to \tracker\ agent
\end{enumerate}

\subsection{Interface\ of\ \tracker\ }
\tracker\ agent has interface similar to Storm \acker, so that it receives \textit{ack} messages (\textit{label}/\textit{global time} together with random \textit{ack value}) and subscribe/unsuscribe messages that is used to manage list of notifications subscribers. 

However, \tracker\  interface has several distinctions. Firstly, \textit{ack} messages are extended with a \textit{segment} that is needed to provide dataflow-local notifications. Secondly, data sources (fronts) should send special messages called \textit{Heartbeats} that contain \textit{global time} if there were no elements with such \textit{global time}. These messages allows \tracker\ to send notifications even if some fronts did not produce records with a specific \textit{global time}. 

\begin{algorithm}
\caption{\tracker\ implementation sketch}
\label{tracker_algo}
\begin{algorithmic}[1]
\State $inputs \leftarrow configured\_inputs;$ 
\State $subscribers \leftarrow configured\_subscribers;$
\State $segments \leftarrow List[Segment]$
\State $segmentChecksums \leftarrow Map[Segment, List[Int64]]$
\State $segmentMinTime \leftarrow Map[Segment, GlobalTime]$
\State $frontHeartbeat \leftarrow Map[Front, GlobalTime]$
\\
\State \textbf{Upon} (segment, gt, ack\_val) $from \ in\in inputs$
\Indent
    \State $segmentChecksums[segment][gt] \gets $
    \par\Indent\Indent$segmentChecksums[segment][gt] \bigoplus checksum$\EndIndent\EndIndent
    \State $CheckMinTime$
\EndIndent
\\
\State \textbf{Upon} (gt, front) $from \ in\in inputs$
\Indent
 \State $frontHeartbeat[front] \leftarrow gt$
 \State $CheckMinTime$
\EndIndent
\\
\Procedure{CheckMinTime}{}
\State $minHeartbeat \leftarrow frontHeartbeat.values.min()$
\For {$segment \in segments$}
\State $checksums \gets segmentChecksums[segment]$
\State $maxTime \gets min($
\par\Indent\Indent$minHeartbeat,$\EndIndent\EndIndent
\par\Indent\Indent$segmentMinTime[segment.inbound].min(),$\EndIndent\EndIndent
\par\Indent$)$\EndIndent
\State $time \gets segmentMinTime[segment]$
\While{$time < maxTime$
\par\Indent$\And checksums[time] = 0$\EndIndent
\par}
\State $time \gets time + 1$
\EndWhile
\If{$segmentMinTime[segment] < time$}
\State $segmentMinTime[segment] \gets time$
\For {$subscriber \in subscribers$}
\State $subscriber.send(segment, time)$
\EndFor
\EndIf
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

Therefore, \tracker\ receives the following messages:
\begin{itemize}
    \item Ack message is a tuple of a \textit{global time}, \textit{segment}, and \textit{ack value} (random number)
    \item Heartbeat message is a pair of a \textit{global time} and a front-end id.
    \item (Un)Subscribe message
\end{itemize}

% On ack message the accumulator, associated with provided \textit{global time} is updated by the received \textit{ack value}. Due to commutation of xor operation the checksum change may contain either single ack or multiple acks xored to a single change. The heartbeat message from front-end guarantees that no more input items with such a \textit{global time} will be emitted by this front-end. Subscribe messages manage the list of notification receivers. 

\tracker\ sends the following messages to subscribers:
\begin{itemize}
    \item \textit{Min segment time update} for time $t$ when no items with the \textit{global time} less or equal $t$ exists in the segment
    \item \textit{Min global time update} for time $t$ when no items with the \textit{global time} less or equal $t$ exists in the system
\end{itemize}


When an accumulator, associated with \textit{global time}, becomes zero, this time is the minimum time in the acker table for this segment and all previous segments the \textit{min time update for segment} is emitted and send to all subscribers. If for all segments of a certain \textit{global time} the XOR result is zero, \tracker\ sends \textit{min global time update}. An implementation sketch of \tracker\ interface is shown in Algorithm~\ref{tracker_algo}.

% \subsection{Centralized \tracker\ }

% \tracker\ is implemented as an "actor" on a dedicated machine and is shared between all system workers. It encapsulates minimal times received from the system fronts and a cyclic buffer for storing tracked elements.

% Cyclic buffer stores checksums keyed by elements Global Time windows. It stores values in a fixed size range starting from current Minimal Global Time window. \tracker\ applies received Ack messages checksum changes to the corresponding value stored in the buffer.

% \tracker\ can tell that there will be no more elements in a specific Global Time window when two conditions are fulfilled: no fronts will emit any more elements within this Global Time window and the cyclic buffer stores zero for it. Every time \tracker\ receives Acks or Heartbeats, it checks these conditions. If there are any matching windows \tracker removes them from the beginning of the buffer and broadcasts an update with the last of the windows.

% \begin{algorithm}
% \caption{\tracker}
% \begin{algorithmic}[1]
% \Procedure{HandleAck}{$time, checksum$}
% \State $checksums[time] \gets checksums[time] \bigoplus checksum$
% \State $CheckMinTime$
% \EndProcedure
% \\
% \Procedure{HandleHeartbeat}{$time$}
% \State $heartbeat \gets time$
% \State $CheckMinTime$
% \EndProcedure
% \\
% \Procedure{CheckMinTime}{}
% \State $time \gets previousMinTime$
% \While{$time < heartbeat \And checksums[time] = 0$}
% \State $time \gets time + 1$
% \EndWhile
% \If{$previousMinTime < time$}
% \State $previousMinTime \gets time$
% \State $SendMinTimeUpdate(time)$
% \EndIf
% \EndProcedure
% \end{algorithmic}
% \end{algorithm}

% To prevent false Min Time Updates this implementation has two requirements on order of processing:

% \begin{itemize}
% 	\item Initial Ack message emitted from front with an input element must be processed before the following Heartbeat message.
% 	\item When operator processes an incoming element Ack messages of elements produced must be processed before an Ack message for incoming element.
% \end{itemize}

% Ordering of messages per sender-receiver pair given to us by Akka framework fulfils these requirements.

\subsection{Traffic optimizations: local \tracker\ }

As \tracker\ communicates with the rest of the system via the network sending Ack messages on every sent or received item, generates a lot of network traffic. A Local \tracker\ is introduced to reduce this traffic. It is deployed at every worker machine and serves as a mediator between it and the \tracker. It buffers incoming Ack and Heartbeat messages and flushes them periodically. The flushing window can be configured and  is an upper bound notifications an overhead on latency from Local \tracker.

\tracker\ agggregates Heartbeat messages to the latest ones per front. \tracker\ aggregates Ack messages within the same Global Time windows to a single Ack message via XOR product. On flush Local \tracker\ sends these messages in an ordered batch: Ack messages come first in order of decreasing Global Time and Heartbeat messages come second.

This aggregation does not break the requirements for order of processing:
\begin{itemize}
	\item \tracker\ processes Ack messages before Heartbeat messages.
	\item \tracker\ processes Ack messages for produced elements before Ack messages for incoming messages as their Global Time is larger.
\end{itemize}

\subsection{Distributed \tracker\ }

As \tracker\ is being shared between all system machines, it is possible for it to become a system bottleneck and prevent scaling. When this becomes a problem \tracker\ and messages sent to it can be sharded:
\begin{itemize}
	\item Ack messages are sharded by Global Time, as it is the same for received and sent items.
	\item Heartbeat messages could be simply broadcasted as their number is less than the number of Ack messages and most likely much less, but still they are also sharded with respect to their meaning that front will not emit items with a preceding Global Time anymore. Considering that Ack messages with next Global Times will be sent to specific \tracker\ shards, for some \tracker\ shards Heartbeat messages Global Times can be rounded up to reduce the number of messages being sent on this Global Time updates.
\end{itemize}

Min Time Update messages received from shards should be aggregated to minimum of Global Times from them which would be a minimal Global Time for the whole system.

Still this sharding mechanism can cause false notifications in case of replay in Grouping operators. Let our system current Minimal Global Time be $t$, two elements with Global Times $t$ and $(t+1)$ have entered the system with Global Times being sharded to first and second \tracker\ shards accordingly, element $(t+1)$ is completely processed ahead of $t$ and has a zero checksum in second \tracker\ shard, while element $t$ is still in flight and has a non-zero checksum in first \tracker\ shard. When element $t$ is being processed it emits Ack messages with checksum change for Global Times $t$ and $(t+1)$. First Ack should change \tracker\ checksum to zero, while the second one should change its checksum from zero. As they are being sent to different shards, an order of their processing is undefined. If the first Ack message will reach its shard and make it emit an incrementing Min Time Update before the second one emits a decrementing Min Time Update then a \tracker\ listener will prematurely count both $t$ and $(t+1)$ as processed.

This problem is solved using the vector clock algorithm. To make sure that both Ack messages are processed they are being followed with a broadcast of incremented clock time of the worker machine, Node Time. \tracker collects a vector of Node Times from all worker machines, attaches it to Min Time Updates and only sends them when Node Times vector changes. Min Time Update listeners collect vectors of Node Times from all acker machines and do not accept a Min Time Update message if its Node Times vector is not less than or equal to element-wise minimum of collected vectors.

This solves the described problem, as the Min Time Update from second shard will not be accepted without accepting the Min Time Update from first shard and the resulting Min Time will be $(t+1)$.

% \subsection{Operation-level tracking}

% Our tracking mechanism can be made granular, tracking different parts of pipeline in case of directed acyclic graphs. For this we enrich Global Times with a pipeline identifier and change \tracker\ to track each pipeline part checksums in a separate buffer and limit minimal time of a pipeline part with minimal times of pipeline parts incoming into it. These changes make it possible for \tracker\ to emit Minimal Time Update messages for pipeline parts.
