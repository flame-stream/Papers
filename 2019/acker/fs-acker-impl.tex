\label {fs-acker-impl}

In the previous section, we introduced a general schema of the \tracker. In this section, we deepen into its implementation details. In the first section, we formally define the \textit{global time} labeling and explore its properties. The second section is dedicated to the \tracker\ agent messaging structure. The next section focuses on the single node implementation of the \tracker\ agent and local \tracker\ optimization. Distributed implementation of the \tracker\ concludes this part.

\subsection{Global time}
In Section~\ref{sec:acker-analysis}, we have introduced the notation of \textit{global time}, but the details of its practical implementation remain behind the scene. We can use time oracle~\cite{10.14778/3055330.3055335}, but in this case, the system will have a single point of failure, and we decided to make our system tolerant to the time difference between font-end. In this case, the comparison between timestamps coming from different front nodes is limited, and the \textit{global time} nice properties are under threat. To overcome this difficulty, we systematically synchronize the system time of the front-nodes and assume that this measure prevents the system time difference more than some fixed $\delta$, which we reference as synchronization slack.

To work with the guarantees formally we need to define suitable notation. We use $\tau$ for precise physical time. If some data item $d$ comes to the system~\footnote{The moment when item is associated with its label by some front-end node} at time $\tau_0$ we write $\tau(d)=\tau_0$. The true order of events $\tau(d_1) > \tau(d_2)$ coming from different front-end nodes can be sometimes restored by their system timestamps $s(d_1)$ and $s(d_2)$. If these timestamps differ more than time synchronization slack then the order is clear: $s(d_1) > s(d_2) + \delta \Rightarrow \tau(d_1) > \tau(d_2)$.

This fact allows us to define \textit{global time} slots $t(d)$ such that $t(d) = [s(d) / \delta]$. This way we make global time less precise,  on one hand but this trick gives us ability to compare global time associated by different front-end nodes on the other. If global time of an item $d_1$ is greater by one than the global time of some other element $d_2$ then their order is defined despite the front-end nodes associated their labels:  $t(d_1) > t(d_2) + 1 \Rightarrow \tau(d_1) > \tau(d_2)$. This property forms a basis of our guarantees mechanism. Here is the summary of labeling mechanism:
\begin{enumerate}
    \item On receiving of input item, front-end node gets the system timestamp
    \item The system timestamp is shrunk up to synchronization slack (practically we use 10ms)
    \item The item is labeled by the result \textit{global time} and this label is sent to \tracker\ agent
\end{enumerate}

\subsection{Interface\ of\ \tracker\ }
\tracker\ agent has interface similar to Storm \acker, so that it receives \textit{ack} messages (\textit{label}/\textit{global time} together with random \textit{ack value}) and subscribe/unsuscribe messages that is used to manage list of notifications subscribers. 

However, \tracker\  interface has several distinctions. Firstly, \textit{ack} messages are extended with a \textit{segment} label that is needed to provide dataflow-local notifications. Secondly, data sources (fronts) should send special messages called \textit{heartbeats} that contain \textit{global time} if there were no elements with such \textit{global time}. These messages allows \tracker\ to send notifications even if some fronts did not produce records with a specific \textit{global time} due to low traffic. 

\begin{algorithm}
\caption{\tracker\ implementation sketch}
\label{tracker_algo}
\begin{algorithmic}[1]
\State $inputs \leftarrow configured\_inputs;$ 
\State $subscribers \leftarrow configured\_subscribers;$
\State $segments \leftarrow List[Segment]$
\State $segmentChecksums \leftarrow Map[Segment, List[Int64]]$
\State $segmentMinTime \leftarrow Map[Segment, GlobalTime]$
\State $frontHeartbeat \leftarrow Map[Front, GlobalTime]$
\\
\State \textbf{Upon} (segment, gt, ack\_val) $from \ in\in inputs$
\Indent
    \State $segmentChecksums[segment][gt] \gets $
    \par\Indent\Indent$segmentChecksums[segment][gt] \bigoplus checksum$\EndIndent\EndIndent
    \State $CheckMinTime$
\EndIndent
\\
\State \textbf{Upon} (gt, front) $from \ in\in inputs$
\Indent
 \State $frontHeartbeat[front] \leftarrow gt$
 \State $CheckMinTime$
\EndIndent
\\
\Procedure{CheckMinTime}{}
\State $minHeartbeat \leftarrow frontHeartbeat.values.min()$
\For {$segment \in segments$}
\State $checksums \gets segmentChecksums[segment]$
\State $maxTime \gets min($
\par\Indent\Indent$minHeartbeat,$\EndIndent\EndIndent
\par\Indent\Indent$segmentMinTime[segment.inbound].min(),$\EndIndent\EndIndent
\par\Indent$)$\EndIndent
\State $time \gets segmentMinTime[segment]$
\While{$time < maxTime$
\par\Indent$\And checksums[time] = 0$\EndIndent
\par}
\State $time \gets time + 1$
\EndWhile
\If{$segmentMinTime[segment] < time$}
\State $segmentMinTime[segment] \gets time$
\For {$subscriber \in subscribers$}
\State $subscriber.send(segment, time)$
\EndFor
\EndIf
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

Therefore, \tracker\ receives the following messages:
\begin{itemize}
    \item Ack message is a tuple of a \textit{global time}, \textit{segment}, and \textit{ack value} (random number)
    \item Heartbeat message is a pair of a \textit{global time} and a front-end id.
    \item (Un)Subscribe message
\end{itemize}

% On ack message the accumulator, associated with provided \textit{global time} is updated by the received \textit{ack value}. Due to commutation of xor operation the checksum change may contain either single ack or multiple acks xored to a single change. The heartbeat message from front-end guarantees that no more input items with such a \textit{global time} will be emitted by this front-end. Subscribe messages manage the list of notification receivers. 

\tracker\ sends the following messages to subscribers:
\begin{itemize}
    \item \textit{Min segment time update} for time $t$ when no items with the \textit{global time} less or equal $t$ exists in the segment and all preceding segments are complete
    \item \textit{Min global time update} for time $t$ when no items with the \textit{global time} less or equal $t$ exists in the system
\end{itemize}

\textit{Min time update for segment} is emitted and send to all subscribers when:
\begin{enumerate}
    \item the \textit{heartbeat} for this time is received from all front-end nodes;
    \item accumulators for all \textit{global times} not greater than this time plus one\footnote{The condition from the previous section requires the completeness of the next global time segment to preserve the order of the input elements in the notifications stream} are nullified;
    \item upstream segments for this \textit{global time} have already sent \textit{min time updates}.
\end{enumerate}
If accumulators for all segments of a certain \textit{global time} become zero, \tracker\ sends \textit{min global time update}. An implementation sketch of \tracker\ interface is shown in Algorithm~\ref{tracker_algo}.

% \subsection{Centralized \tracker\ }

% \tracker\ is implemented as an "actor" on a dedicated machine and is shared between all system workers. It encapsulates minimal times received from the system fronts and a cyclic buffer for storing tracked elements.

% Cyclic buffer stores checksums keyed by elements Global Time windows. It stores values in a fixed size range starting from current Minimal Global Time window. \tracker\ applies received Ack messages checksum changes to the corresponding value stored in the buffer.

% \tracker\ can tell that there will be no more elements in a specific Global Time window when two conditions are fulfilled: no fronts will emit any more elements within this Global Time window and the cyclic buffer stores zero for it. Every time \tracker\ receives Acks or Heartbeats, it checks these conditions. If there are any matching windows \tracker removes them from the beginning of the buffer and broadcasts an update with the last of the windows.

% \begin{algorithm}
% \caption{\tracker}
% \begin{algorithmic}[1]
% \Procedure{HandleAck}{$time, checksum$}
% \State $checksums[time] \gets checksums[time] \bigoplus checksum$
% \State $CheckMinTime$
% \EndProcedure
% \\
% \Procedure{HandleHeartbeat}{$time$}
% \State $heartbeat \gets time$
% \State $CheckMinTime$
% \EndProcedure
% \\
% \Procedure{CheckMinTime}{}
% \State $time \gets previousMinTime$
% \While{$time < heartbeat \And checksums[time] = 0$}
% \State $time \gets time + 1$
% \EndWhile
% \If{$previousMinTime < time$}
% \State $previousMinTime \gets time$
% \State $SendMinTimeUpdate(time)$
% \EndIf
% \EndProcedure
% \end{algorithmic}
% \end{algorithm}

% To prevent false Min Time Updates this implementation has two requirements on order of processing:

% \begin{itemize}
% 	\item Initial Ack message emitted from front with an input element must be processed before the following Heartbeat message.
% 	\item When operator processes an incoming element Ack messages of elements produced must be processed before an Ack message for incoming element.
% \end{itemize}

% Ordering of messages per sender-receiver pair given to us by Akka framework fulfils these requirements.

\subsection{Single node \tracker\ implementation}
The naive version of the \tracker\ agent can be implemented as a single node agent. In this case the logic directly follows the Algorithm~\ref{tracker_algo}. Though, due to commutative nature of xor operation we are able to optimize incoming traffic aggregating ack messages before sending them to acker agent. For each worker node in the system we introduce a Local \tracker\ component. It serves as a mediator between the node and the \tracker, buffering the outgoing ack and \textit{heartbeat} messages and flushing them periodically. The flushing window is the parameter that allows to balance between the system latency and the service traffic. The optimal value of this parameter is defined by the execution graph topology and timing of particular operations.

\tracker\ use the latest \textit{heartbeat} message for each front and aggregates ack messages within the same \textit{global time} window to a single ack message via XOR product. On a flush, Local \tracker\ sends these messages in an ordered batch: ack messages come in order of associated \textit{global time} and \textit{heartbeat} messages come after them.

\subsection{Distributed \tracker}

As \tracker\ agent accumulates all the service traffic from entire system it may become a bottleneck. To deal with this problem we introduce a distributed version of the \tracker\ agent. The challenge here is to support the ability of the user defined code to emit elements of \textit{global time} greater than the current. Otherwise the problem becomes trivial as we can shard the \tracker\ agent by the \textit{global time} and all acker chains will use the same shard along their path and the completion of certain \textit{global time} is permanent. In this case we can send \textit{min time update} based on information from particular shard.

If this is not the case and user is allowed to spawn items with the time grater than the time of the input element it is possible that already nullified \textit{global time} will become active again. In this case we need to take into account all \tracker\ shards to send the \textit{min time update} event.

We employ the vector clock algorithm to resolve this issue. Each worker machine either periodically or with flush of the \textit{local acker} sends its system time to all acker shards. The vector of an observed worker times accompanies all \tracker\ notification events. The acker table is built on \textit{min time update} event receivers side. Each row in this table is associated with vector of worker times. All conditions in the list for accepting \textit{min time update} now check that the associated time vector is not less than the vector of compared time. For example we need to check that all accumulators of the same segment become zero \textit{and} their vector is component-wise not greater than the current one.

The vector clock introduction increase the service traffic, but allows to eliminate bottleneck from the system. The service traffic complexity remains the same, but the $O$ factor increases. In experimental section we will study how big is this increase.

% \subsection{Operation-level tracking}

% Our tracking mechanism can be made granular, tracking different parts of pipeline in case of directed acyclic graphs. For this we enrich Global Times with a pipeline identifier and change \tracker\ to track each pipeline part checksums in a separate buffer and limit minimal time of a pipeline part with minimal times of pipeline parts incoming into it. These changes make it possible for \tracker\ to emit Minimal Time Update messages for pipeline parts.
