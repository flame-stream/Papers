\label {fs-acker-motivation}

Typically, distributed stream processing systems are shared-nothing runtimes which continuously ingest input elements, transform them according to a dataflow graph, and deliver output elements. The dataflow graph consists of operators. We assume that each operator processes items one-by-one so that it can handle only one record at a time. Operators can be stateless or stateful. An output element of an operator may depend on the current state as well as on the corresponding input element. Commonly, all operators are partitioned among workers. In most cases, a partition of the next operator for a record can be determined only in runtime because it may depend on the record payload. 

Streaming operators are often a black box and may contain complex user-provided code: they can produce multiple descendant elements from a single input, filter out some items, or do various transformations according to their current state. Each descendant of an element can be handled by different partitions. In general, it is hard to figure out which input element originated an output without extra information.

% Let us illustrate this problem with an example. Suppose that we implement an online machine learning algorithm: each input record (new example for training) is somehow preprocessed and then updates the machine learning model. Each update causes output of the new model parameters, e.g., weights matrix in case of regularized logistic regression~\cite{mcmahan2013ad}. The preprocessing step can be complicated and usually require multiple variously partitioned operators~\cite{morales2015samoa}. Therefore, having only an output element (weights matrix), it is not possible to determine which raw input record triggered the model update.  

{\em Dependency tracking} is a process of matching streaming elements with its original input items. Typically, this mechanism can provide for system notifications when all descendants of some input elements are entirely or partially processed. A good dependency tracking method should support the following features:
\begin{itemize}
    % \item {\bf Low notification latency}: as we discuss further, notification latency can directly influence the latency of regular processing. For instance, to ensure {\em exactly-once}, the system should output elements only if there is a guarantee that the corresponding set of input elements have been entirely processed~\cite{thepaper}.
    \item {\bf Fine granularity}: a dependency tracking mechanism should be able to provide notifications for individual input elements or small sets of them. The fine granularity of tracking reduces the processing latency of some applications~\cite{we2018adbis} but can induce an overhead on throughput.
    \item {\bf Dataflow locality}: some methods can provide notifications for small parts or {\em segments} of the execution graph, while others can only indicate when input records went through the whole dataflow. Segment-level or even operator-level tracking may significantly improve the performance of distributed state snapshotting~\cite{Carbone:2017:SMA:3137765.3137777, 2015arXiv150608603C}.
    \item {\bf Notifications order preservation}: if element $a$ entered a system before element $b$, notification that element $b$ has been entirely processed should be generated after notification for $a$, even if $a$ had been processed earlier. This property is required for state snapshotting algorithms to ensure exactly-once guarantee~\cite{2015arXiv150608603C}.
    \item {\bf Cyclic dataflows support}: many popular iterative algorithms, including graph analysis~\cite{xu2016efficient} and machine learning models training~\cite{morales2015samoa}, require cycles in a dataflow. Suitability for cyclic dataflows makes a dependency tracking method applicable for streaming systems that intend to support such algorithms.
    \item {\bf Network traffic overhead}: dependency tracking may induce extra network traffic. This traffic may depend on the granularity of tracking, the number of computational nodes, and the dataflow graph size. It may induce a performance overhead on regular processing.
\end{itemize}

Further, in this section, we overview several practical problems that require a dependency tracking mechanism. We demonstrate that this mechanism plays a crucial role in obtaining the correct and consistent processing results. After that, we discuss tracking techniques adopted in state-of-the-art stream processing systems and identify their properties and limitations. 

\begin{table*}
    \caption{Overview of existing dependency tracking solutions}
    \label{solutions-overview-table}
    \begin{threeparttable}
        \centering
        \begin{tabular}{|>{\bfseries}c|c|c|c|c|c|} 
          \hline
          Method & Granularity & Locality & Notifications Order Preservation & Cyclic dataflows & Network traffic  \\ \hline \hline
          Micro-batching & Fine$^*$ & Operator/Segment-level & + & + & $O(\frac{NC^2}{G})$ \\ \hline
          Markers & Fine$^*$ & Operator/Segment-level & + & +/- & $O(\frac{NC^2}{G})$ \\ \hline
          Naiad tracking & Fine$^*$ & Operator/Segment-level & - & + & $O(NC^2)$ \\ \hline
          \acker\ & Fine & Dataflow-level & - & + & $O(N+C)$ \\ \hline
          \tracker\ & Fine & Operator/Segment-level & + & + & $O(N+C)$ \\ \hline
        \end{tabular}
        * can be inefficient due to a large amount of extra traffic
    \end{threeparttable}
\end{table*}

\subsection{Completeness monitoring}

A particular use-case of completeness monitoring is transactional processing. If input elements are split into multiple ones (e.g. within a flatmap), and system processes them independently, a user often assumes that the descendants will be handled atomically. In other words, if a system loses a single element, other items that depend on the same input should not affect a system state. 

To process input items transactionally, a system must check that all descendants of an input item are successfully processed or rollback changes caused by survived elements otherwise. One can solve this problem with dependency tracking. If there is no notification that all descendants of an input item are completely processed before a timeout, a system can initiate a recovery mechanism.

Another application of completeness monitoring is a cleanup of data producers. Typically, input elements are stored in persistent queues (e.g., Kafka~\cite{kreps2011kafka}) to be reprocessed in case of system failures. Items cannot be stored in these queues forever due to memory and disk limitations. A dependency tracking technique can trigger the cleanup of these queues when it observes that some input elements are entirely processed and delivered, so there is no need to store them any further.

\subsection{State snapshotting}
Many streaming systems, including Flink~\cite{Carbone:2017:SMA:3137765.3137777} and Heron~\cite{Kulkarni:2015:THS:2723372.2742788}, apply state snapshotting mechanisms to ensure fault tolerance. The main idea behind this method is to periodically save the overall system state (states of all operators) to persistent storage. In case of a failure, the system rolls back a state from storage and reprocesses missed input elements. 

The main problem here is to determine which input elements should be reprocessed after a failure. If an input element has already {\em affected} the state, reprocessing of this element can make state inconsistent. The tricky point is that the input element can affect the state {\em partially}: some descendants can modify the state, while others can be missed. If a system takes snapshot at such moment, it will not be able to consistently recover it after a failure and will not provide {\em exactly once} or {\em at least once} delivery guarantee~\cite{2015arXiv150608603C, Carbone:2017:SMA:3137765.3137777}.

To prevent such inconsistency, a system can take state snapshot when some specific set of input records have been entirely processed, including all descendant records~\cite{2015arXiv150608603C, thepaper}. Streaming systems use dependency tracking techniques to determine these moments. For instance, the tracking mechanism can notify each operator when it is safe to save its local state. The system commits global snapshot when all operators successfully saved their states affected by certain input elements only. Therefore, in case of a failure, a streaming engine can safely reprocess missed input records that did not affect the snapshot.

The problem of state snapshotting is quite similar to completeness monitoring in general but has two significant distinctions. Firstly, the dataflow locality matters. An operator can snapshot its local state when there is a guarantee that all operators up to this one in a dataflow have entirely processed the specific set of input elements. Hence, if the dependency tracking mechanism is able to provide the operator-level locality, it may significantly reduce the performance overhead on state snapshotting~\cite{Carbone:2017:SMA:3137765.3137777}. Secondly, this problem requires {\em notification order preservation} property. The order of notifications should not contradict the order of input elements arrival. Otherwise, elements which entered a system later but have been quickly processed may be included in the snapshot, while earlier arrived but stuck records may not be included. Such behavior can lead to inconsistent state snapshots and the violation of exactly-once guarantee~\cite{2015arXiv150608603C}. 

\subsection{Existing solutions} \label{existing_solutions}

In this section, we describe the properties of existing dependency tracking mechanisms and make an overview of their main advantages and restrictions. We also analyze their network traffic overhead in terms of the following parameters: computational units number (\textbf{C}), dataflow size (\textbf{N}), and granularity (\textbf{G}). Properties overview and performance analysis are summarized in Table~\ref{solutions-overview-table}.

\subsubsection{Micro-batching}

The micro-batching model applied in Spark Streaming~\cite{Zaharia:2012:DSE:2342763.2342773} and Storm Trident~\cite{apache:storm:trident} can be viewed as a dependency tracking mechanism. Within this model, system groups all input elements into small sets called micro-batches. The system sequentially and atomically processes them. This property ensures notifications order preservation. 

The system monitors the completeness of processing using {\em lineage} that traces all transformations within the execution graph. In case of failure, a current micro-batch can be entirely or partially (depends on a lineage structure) reprocessed because it is guaranteed that an element in a batch may depend only on another record in the same batch. The system can snapshot the state after each successfully handled micro-batch. 

The micro-batching model is commonly applied for both completeness monitoring and state snapshotting tasks.  It supports operator level locality as well as cyclic dataflows~\cite{meng2016mllib}. 

However, the size of a micro-batch defines the granularity of tracking. Hence, a limitation of this method is the coarse granularity of tracking due to the ineffectiveness of very small ($G << C^2$) micro-batches~\cite{Zaharia:2012:DSE:2342763.2342773}. One reason behind this ineffectiveness is the substantial network traffic overhead that is $O(\frac{NC^2}{G})$, because, on each computational stage, an operator must inform the next operator on all computational nodes when the batch is processed~\footnote{That is true only for reduce-like operators, but we assume that their number is $O(N)$}.

\subsubsection{Markers}

Another popular approach bases on punctuations~\cite{Tucker:2003:EPS:776752.776780}. The main idea is to inject special input elements called {\em markers} or {\em checkpoint items} into a system. These items flow through the same network channels and divide groups of ordinary input records. All transformations of elements, filterings, and splits are bounded between consecutive markers. Therefore, a streaming element can depend only on the input item that arrived between the surrounding markers. 

If an operator receives a marker, it is guaranteed that this operator has already received all descendants of certain input records. Hence, this event can initiate state snapshotting. When a marker reaches the end of a pipeline, it indicates that the system has entirely processed input elements that preceded the marker. This technique is practically adopted for both completeness monitoring and state snapshotting problems in Flink~\cite{Carbone:2017:SMA:3137765.3137777}, Heron~\cite{Kulkarni:2015:THS:2723372.2742788}, and IBM Streams~\cite{jacques2016consistent}. 

Markers approach is quite similar to the micro-batching technique. It preserves the order of notifications and provides a fine granularity of tracking. However, markers can be applied only for a limited subset of cyclic dataflows~\cite{carbone2018scalable}. 

The frequency of markers injection determines the granularity of this technique. The amount of service traffic for this method is $O(\frac{NC^2}{G})$. It is quadratic in the number of computational nodes as markers must be broadcasted to all downstream nodes after each dataflow operator. 

If the system injects marker after each input record, it can determine the exact input element by corresponding outputs. Otherwise, it is only possible to determine a set of input records that contains the item which originated outputs. As we demonstrate further, this method adds significant performance overhead on regular processing in case of fine-grained tracking.

\subsubsection{Naiad tracking}

Naiad~\cite{Murray:2013:NTD:2517349.2522738} uses its own mechanism for tracking the progress of iterative computations. Within this method, each data item in a system is assigned with an {\em epoch} and a vector of logical timestamps called {\em loop counter}. Epoch is a user-provided time of an element generation. The value on the $i$th position of the loop counter indicates the number of times this element went through the $i$th {\em loop context} (cycle) in a dataflow. Each processing worker monitors for the items and their timestamps and shares this information with other workers. Worker broadcasts notifications when there is a guarantee that all elements reach some iteration number, or all elements from an epoch are entirely processed.

This method was originally designed for fine-grained tracking with a segment-level locality. However, its amount of service network traffic is $O(NC^2)$, which is also demonstrated by the experiments~\cite{Murray:2013:NTD:2517349.2522738}.

% The main limitation of the mentioned method is extra network usage. The protocol used in Naiad causes the substantial number of extra network messages that quadratically depend on the number of workers, despite the optimizations~\cite{Murray:2013:NTD:2517349.2522738}. 

% Moreover, the tracking agents must be deployed together with workers to ensure the correctness of this approach so that it may induce additional overhead on regular processing.

\subsubsection{Acker}

\acker\ is a mechanism for completeness tracking employed in Apache Storm~\cite{apache:storm}. The main idea is to enrich each data item along the dataflow with a random number that helps tracking processing progress. When an element is transformed, its descendants obtain new random numbers. Each time an item is sent or received, a system forwards its number to a special agent called {\em \acker}. Acker XORs all received numbers into the checksum. If all elements have been successfully processed, the checksum will be zero~\footnote{XOR operation is commutative, and as we send each number twice, the entire combination gives zero in the result.}.

Acker is suitable for cyclic dataflows and can be efficiently used for fine-grained tracking because its amount of extra service traffic is $O(N+C)$ that is less than other methods induce. However, it does not support operator or segment-level locality of tracking. Another limitation is the lack of notifications order preservation that makes \acker\ not suitable for state snapshotting problem.

In this paper, we build our dependency tracking mechanism based on the same idea. We substantially extend it to support segment-level locality of processing, be scalable, and more efficient in terms of network traffic. We also adopt it for state snapshotting problem by enforcing notifications order preservation. In the next section, we discuss details about \acker\ and our method called \tracker .