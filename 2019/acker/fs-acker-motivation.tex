\label {fs-acker-motivation}

Typically, distributed stream processing systems are shared-nothing runtimes which continuously ingest input elements, transform them according to a dataflow graph, and deliver output elements. The dataflow graph consists of operators. We assume that each operator processes items one-by-one so that it can handle only one record at a time. Operators can be stateless or stateful. An output element of an operator may depend on the current state as well as on the corresponding input element. Commonly, all operators are partitioned among workers. In most cases, a partition of the next operator for a record can be determined only in runtime because it may depend on the record payload. 

Streaming operators are often a black box and may contain complex user-provided code: they can produce multiple descendant elements from a single input, filter out some items, or do various transformations according to their current state. Each descendant of an element can be handled by different partitions. In general, it is hard to figure out which input element originated an output without extra information.

% Let us illustrate this problem with an example. Suppose that we implement an online machine learning algorithm: each input record (new example for training) is somehow preprocessed and then updates the machine learning model. Each update causes output of the new model parameters, e.g., weights matrix in case of regularized logistic regression~\cite{mcmahan2013ad}. The preprocessing step can be complicated and usually require multiple variously partitioned operators~\cite{morales2015samoa}. Therefore, having only an output element (weights matrix), it is not possible to determine which raw input record triggered the model update.  

{\em Dependency tracking} is a process of matching streaming elements with its original input items. Typically, this mechanism can provide for system notifications when all descendants of some input elements are entirely or partially processed. A good dependency tracking method should support the following features:
\begin{itemize}
    % \item {\bf Low notification latency}: as we discuss further, notification latency can directly influence the latency of regular processing. For instance, to ensure {\em exactly-once}, the system should output elements only if there is a guarantee that the corresponding set of input elements have been entirely processed~\cite{thepaper}.
    \item {\bf Fine granularity}: a dependency tracking mechanism should be able to provide notifications for individual input elements or small sets of them. The fine granularity of tracking reduces the processing latency of some applications~\cite{we2018adbis} but can induce an overhead on throughput.
    \item {\bf Dataflow locality}: some methods can provide notifications for small parts or {\em blocks} of the execution graph, while others can only indicate when input records went through the whole dataflow. Block-level or even operator-level tracking may significantly improve the performance of distributed state snapshotting~\cite{Carbone:2017:SMA:3137765.3137777, 2015arXiv150608603C}.
    \item {\bf Notifications order preservation}: if element $a$ entered a system before element $b$, notification that element $b$ has been entirely processed should be generated after notification for $a$, even if $a$ had been processed earlier. This property is needed for state snapshotting algorithms to ensure exactly-once guarantee~\cite{2015arXiv150608603C}.
    \item {\bf Network traffic overhead}: dependency tracking technique may induce extra network traffic. This traffic may depend on the number of computational nodes and logical graph size. If the amount of this traffic is huge, it may induce a performance overhead on regular processing.
\end{itemize}

Further, in this section, we overview several practical problems that require a dependency tracking mechanism. We demonstrate that this mechanism plays a crucial role in obtaining the correct and consistent processing results. After that, we discuss tracking techniques adopted in state-of-the-art stream processing systems and identify their properties and limitations. 

\subsection{Completeness monitoring}

A particular use-case of completeness monitoring is transactional processing. If input elements are split and system processes the split results independently, a user often assumes that the descendants will be handled atomically. In other words, if a system loses a single element, other items that depend on the same input should not affect a system state. 

To process input items transactionally, a system must check that all descendants of an input item are not lost or rollback changes caused by survived elements otherwise. One can solve this problem with dependency tracking. If there is no notification that all descendants of an input item are completely processed before timeout, a system can initiate a recovery mechanism.

Another application of completeness monitoring is a cleanup of data producers. Typically, input elements are stored in persistent queues (e.g., Kafka~\cite{kreps2011kafka}) to be reprocessed in case of system failures. Items cannot be stored in these queues forever due to memory and disk limitations. A dependency tracking technique can trigger a cleanup of these queues when it observes that some input elements are entirely processed and delivered, so there is no need to store them any further.

\subsection{State snapshotting}
Many streaming systems, including Flink~\cite{Carbone:2017:SMA:3137765.3137777} and Heron~\cite{Kulkarni:2015:THS:2723372.2742788}, apply state snapshotting mechanisms to ensure fault tolerance. The main idea behind this method is to periodically save the global system state (states of all operators) to persistent storage. In case of a failure, the system rolls back a state from storage and reprocesses missed input elements. 

The main problem here is to determine which input elements should be reprocessed after a failure. If an input element has already {\em affected} the system state, reprocessing of this element will make state inconsistent. The tricky point is that the input element can affect the system state {\em partially}: some descendants can affect the state, while others can be missed. If a system takes snapshot state at such moment, it will not be able to recover it after a failure consistently~\cite{2015arXiv150608603C} and will not provide {\em exactly once} or {\em at least once} delivery guarantee.

To prevent such inconsistency, a system can snapshot state when some specific set of input records have entirely affected it, including all descendant records~\cite{2015arXiv150608603C, thepaper}. Streaming systems use dependency tracking techniques to determine these moments. For instance, the tracking mechanism can notify each operator when it is safe to save its local state. The system commits global snapshot when all operators successfully saved their states affected by certain input elements only. Therefore, in case of a failure, a streaming engine can safely reprocess missed input records that did not affect the snapshot.

The problem of state snapshotting is quite similar to completeness monitoring in general but has two significant distinctions. Firstly, the dataflow locality matters. An operator can snapshot its local state when there is a guarantee that all operators up to this one in a dataflow have entirely processed the specific set of input elements. Hence, if the dependency tracking mechanism is able to provide the operator-level locality, it may significantly reduce the performance overhead on state snapshotting~\cite{Carbone:2017:SMA:3137765.3137777}. Secondly, this problem requires {\em notification order preservation} property. The order of notifications should not contradict the order of input elements arrival. Otherwise, elements which entered a system later but have been quickly processed may be included in the snapshot, while earlier arrived but stuck records may not be included. Such behavior can lead to inconsistent state snapshots and the violation of exactly-once guarantee~\cite{2015arXiv150608603C}. 

\subsection{Existing solutions} \label{existing_solutions}

\subsubsection{Micro-batching}

The micro-batching model applied in Spark Streaming~\cite{Zaharia:2012:DSE:2342763.2342773} and Storm Trident~\cite{apache:storm:trident} can be viewed as a dependency tracking mechanism. Within the micro-batching model, system groups all input elements into small sets called micro-batches. System sequentially and atomically processes micro-batches. It monitors the completeness of processing using {\em lineage} that traces all transformations within the execution graph. In case of failure, a current micro-batch can be entirely or partially (depends on a lineage structure) reprocessed because it is guaranteed that an element in a batch may depend only on another record in the same batch. The system can snapshot the state after each successfully handled micro-batch. 

Therefore, the micro-batching model is applied for both completeness monitoring and state snapshotting tasks. Block-level or operator level locality can be easily achieved within micro-batching. However, the size of a micro-batch defines the granularity of tracking. Hence, a limitation of this method is the coarse granularity of tracking due to the ineffectiveness of very small micro-batches~\cite{Zaharia:2012:DSE:2342763.2342773}.

\subsubsection{Markers}

Another popular approach bases on punctuations~\cite{Tucker:2003:EPS:776752.776780}. The main idea is to inject special input elements called {\em markers} or {\em checkpoint items} into a system. These items flow through the same network channels and divide groups of ordinary input records. Therefore, a streaming element can depend only on the input item that arrived between the surrounding markers. If an operator receives a marker, it is guaranteed that this operator has already received all descendants of certain input records. Hence, this event can initiate state snapshotting. When a marker reaches the end of a pipeline, it indicates that the system has entirely processed input elements which preceded the marker. This technique is practically adopted for both completeness monitoring and state snapshotting problems in Flink~\cite{Carbone:2017:SMA:3137765.3137777}, Heron~\cite{Kulkarni:2015:THS:2723372.2742788}, and IBM Streams~\cite{jacques2016consistent}. 

While markers approach is quite similar to the micro-batching technique, it provides a higher locality of tracking because of independent notifications for different parts of a pipeline. However, markers can be applied only for a limited subset of cyclic dataflows~\cite{carbone2018scalable}. The frequency of markers injection determines the granularity of this technique. If the system injects marker after each input record, it can determine the exact input element by corresponding outputs. Otherwise, it is only possible to determine a set of input records that contains the item which originated outputs. 

However, the number of extra traffic quadratically depends on the number of computational units and linearly depends on the dataflow size. As we demonstrate further, this method adds significant performance overhead on regular processing in case of fine-grained tracking.

\subsubsection{Naiad tracking}

Naiad~\cite{Murray:2013:NTD:2517349.2522738} uses its own mechanism for tracking the progress of iterative computations. Within this method, each data item in a system is assigned with an {\em epoch} and a vector of logical timestamps called {\em loop counter}. Epoch is a user-provided time of an element generation. The value on the $i$th position of the loop counter indicates the number of times this element went through the $i$th {\em loop context} (cycle) in a dataflow. Each processing worker monitors for the items and their timestamps and shares this information with other workers. Worker broadcasts notifications when there is a guarantee that all elements reach some iteration number or all elements from an epoch are entirely processed.

The main limitation of the mentioned method is extra network usage. The protocol used in Naiad causes the substantial number of extra network messages that quadratically depends on the number of workers, despite the optimizations~\cite{Murray:2013:NTD:2517349.2522738}. 

% Moreover, the tracking agents must be deployed together with workers to ensure the correctness of this approach so that it may induce additional overhead on regular processing.

\subsubsection{Acker}

\acker\ is a mechanism for completeness tracking employed in Apache Storm~\cite{apache:storm}. The main idea is to enrich each data record with a random number. When an element is transformed, its descendants obtain new random numbers. Each time an item is sent or received, a system forwards its number to a special agent called {\em \acker}. Acker XORs all received numbers into the checksum. If all elements have been successfully processed, the checksum will be zero~\footnote{XOR operation is commutative, and as we send each number twice, the entire combination gives zero in the result.}.

In this paper, we build our dependency tracking mechanism based on the same idea. However, we substantially extend it to support the fine granularity and high locality of processing, be scalable, and more efficient in terms of network traffic. We also adopt it for state snapshotting problem. In the next section, we discuss details about \acker\ and our method called \tracker .