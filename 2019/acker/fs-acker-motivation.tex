\label {fs-acker-motivation}

Distributed stream processing systems are shared-nothing runtimes which continuously ingest input elements, transform them according to pre-defined operators, and deliver output elements. We assume that an operator processes items one-by-one so that it can handle only one record at a time. Operators can be stateless and stateful. An output element of an operator may depend on the current state as well as on the corresponding input element. Operators are partitioned among workers and independently process their chunks of data items. Streaming operators are often defined just as a user-provided code and can be quite complicated: they can produce multiple output elements from a single input, filter out some items, or do transformations according to their current state. In general, it is hard to figure out which input element originated an output one. For instance, if an operator receives texts and splits them into the words, it is not possible, having only a word, to determine the document this word came from.

{\em Dependency tracking} is a process of matching streaming elements with its original input items and vice versa. One of the most common problems that require this mechanism is to notify the system when all descendants of some input elements are entirely processed. For instance, input elements can be stored in persistent queues to be reprocessed in case of system failures. Items cannot be stored in these queues forever due to memory and disk limits. A dependency tracking mechanism can trigger garbage collection within these queues. As an example, notification, when the system has processed all words from particular documents, can cause removing these documents from the persistent storage. 

A good dependency tracking technique should provide low notification latency as well as do not induce an overhead on regular processing. Another performance metric is a {\em granularity} of tracking: some methods can determine the exact corresponding input element, while others can only indicate a set of input records that contains the original item.

Further, in this section, we present several practical problems that require a dependency tracking mechanism. We demonstrate that this mechanism plays a crucial role in obtaining the correct and consistent processing results. After that, we review tracking techniques adopted in state-of-the-art stream processing systems and identify their limitations. 

\subsection{Transactional processing}
If input elements are split into multiple ones and system processes them independently, a user often assumes that the descendants will be processed atomically. In other words, if a system loses a single element, other items that depend on the same input will not affect a system state. As an illustration, let us consider a streaming pipeline that transfers money between accounts. Assume that input elements are bank transfer actions. An operator that updates an account balance is partitioned by the account identifier. In this case, one can split an input element into items that update source and destination accounts. Note that these items can be processed independently and possibly on different workers. If a system loses only one of these records, e.g., due to a network failure, the system state becomes inconsistent.

To process input items transactionally, a system must check that all descendants of an input item are not lost and rollback changes caused by survived elements otherwise. One can solve this problem with dependency tracking. If there is no notification that all descendants of an input item are completely processed for quite a long time, a system can initiate state rollback and input elements reprocessing.

\subsection{State snapshotting}
Many streaming systems, including Flink~\cite{Carbone:2017:SMA:3137765.3137777} and Heron~\cite{Kulkarni:2015:THS:2723372.2742788}, apply state snapshotting mechanisms to ensure fault tolerance. The main idea behind this method is to periodically save the global system state (states of all operators) to persistent storage. In case of a failure, the system rollbacks a state from storage and reprocesses missed input elements. The main problem here is to determine which input elements should be reprocessed after the failure. If an input element has already influenced the system state, reprocessing of this element will cause state inconsistencies. The tricky point is that the input element can affect the system state {\em partially}. Returning to the example with bank transfer, the element that updates source account can be processed before failure, while the item that should deposit money to the destination account can be lost. If the system reprocesses the input item, it withdraws money from the source account twice.

To prevent the mentioned inconsistency, a system can snapshot state at time moments, when some specific set of input records have entirely affected it, including all descendant records~\cite{2015arXiv150608603C, thepaper}. Streaming systems use dependency tracking techniques to determine these moments. For instance, the tracking mechanism can notify each operator when it is safe to save its local state. The system commits global snapshot when all operators successfully saved their states affected by certain input elements. Therefore, in case of a failure, a streaming engine can safely reprocess input records that did not influence the snapshot.

\subsection{Existing solutions}
