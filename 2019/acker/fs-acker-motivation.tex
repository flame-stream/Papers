\label {fs-acker-motivation}

Typically, distributed stream processing systems are shared-nothing runtimes which continuously ingest input elements, transform them according to a dataflow graph, and deliver output elements. The dataflow graph consists of operators. We assume that an operator processes items one-by-one so that it can handle only one record at a time. Operators can be stateless or stateful. An output element of an operator may depend on the current state as well as on the corresponding input element. Commonly, all operators are partitioned among workers. In most cases, a partition of the next operator for a regular record can be determined in runtime only because it may depend on the record payload. 

Streaming operators are often defined just as a user-provided code and can be quite complicated: they can produce multiple output elements from a single input, filter out some items, or do transformations according to their current state. Different partitions of the next operator can handle descendants of a single element. In general, it is hard to figure out which input element originated an output one. Let us illustrate it by a word count example. Suppose that the first operator in a dataflow graph receives texts and splits them into the words, and the second operator updates the word occurrences state and sends out the tuples $[word, updated\_occurences]$. The first operator can be randomly partitioned, while the second should be partitioned by a word. Without extra information, it is not possible, having only a word and the new number of its occurrences, to determine the document that caused the generation of this tuple.

{\em Dependency tracking} is a process of matching streaming elements with its original input items. Typically, this mechanism can provide for notifications when all descendants of some input elements are entirely or partially processed. For instance, input elements can be stored in persistent queues to be reprocessed in case of system failures. Items cannot be stored in these queues forever due to memory and disk limits. A dependency tracking technique can trigger the clean up of these queues. As an example, notification, when the system has updated occurrences of all words from particular documents, can cause removing these documents from the persistent storage.

A good dependency tracking method should provide for low notification latency as well as do not induce an overhead on regular processing. A {\em granularity} of tracking matters as well: often, a system needs to monitor the completeness of the individual input element or small sets of them. The fine granularity of tracking reduces the processing latency of some applications~\cite{we2018adbis} but can induce an overhead on throughput. Another valuable property is a {\em dataflow locality} of tracking: some methods can provide notifications for small parts or {\em blocks} of the execution graph, while others can only indicate when input records went through the whole dataflow. Block-level or even operator-level tracking may significantly improve the performance of distributed state snapshotting~\cite{Carbone:2017:SMA:3137765.3137777, 2015arXiv150608603C}.

Further, in this section, we present several practical problems that require a dependency tracking mechanism. We demonstrate that this mechanism plays a crucial role in obtaining the correct and consistent processing results. After that, we discuss tracking techniques adopted in state-of-the-art stream processing systems and identify their properties and limitations. 

\subsection{Completeness monitoring}
As we mentioned above, streaming systems often need to monitor the completeness of processing. As we mentioned above, one of the possible applications is to purge input queues when some input elements are entirely processed. Another important problem that requires completeness notifications is to alert the user if some elements are lost.

A particular use-case of completeness monitoring is transactional processing. If input elements are split into multiple ones and system processes them independently, a user often assumes that the descendants will be handled atomically. In other words, if a system loses a single element, other items that depend on the same input will not affect a system state. As an illustration, let us consider a text classification pipeline that labels input elements with topics such as ``Sport'', ``Politics'', ``Technology'', and so on. Assume that the classifier uses inverse document frequency as a feature. IDF computation is a stateful operation that can be partitioned by words. Hence, one can split an input document into words that are processed by various partitions of IDF operation. Note that word items can be processed independently and possibly on different workers. If a system loses some word records, e.g., due to a network failure, the states of IDF partitions becomes inconsistent. This behavior can potentially affect classification accuracy~\cite{webirte}.

To process input items transactionally, a system must check that all descendants of an input item are not lost and rollback changes caused by survived elements otherwise. One can solve this problem with dependency tracking. If there is no notification that all descendants of an input item are completely processed for quite a long time, a system can initiate a recovery mechanism.

\subsection{State snapshotting}
Many streaming systems, including Flink~\cite{Carbone:2017:SMA:3137765.3137777} and Heron~\cite{Kulkarni:2015:THS:2723372.2742788}, apply state snapshotting mechanisms to ensure fault tolerance. The main idea behind this method is to periodically save the global system state (states of all operators) to persistent storage. In case of a failure, the system rollbacks a state from storage and reprocesses missed input elements. The main problem here is to determine which input elements should be reprocessed after the failure. If an input element has already influenced the system state, reprocessing of this element will cause state inconsistencies. The tricky point is that the input element can affect the system state {\em partially}. Returning to the example with text classification, some word records from a given document can successfully update the corresponding IDF partitions, while the others can be lost. If the system reprocesses the input document, some words influence the IDF state twice. As it is demonstrated in~\cite{webirte}, this issue can influence classification results.

To prevent the mentioned inconsistency, a system can snapshot state at time moments, when some specific set of input records have entirely affected it, including all descendant records~\cite{2015arXiv150608603C, thepaper}. Streaming systems use dependency tracking techniques to determine these moments. For instance, the tracking mechanism can notify each operator when it is safe to save its local state. The system commits global snapshot when all operators successfully saved their states affected by certain input elements only. Therefore, in case of a failure, a streaming engine can safely reprocess input records that did not influence the snapshot.

\subsection{Existing solutions} \label{existing_solutions}

\subsubsection{Micro-batching}

We can consider the micro-batching model applied in Spark Streaming~\cite{Zaharia:2012:DSE:2342763.2342773} and Storm Trident~\cite{apache:storm:trident} as a dependency tracking mechanism. Within the micro-batching model, system groups all input elements into small sets called micro-batches. System consecutively and atomically processes micro-batches. It monitors the completeness of processing using {\em lineage} that traces all transformations within the execution graph. In case of failure, a current micro-batch can be entirely or partially (depends on a lineage structure) reprocessed because it is guaranteed that an element in a batch may depend only on another record in the same batch. The system can snapshot the state after each successfully handled micro-batch. Therefore, the micro-batching model is applied for both completeness monitoring and state snapshotting problems. The system does not track items within a batch, so this technique provides low tracking locality. The size of a micro-batch defines the granularity of tracking. Hence, another limitation of this method is the coarse granularity of tracking due to the ineffectiveness of very small micro-batches~\cite{Zaharia:2012:DSE:2342763.2342773}.

\subsubsection{Markers}

Another popular approach bases on punctuations~\cite{Tucker:2003:EPS:776752.776780}. The main idea is to inject special input elements called {\em markers} or {\em checkpoint items} into a system. These items flow through the same network channels and separate groups of ordinary input records. Therefore, a streaming element can depend only on the input item that arrived between the surrounding markers. If an operator receives a marker, it is guaranteed that this operator has already received all descendants of certain input records. Hence, this event can initiate state snapshotting. When a marker reaches the end of a pipeline, it indicates that the system has entirely processed certain input elements. This technique is practically adopted for both completeness monitoring and state snapshotting problems in Flink~\cite{Carbone:2017:SMA:3137765.3137777}, Heron~\cite{Kulkarni:2015:THS:2723372.2742788}, and IBM Streams~\cite{jacques2016consistent}. While this approach is quite similar to the micro-batching technique, it provides a higher locality of tracking because of independent notifications for different parts of a pipeline. However, marker-based methods do not support cyclic dataflows. The frequency of markers injection determines the granularity in the case markers technique. If the system injects marker after each input record, it can determine the exact input element by corresponding outputs. Otherwise, it is only possible to determine a set of input records that contains the item which originated outputs. As we demonstrate further, this method adds significant performance overhead on regular processing in case of fine-grained tracking.

\subsubsection{Acker}

\acker\ is a mechanism for completeness tracking employed in Apache Storm~\cite{apache:storm}. The main idea is to enrich all data records with a random number. Each time an item is sent or received, its random number is XORed into the checksum. If all elements have been successfully processed, the checksum will be zero. In this paper, we build our dependency tracking mechanism based on the same idea. However, we substantially extend it to support the fine granularity and high locality of processing, be scalable, and more efficient in terms of network traffic. We also adopt it for state snapshotting problem. In the next section, we discuss details about \acker\ and our method called \tracker\ .