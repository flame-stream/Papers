\label {fs-acker-motivation}

Distributed stream processing systems are shared-nothing runtimes which continuously ingest input elements, transform them according to pre-defined operators, and deliver output elements. We assume that an operator processes items one-by-one so that it can handle only one record at a time. Operators can be stateless and stateful. An output element of an operator may depend on the current state as well as on the corresponding input element. Operators are partitioned among workers and independently process their chunks of data items. Streaming operators are often defined just as a user-provided code and can be quite complicated: they can produce multiple output elements from a single input, filter out some items, or do transformations according to their current state. In general, it is hard to figure out which input element originated an output one. For instance, if an operator receives texts and splits them into the words, it is not possible, having only a word, to determine the document this word came from.

{\em Dependency tracking} is a process of matching streaming elements with its original input items and vice versa. Typically, this mechanism can notify the system when all descendants of some input elements are entirely or partially processed. For instance, input elements can be stored in persistent queues to be reprocessed in case of system failures. Items cannot be stored in these queues forever due to memory and disk limits. A dependency tracking technique can trigger garbage collection within these queues. As an example, notification, when the system has processed all words from particular documents, can cause removing these documents from the persistent storage. 

A good dependency tracking method should provide low notification latency as well as do not induce an overhead on regular processing. A {\em granularity} of tracking matters as well: often, a system needs to monitor the completeness of the individual input element or small sets of them. In some cases, the granularity of tracking can directly influence the latency of processing~\cite{we2018adbis}. Another valuable property is a {\em locality} of tracking: some methods can provide notifications for small parts of the pipeline, while others can only indicate when input records went through the whole dataflow.

Further, in this section, we present several practical problems that require a dependency tracking mechanism. We demonstrate that this mechanism plays a crucial role in obtaining the correct and consistent processing results. After that, we review tracking techniques adopted in state-of-the-art stream processing systems and identify their limitations. 

\subsection{Completeness monitoring}
As we mentioned above, streaming systems often need to monitor the completeness of processing. One application is to purge input queues when some input elements are entirely processed. Another important problem that requires completeness notifications is to alert the user if some elements are lost.

A particular use-case of completeness monitoring is transactional processing. If input elements are split into multiple ones and system processes them independently, a user often assumes that the descendants will be handled atomically. In other words, if a system loses a single element, other items that depend on the same input will not affect a system state. As an illustration, let us consider a text classification pipeline that labels input elements with topics such as ``Sport'', ``Politics'', ``Technology'', and so on. Assume that the classifier uses inverse document frequency as a feature. IDF computation is a stateful operation that can be partitioned by words. Hence, one can split an input document into words that are processed by various partitions of IDF operation. Note that word items can be processed independently and possibly on different workers. If a system loses some word records, e.g., due to a network failure, the states of IDF partitions becomes inconsistent. This behavior can potentially affect classification accuracy~\cite{webirte}.

To process input items transactionally, a system must check that all descendants of an input item are not lost and rollback changes caused by survived elements otherwise. One can solve this problem with dependency tracking. If there is no notification that all descendants of an input item are completely processed for quite a long time, a system can initiate a recovery mechanism.

\subsection{State snapshotting}
Many streaming systems, including Flink~\cite{Carbone:2017:SMA:3137765.3137777} and Heron~\cite{Kulkarni:2015:THS:2723372.2742788}, apply state snapshotting mechanisms to ensure fault tolerance. The main idea behind this method is to periodically save the global system state (states of all operators) to persistent storage. In case of a failure, the system rollbacks a state from storage and reprocesses missed input elements. The main problem here is to determine which input elements should be reprocessed after the failure. If an input element has already influenced the system state, reprocessing of this element will cause state inconsistencies. The tricky point is that the input element can affect the system state {\em partially}. Returning to the example with text classification, some word records from a given document can successfully update the corresponding IDF partitions, while the others can be lost. If the system reprocesses the input document, some words influence the IDF state twice. As it is demonstrated in~\cite{webirte}, this issue can influence classification results.

To prevent the mentioned inconsistency, a system can snapshot state at time moments, when some specific set of input records have entirely affected it, including all descendant records~\cite{2015arXiv150608603C, thepaper}. Streaming systems use dependency tracking techniques to determine these moments. For instance, the tracking mechanism can notify each operator when it is safe to save its local state. The system commits global snapshot when all operators successfully saved their states affected by certain input elements only. Therefore, in case of a failure, a streaming engine can safely reprocess input records that did not influence the snapshot.

\subsection{Existing solutions}
We can consider the micro-batching model applied in Spark Streaming~\cite{Zaharia:2012:DSE:2342763.2342773} and Storm Trident~\cite{apache:storm:trident} as a dependency tracking mechanism. Within the micro-batching model, system groups all input elements into small sets called micro-batches. System consecutively and atomically processes micro-batches. It monitors the completeness of processing using {\em lineage} that traces all transformations within the execution graph. In case of failure, a current micro-batch can be entirely or partially (depends on a lineage structure) reprocessed because it is guaranteed that an element in a batch may depend only on another record in the same batch. The system can snapshot the state after each successfully handled micro-batch. Therefore, the micro-batching model is applied for both completeness monitoring and state snapshotting problems. The system does not track items within a batch, so this technique provides low tracking locality. Another limitation of this method is the coarse granularity of tracking due to the ineffectiveness of very small micro-batches~\cite{Zaharia:2012:DSE:2342763.2342773}.

Another popular approach bases on punctuations~\cite{Tucker:2003:EPS:776752.776780}. The main idea is to inject special input elements called {\em markers} or {\em checkpoint items} into a system. These items flow through the same network channels and separate groups of ordinary input records. Therefore, a streaming element can depend only on the input item that arrived between the surrounding markers. If an operator receives a marker, it is guaranteed that this operator has already received all descendants of certain input records. Hence, this event can initiate state snapshotting. When a marker reaches the end of a pipeline, it indicates that the system has entirely processed certain input elements. This technique is practically adopted for both completeness monitoring and state snapshotting problems in Flink~\cite{Carbone:2017:SMA:3137765.3137777}, Heron~\cite{Kulkarni:2015:THS:2723372.2742788}, and IBM Streams~\cite{jacques2016consistent}. While this approach is quite similar to the micro-batching technique, it provides a higher locality of tracking because of independent notifications for different parts of a pipeline. However, marker-based methods do not support cyclic dataflows. As we demonstrate further, this method also adds significant performance overhead on regular processing in case of fine-grained tracking.

\acker\ is a mechanism for completeness tracking employed in Apache Storm~\cite{apache:storm}. The main idea is to enrich all data records with a random number. Each time an item is sent or received, its random number is XORed into the checksum. If all elements have been successfully processed, the checksum will be zero. In this paper, we build our dependency tracking mechanism based on the same idea. However, we substantially extend it to support the fine granularity and high locality of processing, be scalable, and more efficient in terms of network traffic. We also adopt it for state snapshotting problem. In the next section, we discuss details about \acker\ and our method called \tracker\ .