\label {fs-acker-motivation}

Distributed stream processing systems are shared-nothing runtimes which continuously ingest input elements, transform them according to pre-defined operators, and deliver output elements. We assume that an operator processes items one-by-one so that it can handle only one record at a time. Operators can be stateless and stateful. An output element of an operator may depend on the current state as well as on the corresponding input element. Operators are partitioned among workers and independently process their chunks of data items. Streaming operators are often defined just as a user-provided code and can be quite complicated: they can produce multiple output elements from a single input, filter out some items, or do transformations according to their current state. In general, it is hard to figure out which input element originated an output one. For instance, if an operator receives texts and splits them into the words, it is not possible, having only a word, to determine the document this word came from.

{\em Dependency tracking} is a process of matching streaming elements with its original input items and vice versa. Typically, this mechanism can notify the system when all descendants of some input elements are entirely or partially processed. For instance, input elements can be stored in persistent queues to be reprocessed in case of system failures. Items cannot be stored in these queues forever due to memory and disk limits. A dependency tracking technique can trigger garbage collection within these queues. As an example, notification, when the system has processed all words from particular documents, can cause removing these documents from the persistent storage. 

A good dependency tracking method should provide low notification latency as well as do not induce an overhead on regular processing. Another valuable property is a {\em granularity} of tracking: some methods can provide notifications for the specific input element and small parts of the pipeline, while others can only indicate when a set of input records went through the whole dataflow.

Further, in this section, we present several practical problems that require a dependency tracking mechanism. We demonstrate that this mechanism plays a crucial role in obtaining the correct and consistent processing results. After that, we review tracking techniques adopted in state-of-the-art stream processing systems and identify their limitations. 

\subsection{Completeness monitoring}
As we mentioned above, streaming systems often need to monitor the completeness of processing. One application is to purge input queues when some input elements are entirely processed. Another important problem that requires completeness notifications is to alert the user if some elements are lost.

A particular use-case of completeness monitoring is transactional processing. If input elements are split into multiple ones and system processes them independently, a user often assumes that the descendants will be handled atomically. In other words, if a system loses a single element, other items that depend on the same input will not affect a system state. As an illustration, let us consider a streaming pipeline that transfers money between accounts. Assume that input elements are bank transfer actions. An operator that updates an account balance is partitioned by the account identifier. In this case, one can split an input element into items that update source and destination accounts. Note that these items can be processed independently and possibly on different workers. If a system loses only one of these records, e.g., due to a network failure, the system state becomes inconsistent.

To process input items transactionally, a system must check that all descendants of an input item are not lost and rollback changes caused by survived elements otherwise. One can solve this problem with dependency tracking. If there is no notification that all descendants of an input item are completely processed for quite a long time, a system can initiate a recovery mechanism.

\subsection{State snapshotting}
Many streaming systems, including Flink~\cite{Carbone:2017:SMA:3137765.3137777} and Heron~\cite{Kulkarni:2015:THS:2723372.2742788}, apply state snapshotting mechanisms to ensure fault tolerance. The main idea behind this method is to periodically save the global system state (states of all operators) to persistent storage. In case of a failure, the system rollbacks a state from storage and reprocesses missed input elements. The main problem here is to determine which input elements should be reprocessed after the failure. If an input element has already influenced the system state, reprocessing of this element will cause state inconsistencies. The tricky point is that the input element can affect the system state {\em partially}. Returning to the example with bank transfer, the element that updates source account can be processed before failure, while the item that should deposit money to the destination account can be lost. If the system reprocesses the input item, it withdraws money from the source account twice.

To prevent the mentioned inconsistency, a system can snapshot state at time moments, when some specific set of input records have entirely affected it, including all descendant records~\cite{2015arXiv150608603C, thepaper}. Streaming systems use dependency tracking techniques to determine these moments. For instance, the tracking mechanism can notify each operator when it is safe to save its local state. The system commits global snapshot when all operators successfully saved their states affected by certain input elements only. Therefore, in case of a failure, a streaming engine can safely reprocess input records that did not influence the snapshot.

\subsection{Existing solutions}
We can consider the micro-batching model applied in Spark Streaming~\cite{Zaharia:2012:DSE:2342763.2342773} and Storm Trident~\cite{apache:storm:trident} as a dependency tracking mechanism. Within the micro-batching model, system groups all input elements into small sets called batches. System consecutively and atomically processes micro-batches and monitors the completeness of the computations. In case of failure, a current micro-batch can be entirely reprocessed, because it is guaranteed that an element in a batch may depend only on another record in the same batch. Therefore, the system can snapshot the state after each successfully handled micro-batch. Very small micro-batches are ineffective~\cite{Zaharia:2012:DSE:2342763.2342773}, so the main limitation of this technique is the low granularity of tracking.

Another popular approach bases on punctuations~\cite{Tucker:2003:EPS:776752.776780}. The main idea is to inject special input elements called {\em checkpoints} into a system. These items flow through the same network channels and separate groups of ordinary input records. Therefore, a streaming element can depend only on the input item that arrived between the surrounding checkpoints. If an operator receives a checkpoint, it is guaranteed that this operator has already received all descendants of certain input records. Hence, this event can initiate state snapshotting. While this approach is quite similar to the micro-batching technique, it provides more fine-grained tracking because of independent notifications for different parts of a pipeline. However, checkpoint-based methods do not support cyclic dataflows as well as can affect the throughput of a system in case of tracking for the individual input elements.

\acker\ is a mechanism for completeness tracking employed in Apache Storm~\cite{apache:storm}. The main idea is to enrich all data records with a random number. Each time an item is sent or received, its random number is XORed into the checksum. If all elements have been successfully processed, the checksum will be zero. In this paper, we build our dependency tracking mechanism based on the same idea. However, we substantially extend it to support the high granularity of processing, be scalable, and more efficient in terms of network traffic. In the next section, we discuss details about \acker\ and our method called \tracker\ .