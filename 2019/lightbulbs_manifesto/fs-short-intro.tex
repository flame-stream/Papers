\label {fs-short-intro}

Streaming applications often work with data that can be considered as values of some random variable with a given probability distribution. For example, user identifiers may be distributed uniformly or word frequencies from social networks text posts can fit Zipf distribution. The unexpected probability distribution of streaming data can directly influence the correctness of the final results:

\begin{itemize}
    \item In machine learning it is often assumed that training data fits a known distribution, e.g. a Gaussian. If this assumption is not satisfied, the model can provide poor performance.
    \item In A/B testing task, there is a need to ensure that users are uniformly distributed among application versions. Otherwise, experiment results may lead to inaccurate business decisions.
    \item ...
\end{itemize}

In this paper, we highlight the challenges of testing that streaming data fits a given distribution in {\em distributed} stream processing. In this case, the whole data is not processed on a single machine: it is partitioned among computational units in a cluster based on business-logic requirements. We demonstrate trade-offs between a naive and more complicated approach and discuss the preliminary results.

% There are a lot of statistical tools that aim to solve this problem. Most of them assume that the whole data is processed on a single computational unit. However, this approach does not fit in distributed stream processing, because merging all data to a single machine limits throughput and causes extra communication cost. In this work, we investigate the decentralized method for statistical data validation of data streams: each computational unit independently verifies data that is assigned to it. We demonstrate statistical limitations and challenges that we recognized during experiments on a continuous and discrete data. We also show that the API of state-of-the-art stream processing systems is not enough to solve this problem efficiently. Possible solutions to the mentioned issues and open problems are discussed.

% \subsection{Applications}
% A large number of applications process data not as a fixed size dataset but as the continuous stream. The difference between processing fixed size datasets and streams is not only algorithmic. Data from the fixed size dataset is assumed to be generated from a single process. For instance, a dataset from fixed distribution(???) . But streaming data is time dependent. The data generating process may change over time. Detection of such changes is one of the most important problems in the field of streaming data processing.


% \subsubsection{A/B testing}
% Suppose the main page of the web site is up to be changed. We want to evaluate how changes would affect the audience of the site. For example, will users begin to click on ads more often? Suppose the system has started to unevenly distribute users to groups corresponding to different versions of the page. For example, all Europeans or users of a specific browser get into (???) the same group. Analysts performing such tests may want to know about such events as soon as possible because it can lead to financial losses.

% \subsubsection{Quality control}
% Suppose a factory produces goods and services. Estimating the number of defective products and determining the statistical significance of this assessment is a well-studied (???) problem in mathematical statistics. It may be important for the factory to analyze the distribution of defective items over time. Changes in this distribution may signal that there is a problem or, on the contrary, provide evidence that the new production technology has brought improvements.


% \subsubsection{(???) Statistical approaches}
%     Recently, many techniques evaluating changes detection in data distribution have been presented. All these approaches can be divided into the following three groups.
%     \subsubsection{Parametric approach}
%     A parametric approach to changes detection is based on the availability of comprehensive information on the distribution of data before and after the change. The main advantage of such approaches is very accurate results. Although, it is impossible to always know the distribution of data. Semi-parametric methods and non-parametric approaches are used in such cases (???).

% 	\subsubsection{Semi-parametric approaches}
% Semi-parametric approaches are based on the assumption that the distribution of observations belongs to a certain family of distributions so changing parameters of the distribution are to be estimated.

% 	\subsubsection{Non-parametric approaches}
%     Such approaches do not make any assumptions about data distribution and can be divided into window-based and non-window based approaches.
%     The techniques based on sliding-window are more frequently used in real problems. (???). Although, the high delay is the main disadvantage of such techniques.