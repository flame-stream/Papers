\label {fs-short-intro}

Streaming data validation is a hot research topic~\cite{Xu:2013:MVS:2488222.2488275, frank2018semantic} with a lot of practical applications, including:

{\bf A/B testing}. To obtain reliable A/B testing results there is a need to ensure that approximately the same number of users have experienced each application version. Otherwise, the comparison between versions may be unfair due to unbalanced user groups. This issue can lead to inaccurate business decisions, an increase of churn rate, and financial losses.
    
{\bf Spam detection}. It is assumed that input data for spam detectors contain natural language texts. However, issues in data producers or in a data preparation pipeline could result in a violation of this assumption, e.g. incorrect stemming may shift term frequencies which are commonly used as features for spam classification. Without early detection, this bug can lead to many false decisions and affect service reputation.

Both mentioned problems require validation of the general properties of the whole input data, and hardly can be solved by individual stream elements filtering. A typical solution for this class of problems is a statistical hypothesis testing. In A/B testing example, we can check that the number of users per application version has a Poisson distribution with a constant parameter $\lambda$. Regarding spam detector, one can examine if word frequencies in the whole corpus of input texts fit Zipf law. 

Despite the fact that statistical hypothesis testing on streams is a well-studied topic~\cite{kifer2004detecting, lall2015data}, it becomes much more challenging if a stream is processed on different networked computers as it is implemented in state-of-the-art systems like Flink~\cite{Carbone:2017:SMA:3137765.3137777} and Spark Streaming~\cite{Zaharia:2012:DSE:2342763.2342773}. In this case, individual stream elements may be processed by independent computational units. To apply existing approaches in this setting, we can merge data on a single unit for validation. The main disadvantage of this method is that validation may become a throughput bottleneck for the whole data processing pipeline. 

In this work, we highlight an approach where data is independently validated on each node in a cluster. In our setting, a final decision about hypothesis acceptance is made based on local decisions of computational units. In the rest of this paper, we discuss the main challenges and trade-offs of the proposed approach and share preliminary results.