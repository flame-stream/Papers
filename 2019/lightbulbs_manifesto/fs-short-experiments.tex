\label {fs-short-experiments}

\begin{figure*}[t!]
    \begin{subfigure}[b]{0.31\textwidth}
            \includegraphics[width=\linewidth]{pics/detection_rate}
            \caption{Change detection speed for round-robin and hash-based data partitionings}
            \label{detection_rate}
    \end{subfigure}%
    \hspace{5mm}
    \begin{subfigure}[b]{0.31\textwidth}
            \includegraphics[width=\linewidth]{pics/decision_making.png}
            \caption{A comparison between decision making approaches}
            \label{decision_making}
    \end{subfigure}%
    \hspace{5mm}
    \begin{subfigure}[b]{0.31\textwidth}
            \includegraphics[width=\linewidth]{pics/throughput}
            \caption{Throughput for centralized and decentralized approaches}
            \label{throughput}
    \end{subfigure}%
    \caption{Results of preliminary experiments}
\end{figure*}


\indent

{\bf Setup.} We conducted several experiments to demonstrate that our problem statement is practically important and to determine basic directions towards a solution. As a validation task, we used checking that frequencies of words in an input stream fit Zipf law. Stream of words can be considered as a part of word count or IDF computing pipelines. We used Wikipedia corpus as an input stream for experiments and $\chi^{2}$ goodness of fit test for validation.

{\bf Change detection speed.} In this experiment, we measured the duration (in input events) of change detection. A change in data was simulated using input from the generated corpus with lognormally distributed word frequencies. We compared two data partitioning methods $P$. The first one is natural for word count and IDF computing task: words are partitioned based on a value of the hash function. In this case, elements with a given word stick to a single node. The second approach is round-robin. While it is not suitable for word count, such partitioning ensures that each word may be processed on all computational nodes. As expected, Figure~\ref{detection_rate} demonstrates that round-robin provides faster detection, because simulated lognormal data is distributed among nodes in a more uniform way. For faster detection, it may be reasonable to reshuffle words in a round-robin manner after, e.g. IDF aggregation.

{\bf Approaches to making decisions.} This experiment demonstrates a comparison between three simple mechanisms for making a global decision on hypothesis acceptance $\varphi$ based on local statistics of each node $y_T$. The first method rejects a hypothesis if at least one node rejects it. The second technique makes the decision based on a majority vote of all nodes decisions. The last method rejects hypothesis only if all nodes reject it. Figure~\ref{decision_making} shows a dependency between a number of nodes and the number of input elements needed to detect change for each of the proposed methods within round-robin partitioning. The choice of a method for making decision significantly affects detection speed. On the other hand, various methods may provide distinct false positive and false negative rates, e.g. it is obvious that the first method provides a higher false negative rate. The investigation of the statistical properties of methods for making a decision is in the scope of our future work.

{\bf Throughput.} We measured the throughput of two techniques: centralized, where words are merged into a single node for validation, and decentralized. Experiments were conducted with Apache Flink on a cluster of Amazon EC2 small instances with 1 core CPU and 2GB RAM. Figure~\ref{throughput} shows that centralized method significantly limits throughput and scalability and may influence the performance of the whole data flow. This result demonstrates that decentralized validation problem statement has a practical rationale.