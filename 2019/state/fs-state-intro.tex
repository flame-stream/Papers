%%% fs-state-intro - Introduction

\label {fs-intro-seciton}

Distributed batch processing systems, such as Google's MapReduce~\cite{Dean:2008:MSD:1327452.1327492}, Apache Hadoop~\cite{hadoop2009hadoop}, and Apache Spark~\cite{Zaharia:2016:ASU:3013530.2934664}, address a need to process vast amounts of data (e.g., Internet-scale). 
The basic   idea behind them is to independently process large data blocks (batches) that are collected from static datasets. 
%  These engines are able to run in a massively parallel fashion on clusters consisting of thousands of commodity computational units. 
The main advantages of these systems are fault tolerance and scalability~\cite{borthakur2011apache} of massively parallel computations on commodity hardware.

However, there are plenty of scenarios where processing results are most valuable at the time of data arrival, for example, IoT, news processing, financial analysis, anti-fraud, network monitoring, etc. 
Such problems cannot be directly addressed by classical MapReduce~\cite{Doulkeridis:2014:SLA:2628707.2628782}. 
State-of-the-art stream processing systems, such as Flink \cite{carbone2015apache}, Samza \cite{Noghabi:2017:SSS:3137765.3137770}, Storm \cite{apache:storm}, Spark Streaming~\cite{Zaharia:2012:DSE:2342763.2342773},   provide a computational model addressing these needs.
%   aim at filling this gap by introducing another computational models. 
%  These systems receive a record or a set of records, updates internal state if any, and sends out new records. % можно выпилить

One of the most challenging tasks for streaming systems is to provide guarantees on data processing. 
Streaming systems must release output elements before processing has finished because input data is assumed to be unbounded. 

In distributed stream processing, consistency is usually described in terms of delivery guarantees: {\em at-most-once}, {\em at-least-once}, and {\em \eo}~\cite{carbone2015apache}. 
These guarantees describe a contract regarding {\em which data} will be  processed and released in case of failures. 
\Eo\ is the strongest and the most valuable guarantee from the user perspective as it ensures that input elements are processed atomically and are not lost. These notions are seemingly simple but shadow  the dependency   of  an output item on the {\em system state} as well as on the  input item. 
Streaming systems face a need to recover computations consistently with previous input data, the current system state, and with the already delivered elements.
This requirement makes failure recovery mechanisms somewhat complex. 
%  The above implies that a system can process each element exactly once, but in practice, it can release completely invalid results due to inconsistencies in the state or in-flight elements.

This complication is resolved  by most of the existing stream processing engines. 
Flink ensures atomicity of  state updates and   delivery using a protocol based on distributed transactions. 
%   This protocol prevents inconsistencies but leads to a significant latency increase. 
Google MillWheel~\cite{Akidau:2013:MFS:2536222.2536229} enforces consistency between state and output elements by writing results of each operation to persistent external storage. 
%  The lower bound of latency is a duration of all external writes within routes of an input element and its descendants. 
Micro-batching engines like Storm Trident~\cite{apache:storm:trident} and Spark Streaming~\cite{Zaharia:2012:DSE:2342763.2342773} process data in small-sized blocks. 
Each block is atomically processed on each stage of a data flow  providing properties similar to batch processing. 
%   The main downside of this approach is high latency, about a few seconds~\cite{7530084, 7474816}. 
%Therefore,  it is clear that state-of-the-art systems are aware of the issues regarding exactly once, but pay performance price for their resolution.
The price for~\eo\ delivery is a high latency  observed in these implementations (e.g.~\cite{7530084, 7474816}).


The huge gap between the notion of~\eo\ and the properties of its implementations indicates the lack of formalization. 
Misunderstandings of streaming delivery guarantees frequently cause debates and discussions~\cite{JerryPengStreamIO, PaperTrail}. Without a formal model, it is hard to observe similarities and distinctions between existing solutions and to recognize their limitations. In this work, we introduce a formal model of stream processing that captures delivery guarantees existing in most of the state-of-the-art systems.

Another property that can be easily obtained in batch processing systems but is hard to achieve in streaming engines is {\em a determinism}. 
The determinism means that repeated runs of the system on the same data produce the same results. It is commonly considered as a challenging task~\cite{Zacheilas:2017:MDS:3093742.3093921}. On the other hand, this property is desirable, because it implies reproducibility and predictability. Intuitively, determinism is connected with delivery guarantees~\cite{Stonebraker:2005:RRS:1107499.1107504}, but, to the best of our knowledge, this relation has not been deeply investigated. 

% We demonstrate that determinism is tightly connected with the implementation of delivery guarantees. In a deterministic system, a state of a non-commutative operation can be reprocessed consistently with the previous output elements. Hence, there is no need to save the results of non-commutative operations before output delivery. This property opens a wide range for performance optimizations.

In this work, we demonstrate that the property of determinism can mitigate an overhead on~\eo\. 
In order to prove the feasibility of efficient~\eo\ over determinism, we introduce  fault tolerance protocols on top of {\em drifting state} model~\cite{we2018adbis}. 
%  This optimistic technique provides determinism with low overhead. 
We show that lightweight determinism together with the results of the formal inference provides for~\eo\ for a negligible  cost.
%   It is verified by the experiments on a realistic  problem.

The contributions of this paper are the following: 
\begin{itemize}
    \item Formal model of a distributed stream processing  and   a   definition of  delivery guarantees 
    \item Demonstration that in non-deterministic systems providing~\eo\, the lower bound of latency is the duration of state snapshotting
    \item Techniques for lightweight implementation of~\eo\ guarantee on top of a deterministic engine
    \item Study of the practical feasibility of the proposed approaches
\end{itemize}

The rest of the paper is structured as follows: 
in section~\ref{fs-preliminaries} the notion of consistency applied to a stream processing is discussed, 
we introduce our formal framework in section~\ref{fs-formalism}, 
existing implementations of~\eo\ in terms of the proposed formal framework are described in section~\ref{fs-eo-impl}, 
implementation details of~\eo\ over determinism are mentioned in section~\ref{fs-consistency-section}, 
experiments that demonstrate the feasibility of the proposed concept are detailed in section~\ref{fs-experiments-seciton}, 
and we discuss prior works on the topic in section~\ref{fs-related-seciton}. 
