%%% fs-state-intro - Introduction

\label {fs-intro-seciton}

Distributed batch processing systems, such as Google's MapReduce~\cite{Dean:2008:MSD:1327452.1327492}, Apache Hadoop~\cite{hadoop2009hadoop}, and Apache Spark~\cite{Zaharia:2016:ASU:3013530.2934664}, address a need to process vast amounts of data (e.g., Internet-scale). The fundamental idea behind them is to independently process large data blocks that are collected from static datasets. These engines are able to run in a massively parallel fashion on clusters consisting of thousands of commodity computational units. The main advantages of these systems are consistency, fault tolerance, and scalability~\cite{borthakur2011apache}.

However, there are plenty of scenarios where processing results are most valuable at the time of data arrival, for example, IoT, news processing, financial analysis, anti-fraud, network monitoring, etc. Such problems cannot be directly addressed by classical MapReduce~\cite{Doulkeridis:2014:SLA:2628707.2628782}. State-of-the-art stream processing systems, such as Flink \cite{carbone2015apache}, Samza \cite{Noghabi:2017:SSS:3137765.3137770}, Storm \cite{apache:storm}, Spark Streaming~\cite{Zaharia:2012:DSE:2342763.2342773}, aim at filling this gap by introducing another computational model. According to this model, a system receives a record or a set of records, updates internal state if any, and sends out new records. 

One of the most challenging tasks for streaming systems is to provide guarantees on data processing. 
%Unlike batch systems, 
Streaming systems must release output elements before processing has finished because input data is assumed to be unbounded. This requirement makes failure recovery mechanisms more complex. Streaming systems face a need to recover computations consistently with previous input data, the current system state, and with the elements which have been already delivered to a consumer. 

A contract regarding {\em which data} will be eventually processed and released in case of failures is usually described in terms of so-called {\em delivery guarantees}. They include {\em at most once}, {\em at least once}, and {\em exactly once}. Exactly once is the strongest and the most valuable guarantee from the user perspective because it mitigates efforts on output consistency enforcement. These notions are seemingly simple but have important flaws. Typically, an output item depends not only on the corresponding input item but also on the system state. 
The above implies that a system can process each element exactly once, but in practice can release completely invalid results due to inconsistencies in the state or in-flight elements.

The problem shows up if a data flow contains a non-commutative operation. Let us consider a data flow with an operation that concatenates input strings and delivers the result of each iteration. 
After a failure, the system must restore its state, in this case, a concatenation of strings. 
A straightforward approach to restoring state is to replay missing input elements. 
However, these elements can be reordered in an asynchronous distributed environment. 
Such behavior may lead to the concatenation being affected by several input elements exactly once but is inconsistent with elements released before the failure.

This example illustrates that naive definitions of delivery guarantees are not sufficient to provide output consistency. 
Most of the existing implementations prevent anomalies illustrated by the example above. Flink ensures atomicity between state updates and output elements delivery using a protocol based on distributed transactions. This protocol prevents inconsistencies but leads to a significant increase of latency. 
Google MillWheel~\cite{Akidau:2013:MFS:2536222.2536229} enforces consistency between state and output elements by writing results of each operation to persistent external storage. 
The lower bound of latency is a duration of all external writes within routes of an input element and its descendants. 
Micro-batching engines like Storm Trident~\cite{apache:storm:trident} and Spark Streaming~\cite{Zaharia:2012:DSE:2342763.2342773} process data in small-sized blocks. 
Each block is atomically processed on each stage of a data flow that provides properties similar to batch processing.
 The main downside of this approach is high latency, about a few seconds~\cite{7530084, 7474816}.

The huge gap between the notion of exactly once and the properties of its implementations indicates the lack of formalization. Misunderstandings of streaming delivery guarantees frequently cause debates and discussions~\cite{JerryPengStreamIO, PaperTrail}. Without a formal model, it is hard to observe similarities and distinctions between existing solutions and to recognize their limitations.

Another property inherent in batch processing systems but hard to achieve in streaming engines is {\em a determinism}. 
The determinism means that repeated runs of the system on the same data produce the same results. It is commonly considered as a challenging task~\cite{Zacheilas:2017:MDS:3093742.3093921}. 
On the other hand, this property is desirable, because it implies reproducibility and predictability. 
Intuitively, determinism is connected with consistency~\cite{Stonebraker:2005:RRS:1107499.1107504}, but, to the best of our knowledge, this relation has not been deeply investigated. 

In this work, we introduce a formal model of stream processing that captures delivery guarantees existing in most of the state-of-the-art systems. We show that the concepts of delivery guarantees are closely related to output data {\em consistency}. We demonstrate that determinism is tightly connected with both delivery and consistency. In a deterministic system, a state of a non-commutative operation can be reprocessed consistently with the previous output elements. Hence, there is no need to save the results of non-commutative operations before output delivery. This property opens a wide range for performance optimizations.

In order to prove the feasibility of efficient exactly once over determinism, we design fault tolerance protocols on top of {\em drifting state} model~\cite{we2018adbis}. Unlike naive expectations, this optimistic technique provides determinism with low overhead. We show that lightweight determinism together with the results of the formal inference allows achieving exactly once with almost no extra cost. It is verified by the experiments on a realistic problem.

The contributions of this paper are the following: 
\begin{itemize}
    \item Formalization of streaming delivery guarantees 
    \item Demonstration that the property of determinism is tightly related to exactly once
    \item Techniques for lightweight implementation of exactly once guarantee on top of the deterministic engine
    \item Study of practical feasibility of the proposed approaches
\end{itemize}

The rest of the paper is structured as follows: section~\ref{fs-preliminaries} recalls basic concepts of stream processing, we introduce our formal framework in section~\ref{fs-formalism}, existing implementations of exactly once in terms of the proposed formal framework are described in section~\ref{fs-eo-impl}, implementation details of exactly once over determinism are mentioned in section~\ref{fs-consistency-section}, experiments that demonstrate feasibility of the proposed concept are detailed in section~\ref{fs-experiments-seciton}, and we discuss prior works on the topic in section~\ref{fs-related-seciton}. 