\label{fs-related}

While the classification of text streams is a well-studied problem~\cite{zhang2008one, tampakas2005}, it is challenging to solve this task at scale. There are plenty of projects that apply batch or micro-batch processing systems to distributed text classification~\cite{semberecki2016distributed, 8029336, baltas2016apache, svyatkovskiy2016large}. However, as it was mentioned above, these methods are not suitable for streaming classification due to low latency requirements. 

General problems regarding machine learning at scale are formulated by the TFX project team~\cite{Baylor:2017:TTP:3097983.3098021}. Among them are continuous training, reliability, reproducibility, etc. It is shown that these problems are hard even if an environment is reliable and fault tolerant. In this work, we argue that these goals are even harder to achieve using state-of-the-art distributed stream processing systems.

Despite the fact that machine learning on top of distributed stream processing is a hot topic~\cite{qiu2016survey}, previous works in this field do not consider issues related to the delivery guarantees and distributed environment~\cite{khumoyun2016real}. A SAMOA framework~\cite{morales2015samoa} provides implementations of several popular algorithms which can be executed on Flink, Storm or Samza but does not take responsibility for reproducible results and correctness in case of failures. Therefore, SAMOA delegates the work on enforcing reproducibility and fault tolerance on a developer. Spark MLlib~\cite{meng2016mllib} is another popular library for adaptation of machine learning pipelines to scalable batching or micro-batching data flows. While Spark Streaming can produce reproducible and reliable results, it is not able to provide latency less than a second~\cite{karimov2018benchmarking, S7530084}.

The problem of non-deterministic and non-reproducible data flows within distributed stream processing is also have been studied in recent years.  As we demonstrated above, the key problem regarding determinism enforcement is races in a data flow. Popular methods to handle such races is {\em in-order} and {\em out-of-order} processing approaches~\cite{Li:2008:OPN:1453856.1453890}. Both these techniques require buffering before each order-sensitive operations until there is a guarantee that all elements are properly ordered. A mechanism that allows a user to control the trade-off between determinism and latency is proposed in~\cite{Doulkeridis:2014:SLA:2628707.2628782}. However, this technique provides for low latency only with a {\em some level} of determinism. An optimistic approach to handle out-of-order elements approach is introduced in~\cite{we2018seim}. Basically, this method mitigates a need for buffering before each operation and reduces the latency of the whole data flow. An adaptation of this approach for machine learning pipelines relates to our future work.

% Techniques for achieving exactly once the delivery guarantee is discussed in plenty of works~\cite{Carbone:2017:SMA:3137765.3137777, Akidau:2013:MFS:2536222.2536229, Zaharia:2012:DSE:2342763.2342773}. However, most of them have a high performance overhead. A promising technique to obtain exactly once was proposed in~\cite{we2018beyondmr}. The key idea behind it is .