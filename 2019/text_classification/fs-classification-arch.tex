

In order to solve real life tasks with high-velocity data, the stream processing systems can be used as a solution. The main idea of these systems is as follows. Every piece of data is enters and exists the system one by one. After entering, every element then evenly distributed using sharding technique for balancing the workload. This allows to produce a low latency and provide scalablity. The examples of such systems are Apache Flink [?], Google's MillWheel [?], Spark Streaming[?] or Apache Storm.

In this work, our computations based on FlameStream \cite{kuralenok2018flamestream} distributed model. FlameStream provides the following advantages:

\textbf{Determinism.} The result of computing is determined only by input data and remain the same between independent launches. This makes whole classifying process more predictable as it was discussed in \cite{stonebraker20058} Rule 4. In addition, the concept ensures an opportunity for tests that validate the whole pipeline. FlameStream determinism is considered as lightweight.

\textbf{Exactly-once.} Unlike the others stream processing systems, FlameStream produces results with lower latency using the same data processing. FlameStream process data in exactly-once manner, which ensures a low latency with almost no overhead.

These advantages allows us to create a classifier with better performance and due to the determinism and exactly-once delivery guarantee ensures a consistent results.

Similar to other stream processing systems, FlameStream is doing its calculations using a computational graph. This graph is  commonly referred to as logical graph. Logical graph transforms into actutal physical for processing computations. The graph provides a scheme of data flow and can be presented like this:

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.45]{pics/tf-idf-graph}
  \caption{The computational pipeline}
  \label {TF-IDF Graph}
\end{figure}

The Figure 1 illustrates our pipeline. An oriented edge indicates the flow and kind of the data. Every vertex in the graph represents a processing unit such as a single computer or a cluster. The initial text document splits into two computations: the first is calculates term frequencies and the second calculates inverse document frequencies. TF features is computed on a single sharding machine and pass further into TF-IDF vertex. On the other hand, IDF features computations is balanced among all shards. The range $[0, 2^{32} - 1]$ is divided into equal parts by the number of the shards. Every single shard is responsible for its own range. Hash of each word defines what shard will be used for further computing IDF.

Results from IDF vertex aggregates and text classifier receives TF-IDF features of the document and produces result based on Logistic Regression model. The initial classifier weights provided by a pretrain process. Somewhere here is a punctuation comparison: \cite{tucker2003exploiting}

In addition, the Figure 1 provides a scheme for a partial fitting. Some of the text documents is labelled and such documents accumulate in the corresponding vertex. When the amount of new texts is enough for additional training, the existing classifier's weights are being updated as it is described below. After that, new weights is distributed among the system and process of classification continues. 

Further fitting process works according Multinomial Logistic Regression model, where cost function can be illustrates as follows(FIX THE FORMULA):

$$J(W) = -\frac{1}{m} \cdot \sum \limits_{i = 1}^{m} \sum \limits_{j = 1}^{k} \mathbbm{1}_{y^{(i)} == j} \cdot \log \frac{e^{W_{j}^Tx^{(i)}}}{\sum \limits_{l = 1}^{k}  e^{W_{l}^Tx^{(i)}}} + \lambda_1 ||W||_1 + \lambda_2 ||W - W_{prev}||_2 $$

The number of points in new dataset is denoted as $m$. The number of classes is $k$. New weights are designated as $W$ and previous weights are $W_{prev}$.

The gradient of the cost function can be put as(FIX TOO):

$$ \nabla_{W_j} J(W) = -\frac{1}{m} \cdot \sum \limits_{i = 1}^{m} [ x^{(i)} ( \mathbbm{1}_{y^{(i)} == j} - \frac{e^{W_{j}^Tx^{(i)}}}{\sum \limits_{l = 1}^{k}  e^{W_{l}^Tx^{(i)}}} ) ] $$

We calculate the gradient of each component and update the weights in one iteraton of the gradient descent. Maximum number of iterations of the descent is 100.