
Our task is to create a service, which analyzes user messages in real-time, also during the process the service is intended to train on a dataset of incoming labelled texts. The final solution should have a scalablity and a low latency.

\subsection{Distributed stream processing}

In order to solve real life tasks with high-velocity data, the stream processing systems can be used as a solution. The examples of such systems are Apache Flink [?], Google's MillWheel [?], Spark Streaming[?] or Apache Storm[?]. In these systems every piece of data enters and exists the system one by one. Unlike batch-processing systems, this is done without any bufferization during the processing, which provides a low latency for the elements. After entering, the elements are being transformed from one state to another by a map operation. For a scalablity, these systems are running on clusters of computers. To maximize the performance, calculations are evenly distributed among all computers. This method is called sharding technique. Usually each element in streaming systems has a balancing function for determining the shard, where further map transformations will be done with this element. That is, after each transormation every element can be sent to another machine. 

Stream processing systems have several issues to deal with. Most of them have a lack of determinism, which means that the result of computing is not the same between independent launches. Another issue is connected with exactly-once delivery guarantee. This guarantee provides processing each element by exactly one time. Another cases are processing it at most one time and at least one time. The exactly-once guarantee is desirable, because valuable data should not be lost and, on the other hand, multiple processing of the same data causes latency.

In this work, our computations based on FlameStream \cite{kuralenok2018flamestream} distributed model, which provides the following advantages:

\textbf{Determinism.} This makes whole classifying process more predictable as it was discussed in \cite{stonebraker20058} Rule 4. In addition, the concept ensures an opportunity for tests that validate the whole pipeline. FlameStream determinism is considered as lightweight.

\textbf{Exactly-once.} FlameStream process data in exactly-once manner by default, which the same data processing on the Spark systems will be at better performance. The exactly-once guarantee ensures a low latency with almost no overhead.

These advantages allows us to create a classifier with better performance and due to the determinism and exactly-once delivery guarantee provide consistent results.

\subsection{Data flow}

\subsubsection{Computational pipeline}

Similar to other stream processing systems, FlameStream set calculations scheme by a computational graph. This graph is  commonly referred to as logical graph. Logical graph maps into actual physical for processing computations. The graph provides a scheme of data flow and can be presented like this:

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.48]{pics/logical-graph}
  \caption{The logical pipeline}
  \label {logical graph}
\end{figure}

The Figure 1 illustrates the principle pipeline. An oriented edge indicates the flow and kind of the data. Every vertex in the graph represents a processing unit such as a single computer or a cluster. The initial text document splits into two computations: the first is calculates term frequencies and the second calculates inverse document frequencies. TF features is computed on the same machine, where the document is arrived, and passes further into TF-IDF vertex. On the other hand, IDF features computations is balanced among all shards. The process of balancing the workload is same as in FlameStream. The range $[0, 2^{32} - 1]$ is divided into equal parts by the number of the shards. Every single shard is responsible for its own range. Hash of each word defines what shard will be used for further IDF computation. In case of words with high frequency such as conjunctions or prepositions, we add to them the salt to the end thereby distributing all the words between the shards evenly. Results from IDF vertex in TF-IDF vertex aggregate with TF features and text classifier receives TF-IDF features of the document. The classifier outputs labelled document and this document returns to the user.

\subsubsection{Dealing with concept drift}

Concept drift is a phenomenon of changing users' interests from time to time, which usually depends on recent events, and results in shifting the distribution of text classes to particular ones. Essentially, this may affect correctness of the pipeline, more specifically, the computing of the IDF features. To overcome the issue, we use windowed IDF calculation: concrete IDF values will be provided based on input within a timed window. For instance, the window can be set to a day or a week. This scheme allows to deal with the sudden changes of the topics.

\subsubsection{Partial fit}

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.375]{pics/physical-graph}
  \caption{The physical pipeline}
  \label {physical graph}
\end{figure}

The Figure 2 shows the physical graph of the pipeline for real calculations. Input text can be received by any machine. After that, during IDF computing the corresponding inputs reshuffle for balancing workload and final TF-IDF features aggregate in TF-IDF stage. In final stage text classifier obtains a label for the text and returns to the user.

In addition, the two figures above provide a scheme for a partial fitting. Some of the text documents is labelled and such documents accumulate in the Partial Fit vertex. Additional training is triggered by a special element, which is submitted similarly to the input. This process is analogous to punctuation processing \cite{tucker2003exploiting}. The conditions, when the partial fitting starts, can be chosen arbitrarily, for example, train on each 10000 documents. During this process, the existing classifier's weights are being updated as it is described below. After that, new weights is distributed among the system and the classification continues. 

\subsection{ML model}

The classifier's model can be chosen independently from other computations. In our case, we use Multinomial Logistic Regression. The initial classifier parameters such as weights provided by a pretrain process, which can be executed using sklearn library. We vectorise texts using TfidfTransformer class and the model fits the documents by SGDClassifier class with l1 regularization. The regularization provides us weights as sparse matrix.

The following fitting process can be described in terms of optimization of a cost function. This function in our case is written below:

\begin{center}

$$ J(W) = -\frac{1}{m} \sum \limits_{i = 1}^{m} \sum \limits_{j = 1}^{k} \mathbbm{1}_{\{y^{(i)} == j\}} \cdot \log \frac{\exp\left({W_{j}^Tx^{(i)}}\right) }{\sum \limits_{l = 1}^{k}  \exp\left({W_{l}^Tx^{(i)}}\right) }$$ 
 $$ +  \lambda_1 ||W||_1 + \lambda_2 ||W - W_{prev}||_2 $$

\end{center} 

The number of points in new dataset is denoted as $m$. The point with index $i$ showed as $x^{(i)}$. The number of classes is $k$. New weights are designated as $W$ and previous weights are $W_{prev}$.

The formula provides the goal of the training. The first component is standard softmax function for multiple classes. The second component keeps the l1 regularization of the weights. To use previous history of the classifier weights we apply l2 regularization as the third component. Fitting new points and the consideration of the previous weights ensure better accuracy of the classifier.

We interested in finding such $W$ that minimizes $J(W)$. Taking derivatives, one can show that the gradient for each class component is:

\begin{center}

$$ \nabla_{W_j} \; J(W) = -\frac{1}{m} \sum \limits_{i = 1}^{m} \left[ x^{(i)} \left( \mathbbm{1}_{\{y^{(i)} == j\} } - \frac{\exp\left({W_{j}^Tx^{(i)}}\right)}{\sum \limits_{l = 1}^{k}  \exp\left({W_{l}^Tx^{(i)}}\right)} \right) \right] $$
$$ - \; \lambda_1 sign(W) - \frac{\lambda_2}{2} \left(W - W_{prev} \right), j = [1..k] $$

\end{center} 

This formula is applied to each component during one step of gradient descent. There are a few steps of the gradient is executed and new models' parameters is distributed among shards for further classification.