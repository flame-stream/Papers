\label {fs-short-intro}

Multi-classification of large text streams is hard, but important task that researchers and practitioners face. It has a wide range of applications including detection of emerging news and current user interests, suspicious traffic analysis, spam detection, etc. Regarding this task, the following challenges arise:

\textbf{Low latency.} The key performance metric in most streaming applications is the latency between input element arrival and result delivery. There is a need to achieve as small latency as possible.

\textbf{Scalability.} On the other hand, the classification framework must be scalable in terms of throughput in order to handle high-volume streams.

\textbf{Online model updating.} Machine learning model should be updated with new training data on the fly: redeployments of a new model may cause significant latency spikes.

\textbf{Fault tolerance}. If training data can be lost or duplicated in case of failures, it can lead to a non-representative training set and, hence, biased model parameters of a machine learning model~\cite{Baylor:2017:TTP:3097983.3098021}.

\textbf{Interpretability of results.} In order to make predictions interpretable, a framework should provide the same results on the same data within different deployments.

\textbf{Handling the concept drift.} In high-velocity streams, words can have different meanings depending on recent events. Mitigation of this problem would be a benefit for a framework.

There are several existing solutions, which solve some of the mentioned problems. Popular open-source libraries like sklearn~\cite{sklearn_api} and NLTK~\cite{bird2009natural} provide a rich set of tools for text classification. However, these libraries mostly aim at handling static datasets. The lack of scalability across multiple computational units is another restriction of these solutions. There are plenty of works which adopt batch processing systems for text classification~\cite{semberecki2016distributed, svyatkovskiy2016large, baltas2016apache, Nodarakis2016LargeSS}. Their advantages are high throughput and practically unlimited scalability. However, these solutions do not fit in a low latency requirement.

A natural idea is to apply a distributed stream processing engine to the problem. Without loss of generality, let us consider the news topic classification task that can be generalized to any other one. If stream processing system does not support {\em exactly once} delivery guarantee, failures may cause significant bias of emerging topics due to duplicates or losses. In other words, there would be a hidden parameter that directly affects a machine learning model. However, most state-of-the-art stream processing systems do not provide exactly once like Storm~\cite{apache:storm} and Samza~\cite{Noghabi:2017:SSS:3137765.3137770}, while others, like Flink~\cite{Carbone:2017:SMA:3137765.3137777} and Spark Streaming~\cite{Zaharia:2012:DSE:2342763.2342773} have high latency overhead on these mechanisms. Another issue is that most streaming systems are non-deterministic and may provide different results within runs on the same input data. This property makes it difficult to interpret the results and reduces testing and validation capabilities. For example, it can be unclear which news events affect model parameters more than others. 

In this work, we introduce a streaming classification framework on top of~\FlameStream\ processing engine~\cite{we2018beyondmr, we2018adbis}. The main features of~\FlameStream\ are determinism and exactly once with extremely low performance overhead. Hence, this engine allows us to achieve predictable and interpretable results with fault tolerance under low latency requirements. We design a data flow that supports online model updating and can deal with concept drift. We demonstrate that the proposed framework has linear throughput scalability trend with low latency (under 100 ms for 99th percentile) while providing reasonable classifier accuracy.

The rest of the paper is structured as follows: proposed classification framework is detailed in section~\ref{fs-framework}, the performance of the data flow and machine learning model is discussed in section~\ref{fs-experiments}, prior works on the topic are mentioned in section~\ref{fs-related}.
