\label {fs-short-intro}

Multi-classification of large text streams is hard, but important task that researchers and practitioners face. It has a wide range of applications including detection of emerging news and current user interests, suspicious traffic analysis, spam detection, etc. Regarding this task, the following challenges arise:

\textbf{Low latency.} The key performance metric in most streaming applications is the latency between input element arrival and result delivery. There is a need to achieve as small latency as possible.

\textbf{Scalability.} On the other hand, the classification framework must be scalable in terms of throughput in order to handle high-volume streams. Text features must be computed in a distributed manner, otherwise, this part of the pipeline would be a bottleneck.

\textbf{Online model updating.} Solution should be able to update a machine learning model with new training data on the fly. Redeployments with a new model can cause significant latency spikes.

\textbf{Predictability of results.} In order to build a stable framework that is easy to test and validate, there is a need to obtain deterministic predictable results. 

\textbf{Guarantees on data}. In many applications, data can be expensive, and it is not allowed to lose its part. On the other hand, data duplication is also not permitted in some cases. Hence, a framework must guarantee that data will not be lost or duplicated even in case of failures. This guarantee is usually claimed as {\em exactly once}.

\textbf{Handling the concept drift.} Streaming nature of data implies high data velocity. For instance, words can have different meanings depending on user interest or recent events. Such behavior can shift the labels of the input texts. A solution should overcome this problem to achieve more reliable results.

There are several existing solutions, which solve some of the mentioned problems. Popular open-source libraries like sklearn~\cite{sklearn_api} and NLTK~\cite{bird2009natural} provide a rich set of tools for text classification. However, these libraries mostly aim at handling static datasets. The lack of scalability across multiple computational units is another restriction of these solutions. There are plenty of works which adopt batch processing systems for text classification~\cite{semberecki2016distributed, svyatkovskiy2016large, baltas2016apache, Nodarakis2016LargeSS}. Their advantages are high throughput and practically unlimited scalability. However, these solutions do not fit in a low latency requirement.

A natural idea is to apply a distributed stream processing engine to the task. However, most state-of-the-art stream processing systems are not deterministic and do not provide strong guarantees on data like Storm~\cite{apache:storm} and Samza~\cite{Noghabi:2017:SSS:3137765.3137770}, while others, like Flink~\cite{Carbone:2017:SMA:3137765.3137777} and Spark Streaming~\cite{Zaharia:2012:DSE:2342763.2342773} have high latency overhead on these mechanisms. 

In this work, we introduce a streaming classification framework on top of~\FlameStream\ processing engine~\cite{we2018beyondmr, we2018adbis}. The main features of~\FlameStream\ are determinism and exactly once with extremely low performance overhead. Hence, this engine allows us to achieve determinism and exactly once under low latency requirements. We design a pipeline that supports online model updating and can deal with concept drift. We demonstrate that the proposed framework has linear scalability trend with low latency (under 100 ms for 99th percentile) while providing reasonable classifier accuracy.

The rest of the paper is structured as follows:
