%%% fs-run-time-intro - Introduction

\label {fs-intro-seciton}

A need to process huge amounts of data (e.g. Internet scale) was addressed by distributed data processing systems such as Google's MapReduce~\cite{Dean:2008:MSD:1327452.1327492} and Apache Hadoop~\cite{hadoop}. These systems are able to run in a massively parallel fashion on clusters consisting of thousands of commodity computational units. The main advantages of this approach are fault-tolerance and practically unlimited scalability.

Several drawbacks of the initial models and architectures of this kind are deeply analyzed in~\cite{Doulkeridis:2014:SLA:2628707.2628782}. 
Many of these drawbacks were addressed in the next generation of scalable distributed data processing architectures, e.g. 
Asterix~\cite{Alsubaiee:2012:ASW:2331801.2331803}, 
Spark~\cite{Zaharia:2016:ASU:3013530.2934664,Franklin:2015:MSB:2684822.2685326}, 
and Flink~\cite{Carbone:2017:SMA:3137765.3137777}. 

The goal of the \FlameStream\ is to address special case of the data processing: real time stream based computing. This case emerges in short term personalization, real-time analytics and other problems demanding both low update latency and flexibility of the computational subsystem. Specifically, the objectives are:

\begin {itemize}
\item Support map/reduce computations
\item Provide ``exactly once'' semantics
\item Be optimistic in terms of collision management
\item Be able to optimize computational layout based on execution statistics
\item Let the keys to get ready independently
\end {itemize}

These nice properties we trade for in-memory processing: the total state volume must be less then total available memory of the compute units. We analysed where the states are coming from and reshaped the operations to optimize the state management while remaining map/reduce complete. 

The optimistic collision management is done over collisions that already happen and it is often more lightweight then prevention mechanisms. We get this property with the ability to replay certain fragments of the stream. These replays demand the computation process to be deterministic. As many other systems we use the graph representation of the computation and dynamically optimize this graph using statistics available. 

``Exactly once'' semantic is arguable topic, we provide this guarantee inside our system but the overall contract could be violated because of source/sink flaws. To achieve this objective we introduce dynamic batching technique, based on Storm~\ref{} Ackers idea. The same mechanism allows us early key availability~\ref{}. 

We follow these principles in our system architecture:

\begin{itemize}
\item The set of basic operations should not require state handling from the user. At the same time, it should provide for specification of arbitrary processing workflows, similar to those found in Spark and Flink.
\item State is the system internal term and is provided to the developer as a simple coupling abstraction.
\item The processing is divided into ticks. Each tick could use different execution graph. These graphs must be equivalent in terms of the resulting stream, but layout the computation differently.
\item The computation is deterministic, that is, repeated processing of the same data should yield the same output
\item Exactly once execution: each output item is valid and get out of the system exactly once even in case of partial system failures.
\end{itemize}

The computation to be performed with \FlameStream\ is specified in terms of predefined operations that are parametrized with user-defined procedures (collectively called {\em business logic} in the jargon of developers community). Each operation accepts a stream or several streams of data items, and produces one or several output streams. The whole computational workflow is described in terms of a graph. The nodes of the graph represent operations, while edges set up logical routing of data items between operations.

The \FlameStream\ runtime organizes the data items to be processed by operations into prioritized queues. The logical workflow graph is replicated to every computing unit in the cluster, providing for scalable execution. The data items are partitioned between computational units based on user-defined hashing. This allows user to control the computational layout.

\subsection{TODO: rethink}

In contrast with existing solutions, \FlameStream\ provides the only operation that maintains state called {\it grouping}. Grouping does not require any state management by a user. Moreover, the structure of its state can be easily made persistent, which is useful for capturing asynchronous snapshots.

The contributions of this paper are the following:

\subsection{TODO: rethink contribution}

\begin {itemize}
\item Definition of the computational model
\item Implementation and proof of the concept
\end {itemize}

\subsection{TODO}

The rest of the paper is structured as follows 
Describe sections here: model~\ref {fs-model-section}
implementation~\ref{fs-implementation-section}
experiments ~\ref{fs-experiments-section}
related work~\ref{fs-related-section}.


\endinput
