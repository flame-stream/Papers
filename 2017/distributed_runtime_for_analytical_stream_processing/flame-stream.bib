Jeffrey Dean and Sanjay Ghemawat. 2008. MapReduce: simplified data processing on large clusters. Commun. ACM 51, 1 (January 2008), 107-113. DOI: https://doi.org/10.1145/1327452.1327492

@article{Dean:2008:MSD:1327452.1327492,
 author = {Dean, Jeffrey and Ghemawat, Sanjay},
 title = {MapReduce: Simplified Data Processing on Large Clusters},
 journal = {Commun. ACM},
 issue_date = {January 2008},
 volume = {51},
 number = {1},
 month = jan,
 year = {2008},
 issn = {0001-0782},
 pages = {107--113},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/1327452.1327492},
 doi = {10.1145/1327452.1327492},
 acmid = {1327492},
 publisher = {ACM},
 address = {New York, NY, USA},
} 
MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.


Carbone, P., Katsifodimos, A., Ewen, S., Markl, V., Haridi, S. and Tzoumas, K., 2015. Apache flink: Stream and batch processing in a single engine. Bulletin of the IEEE Computer Society Technical Committee on Data Engineering, 36(4).
Vancouver	

@article{carbone2015apache,
  title={Apache flink: Stream and batch processing in a single engine},
  author={Carbone, Paris and Katsifodimos, Asterios and Ewen, Stephan and Markl, Volker and Haridi, Seif and Tzoumas, Kostas},
  journal={Bulletin of the IEEE Computer Society Technical Committee on Data Engineering},
  volume={36},
  number={4},
  year={2015},
  publisher={IEEE Computer Society}
}
Apache Flink is an open-source system for processing streaming and batch data. Flink is built on the
philosophy that many classes of data processing applications, including real-time analytics, continuous
data pipelines, historic data processing (batch), and iterative algorithms (machine learning, graph
analysis) can be expressed and executed as pipelined fault-tolerant dataflows. In this paper, we present
Flink’s architecture and expand on how a (seemingly diverse) set of use cases can be unified under a
single execution model


Gábor Horváth, Norbert Pataki, and Márton Balassi. 2017. Code Generation in Serializers and Comparators of Apache Flink. In Proceedings of the 12th Workshop on Implementation, Compilation, Optimization of Object-Oriented Languages, Programs and Systems (ICOOOLPS'17). ACM, New York, NY, USA, Article 5, 6 pages. DOI: https://proxy.library.spbu.ru:3316/10.1145/3098572.3098579

@inproceedings{Horvath:2017:CGS:3098572.3098579,
 author = {Horv\'{a}th, G\'{a}bor and Pataki, Norbert and Balassi, M\'{a}rton},
 title = {Code Generation in Serializers and Comparators of Apache Flink},
 booktitle = {Proceedings of the 12th Workshop on Implementation, Compilation, Optimization of Object-Oriented Languages, Programs and Systems},
 series = {ICOOOLPS'17},
 year = {2017},
 isbn = {978-1-4503-5088-4},
 location = {Barcelona, Spain},
 pages = {5:1--5:6},
 articleno = {5},
 numpages = {6},
 url = {http://proxy.library.spbu.ru:2393/10.1145/3098572.3098579},
 doi = {10.1145/3098572.3098579},
 acmid = {3098579},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Flink, Janino, Java, big data, code generation},
} 

here is a shift in the Big Data world. Applications used to be I/O bound. InfiniBand, SSDs reduced the I/O overhead and more sophisticated algorithms were developed. CPU became a bottleneck for some applications. Using state of the art CPUs, reduced CPU usage can lead to reduced electricity costs even when an application is I/O bound.

Apache Flink is an open source framework for processing streams of data and batch jobs. It is using serialization for wide variety of purposes. Not only for sending data over the network, saving it to the hard disk, or for fault tolerance, but also some of the operators can work on the serialized representation of the data instead of Java objects. This approach can improve the performance significantly. Flink has a custom serialization method that enables operators to work on the serialized formats.

Currently, Apache Flink uses reflection to serialize Plain Old Java Objects (POJOs). Reflection in Java is notoriously slow. Moreover, the structure of the code is harder to optimize for the JIT compiler. As a Google Summer of Code project in 2016, we implemented code generation for serializers and comparators for POJOs to improve the performance of Apache Flink. Flink has a delicate type system which provides us with lots of information about the types that need to be serialized. Using this information it is possible to generate specialized code with great performance.

We achieved more than 6X performance improvement in the serialization which was a 20% overall improvement.




Yi Chen and Behzad Bordbar. 2016. DRESS: a rule engine on spark for event stream processing. In Proceedings of the 3rd IEEE/ACM International Conference on Big Data Computing, Applications and Technologies (BDCAT '16). ACM, New York, NY, USA, 46-51. DOI: https://proxy.library.spbu.ru:3316/10.1145/3006299.3006326

@inproceedings{Chen:2016:DRE:3006299.3006326,
 author = {Chen, Yi and Bordbar, Behzad},
 title = {DRESS: A Rule Engine on Spark for Event Stream Processing},
 booktitle = {Proceedings of the 3rd IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
 series = {BDCAT '16},
 year = {2016},
 isbn = {978-1-4503-4617-7},
 location = {Shanghai, China},
 pages = {46--51},
 numpages = {6},
 url = {http://proxy.library.spbu.ru:2393/10.1145/3006299.3006326},
 doi = {10.1145/3006299.3006326},
 acmid = {3006326},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {apache spark, event stream processing, rule engine},
} 


Rule-based systems process event streams and trigger actions according to pre-defined rule-sets. Over the last three decades, such systems have been widely used in businesses, governments and organisations. However, with today's need to process larger event streams such as events produced in Internet of Things (IoT), current rule-based systems face serious challenges in terms of speed, scalability and fault tolerance. Spark Streaming is emerging as a novel solution to address these challenges. This paper presents an approach for adapting rule-based systems to work with Spark Streaming. We focus on Rete algorithm which is behind many of the current rule engines. We present DRESS (Distributed Rule Engine on Spark Streaming), an infrastructure for executing Rete algorithm on Spark Streaming. In addition, we present an automated method of transforming rules written in Drools' style to be executed on DRESS. The performance of our system was evaluated with the help of a case study.


Christos Doulkeridis and Kjetil NØrvåg. 2014. A survey of large-scale analytical query processing in MapReduce. The VLDB Journal 23, 3 (June 2014), 355-380. DOI=http://proxy.library.spbu.ru:2083/10.1007/s00778-013-0319-9

@article{Doulkeridis:2014:SLA:2628707.2628782,
 author = {Doulkeridis, Christos and Norvaag, Kjetil},
 title = {A Survey of Large-scale Analytical Query Processing in MapReduce},
 journal = {The VLDB Journal},
 issue_date = {June      2014},
 volume = {23},
 number = {3},
 month = jun,
 year = {2014},
 issn = {1066-8888},
 pages = {355--380},
 numpages = {26},
 url = {http://proxy.library.spbu.ru:2083/10.1007/s00778-013-0319-9},
 doi = {10.1007/s00778-013-0319-9},
 acmid = {2628782},
 publisher = {Springer-Verlag New York, Inc.},
 address = {Secaucus, NJ, USA},
 keywords = {Big Data, Data analysis, Large-scale, MapReduce, Query processing, Survey},
} 
Enterprises today acquire vast volumes of data from different sources and leverage this information by means of data analysis to support effective decision-making and provide new functionality and services. The key requirement of data analytics is scalability, simply due to the immense volume of data that need to be extracted, processed, and analyzed in a timely fashion. Arguably the most popular framework for contemporary large-scale data analytics is MapReduce, mainly due to its salient features that include scalability, fault-tolerance, ease of programming, and flexibility. However, despite its merits, MapReduce has evident performance limitations in miscellaneous analytical tasks, and this has given rise to a significant body of research that aim at improving its efficiency, while maintaining its desirable properties. This survey aims to review the state of the art in improving the performance of parallel query processing using MapReduce. A set of the most significant weaknesses and limitations of MapReduce is discussed at a high level, along with solving techniques. A taxonomy is presented for categorizing existing research on MapReduce improvements according to the specific problem they target. Based on the proposed taxonomy, a classification of existing research is provided focusing on the optimization objective. Concluding, we outline interesting directions for future parallel data processing systems.


Sattam Alsubaiee, Alexander Behm, Raman Grover, Rares Vernica, Vinayak Borkar, Michael J. Carey, and Chen Li. 2012. ASTERIX: scalable warehouse-style web data integration. In Proceedings of the Ninth International Workshop on Information Integration on the Web (IIWeb '12). ACM, New York, NY, USA, , Article 2 , 4 pages. DOI=http://proxy.library.spbu.ru:2083/10.1145/2331801.2331803

@inproceedings{Alsubaiee:2012:ASW:2331801.2331803,
 author = {Alsubaiee, Sattam and Behm, Alexander and Grover, Raman and Vernica, Rares and Borkar, Vinayak and Carey, Michael J. and Li, Chen},
 title = {ASTERIX: Scalable Warehouse-style Web Data Integration},
 booktitle = {Proceedings of the Ninth International Workshop on Information Integration on the Web},
 series = {IIWeb '12},
 year = {2012},
 isbn = {978-1-4503-1239-4},
 location = {Scottsdale, Arizona, USA},
 pages = {2:1--2:4},
 articleno = {2},
 numpages = {4},
 url = {http://proxy.library.spbu.ru:2393/10.1145/2331801.2331803},
 doi = {10.1145/2331801.2331803},
 acmid = {2331803},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {ASTERIX, cloud computing, data-intensive computing, hyracks, semistructured data},
} 
A growing wealth of digital information is being generated on a daily basis in social networks, blogs, online communities, etc. Organizations and researchers in a wide variety of domains recognize that there is tremendous value and insight to be gained by warehousing this emerging data and making it available for querying, analysis, and other purposes. This new breed of "Big Data" applications poses challenging requirements against data management platforms in terms of scalability, flexibility, manageability, and analysis capabilities. At UC Irvine, we are building a next-generation database system, called ASTERIX, in response to these trends. We present ongoing work that approaches the following questions: How does data get into the system? What primitives should we provide to better cope with dirty/noisy data? How can we support efficient data analysis on spatial data? Using real examples, we show the capabilities of ASTERIX for ingesting data via feeds, supporting set-similarity predicates for fuzzy matching, and answering spatial aggregation queries.


@article{Carbone:2017:SMA:3137765.3137777,
 author = {Carbone, Paris and Ewen, Stephan and F\'{o}ra, Gyula and Haridi, Seif and Richter, Stefan and Tzoumas, Kostas},
 title = {State Management in Apache Flink\&Reg;: Consistent Stateful Distributed Stream Processing},
 journal = {Proc. VLDB Endow.},
 issue_date = {August 2017},
 volume = {10},
 number = {12},
 month = aug,
 year = {2017},
 issn = {2150-8097},
 pages = {1718--1729},
 numpages = {12},
 url = {https://proxy.library.spbu.ru:3316/10.14778/3137765.3137777},
 doi = {10.14778/3137765.3137777},
 acmid = {3137777},
 publisher = {VLDB Endowment},
} 
Stream processors are emerging in industry as an apparatus that drives analytical but also mission critical services handling the core of persistent application logic. Thus, apart from scalability and low-latency, a rising system need is first-class support for application state together with strong consistency guarantees, and adaptivity to cluster reconfigurations, software patches and partial failures. Although prior systems research has addressed some of these specific problems, the practical challenge lies on how such guarantees can be materialized in a transparent, non-intrusive manner that relieves the user from unnecessary constraints. Such needs served as the main design principles of state management in Apache Flink, an open source, scalable stream processor.

We present Flink's core pipelined, in-flight mechanism which guarantees the creation of lightweight, consistent, distributed snapshots of application state, progressively, without impacting continuous execution. Consistent snapshots cover all needs for system reconfiguration, fault tolerance and version management through coarse grained rollback recovery. Application state is declared explicitly to the system, allowing efficient partitioning and transparent commits to persistent storage. We further present Flink's backend implementations and mechanisms for high availability, external state queries and output commit. Finally, we demonstrate how these mechanisms behave in practice with metrics and large-deployment insights exhibiting the low performance trade-offs of our approach and the general benefits of exploiting asynchrony in continuous, yet sustainable system deployments.


Zhengping Qian, Yong He, Chunzhi Su, Zhuojie Wu, Hongyu Zhu, Taizhi Zhang, Lidong Zhou, Yuan Yu, and Zheng Zhang. 2013.
TimeStream: reliable stream computation in the cloud. In Proceedings of the 8th ACM European Conference on Computer
Systems (EuroSys '13). ACM, New York, NY, USA, 1-14. DOI=http://dx.doi.org/10.1145/2465351.2465353

@inproceedings{Qian:2013:TRS:2465351.2465353,
 author = {Qian, Zhengping and He, Yong and Su, Chunzhi and Wu, Zhuojie and Zhu, Hongyu and Zhang, Taizhi and Zhou, Lidong and Yu, Yuan and Zhang, Zheng},
 title = {TimeStream: Reliable Stream Computation in the Cloud},
 booktitle = {Proceedings of the 8th ACM European Conference on Computer Systems},
 series = {EuroSys '13},
 year = {2013},
 isbn = {978-1-4503-1994-2},
 location = {Prague, Czech Republic},
 pages = {1--14},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/2465351.2465353},
 doi = {10.1145/2465351.2465353},
 acmid = {2465353},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {StreamInsight, cluster computing, distributed stream processing, dynamic reconfiguration, fault-tolerance, real-time, resilient substitution},
} 
TimeStream is a distributed system designed specifically for low-latency continuous processing of big streaming data on a large cluster of commodity machines. The unique characteristics of this emerging application domain have led to a significantly different design from the popular MapReduce-style batch data processing. In particular, we advocate a powerful new abstraction called resilient substitution that caters to the specific needs in this new computation model to handle failure recovery and dynamic reconfiguration in response to load changes. Several real-world applications running on our prototype have been shown to scale robustly with low latency while at the same time maintaining the simple and concise declarative programming model. TimeStream handles an on-line advertising aggregation pipeline at a rate of 700,000 URLs per second with a 2-second delay, while performing sentiment analysis of Twitter data at a peak rate close to 10,000 tweets per second, with approximately 2-second delay.


Sanjeev Kulkarni, Nikunj Bhagat, Maosong Fu, Vikas Kedigehalli, Christopher Kellogg, Sailesh Mittal, Jignesh M. Patel, Karthik Ramasamy, and Siddarth Taneja. 2015. Twitter Heron: Stream Processing at Scale. In Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data (SIGMOD '15). ACM, New York, NY, USA, 239-250. DOI: https://doi.org/10.1145/2723372.2742788


@inproceedings{Kulkarni:2015:THS:2723372.2742788,
 author = {Kulkarni, Sanjeev and Bhagat, Nikunj and Fu, Maosong and Kedigehalli, Vikas and Kellogg, Christopher and Mittal, Sailesh and Patel, Jignesh M. and Ramasamy, Karthik and Taneja, Siddarth},
 title = {Twitter Heron: Stream Processing at Scale},
 booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
 series = {SIGMOD '15},
 year = {2015},
 isbn = {978-1-4503-2758-9},
 location = {Melbourne, Victoria, Australia},
 pages = {239--250},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/2723372.2742788},
 doi = {10.1145/2723372.2742788},
 acmid = {2742788},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {real-time data processing., stream data processing systems},
} 
Storm has long served as the main platform for real-time analytics at Twitter. However, as the scale of data being processed in real-time at Twitter has increased, along with an increase in the diversity and the number of use cases, many limitations of Storm have become apparent. We need a system that scales better, has better debug-ability, has better performance, and is easier to manage -- all while working in a shared cluster infrastructure. We considered various alternatives to meet these needs, and in the end concluded that we needed to build a new real-time stream data processing system. This paper presents the design and implementation of this new system, called Heron. Heron is now the de facto stream data processing engine inside Twitter, and in this paper we also share our experiences from running Heron in production. In this paper, we also provide empirical evidence demonstrating the efficiency and scalability of Heron.


Matei Zaharia, Tathagata Das, Haoyuan Li, Scott Shenker, and Ion Stoica. 2012. Discretized streams: an efficient and fault-tolerant model for stream processing on large clusters. In Proceedings of the 4th USENIX conference on Hot Topics in Cloud Ccomputing (HotCloud'12). USENIX Association, Berkeley, CA, USA, 10-10.

@inproceedings{Zaharia:2012:DSE:2342763.2342773,
 author = {Zaharia, Matei and Das, Tathagata and Li, Haoyuan and Shenker, Scott and Stoica, Ion},
 title = {Discretized Streams: An Efficient and Fault-tolerant Model for Stream Processing on Large Clusters},
 booktitle = {Proceedings of the 4th USENIX Conference on Hot Topics in Cloud Ccomputing},
 series = {HotCloud'12},
 year = {2012},
 location = {Boston, MA},
 pages = {10--10},
 numpages = {1},
 url = {http://dl.acm.org/citation.cfm?id=2342763.2342773},
 acmid = {2342773},
 publisher = {USENIX Association},
 address = {Berkeley, CA, USA},
} 
Many important "big data" applications need to process data arriving in real time. However, current programming models for distributed stream processing are relatively low-level, often leaving the user to worry about consistency of state across the system and fault recovery. Furthermore, the models that provide fault recovery do so in an expensive manner, requiring either hot replication or long recovery times. We propose a new programming model, discretized streams (D-Streams), that offers a high-level functional programming API, strong consistency, and efficient fault recovery. D-Streams support a new recovery mechanism that improves efficiency over the traditional replication and upstream backup solutions in streaming databases: parallel recovery of lost state across the cluster. We have prototyped D-Streams in an extension to the Spark cluster computing framework called Spark Streaming, which lets users seamlessly intermix streaming, batch and interactive queries.


Tyler Akidau, Robert Bradshaw, Craig Chambers, Slava Chernyak, Rafael J. Fernández-Moctezuma, Reuven Lax, Sam McVeety, Daniel Mills, Frances Perry, Eric Schmidt, and Sam Whittle. 2015. The dataflow model: a practical approach to balancing correctness, latency, and cost in massive-scale, unbounded, out-of-order data processing. Proc. VLDB Endow. 8, 12 (August 2015), 1792-1803. DOI=http://dx.doi.org/10.14778/2824032.2824076

@article{Akidau:2015:DMP:2824032.2824076,
 author = {Akidau, Tyler and Bradshaw, Robert and Chambers, Craig and Chernyak, Slava and Fern\'{a}ndez-Moctezuma, Rafael J. and Lax, Reuven and McVeety, Sam and Mills, Daniel and Perry, Frances and Schmidt, Eric and Whittle, Sam},
 title = {The Dataflow Model: A Practical Approach to Balancing Correctness, Latency, and Cost in Massive-scale, Unbounded, Out-of-order Data Processing},
 journal = {Proc. VLDB Endow.},
 issue_date = {August 2015},
 volume = {8},
 number = {12},
 month = aug,
 year = {2015},
 issn = {2150-8097},
 pages = {1792--1803},
 numpages = {12},
 url = {http://dx.doi.org/10.14778/2824032.2824076},
 doi = {10.14778/2824032.2824076},
 acmid = {2824076},
 publisher = {VLDB Endowment},
} 
Unbounded, unordered, global-scale datasets are increasingly common in day-to-day business (e.g. Web logs, mobile usage statistics, and sensor networks). At the same time, consumers of these datasets have evolved sophisticated requirements, such as event-time ordering and windowing by features of the data themselves, in addition to an insatiable hunger for faster answers. Meanwhile, practicality dictates that one can never fully optimize along all dimensions of correctness, latency, and cost for these types of input. As a result, data processing practitioners are left with the quandary of how to reconcile the tensions between these seemingly competing propositions, often resulting in disparate implementations and systems.

We propose that a fundamental shift of approach is necessary to deal with these evolved requirements in modern data processing. We as a field must stop trying to groom unbounded datasets into finite pools of information that eventually become complete, and instead live and breathe under the assumption that we will never know if or when we have seen all of our data, only that new data will arrive, old data may be retracted, and the only way to make this problem tractable is via principled abstractions that allow the practitioner the choice of appropriate tradeoffs along the axes of interest: correctness, latency, and cost.

In this paper, we present one such approach, the Dataflow Model, along with a detailed examination of the semantics it enables, an overview of the core principles that guided its design, and a validation of the model itself via the real-world experiences that led to its development.


Tyler Akidau, Alex Balikov, Kaya Bekiroğlu, Slava Chernyak, Josh Haberman, Reuven Lax, Sam McVeety, Daniel Mills, Paul Nordstrom, and Sam Whittle. 2013. MillWheel: fault-tolerant stream processing at internet scale. Proc. VLDB Endow. 6, 11 (August 2013), 1033-1044. DOI: http://dx.doi.org/10.14778/2536222.2536229

@article{Akidau:2013:MFS:2536222.2536229,
 author = {Akidau, Tyler and Balikov, Alex and Bekiro\u{g}lu, Kaya and Chernyak, Slava and Haberman, Josh and Lax, Reuven and McVeety, Sam and Mills, Daniel and Nordstrom, Paul and Whittle, Sam},
 title = {MillWheel: Fault-tolerant Stream Processing at Internet Scale},
 journal = {Proc. VLDB Endow.},
 issue_date = {August 2013},
 volume = {6},
 number = {11},
 month = aug,
 year = {2013},
 issn = {2150-8097},
 pages = {1033--1044},
 numpages = {12},
 url = {http://dx.doi.org/10.14778/2536222.2536229},
 doi = {10.14778/2536222.2536229},
 acmid = {2536229},
 publisher = {VLDB Endowment},
} 
MillWheel is a framework for building low-latency data-processing applications that is widely used at Google. Users specify a directed computation graph and application code for individual nodes, and the system manages persistent state and the continuous flow of records, all within the envelope of the framework's fault-tolerance guarantees.

This paper describes MillWheel's programming model as well as its implementation. The case study of a continuous anomaly detector in use at Google serves to motivate how many of MillWheel's features are used. MillWheel's programming model provides a notion of logical time, making it simple to write time-based aggregations. MillWheel was designed from the outset with fault tolerance and scalability in mind. In practice, we find that MillWheel's unique combination of scalability, fault tolerance, and a versatile programming model lends itself to a wide variety of problems at Google.


Nathan Marz and James Warren. 2015. Big Data: Principles and Best Practices of Scalable Realtime Data Systems (1st ed.). Manning Publications Co., Greenwich, CT, USA.

@book{Marz:2015:BDP:2717065,
 author = {Marz, Nathan and Warren, James},
 title = {Big Data: Principles and Best Practices of Scalable Realtime Data Systems},
 year = {2015},
 isbn = {1617290343, 9781617290343},
 edition = {1st},
 publisher = {Manning Publications Co.},
 address = {Greenwich, CT, USA},
} 
Services like social networks, web analytics, and intelligent e-commerce often need to manage data at a scale too big for a traditional database. As scale and demand increase, so does Complexity. Fortunately, scalability and simplicity are not mutually exclusiverather than using some trendy technology, a different approach is needed. Big data systems use many machines working in parallel to store and process data, which introduces fundamental challenges unfamiliar to most developers. Big Data shows how to build these systems using an architecture that takes advantage of clustered hardware along with new tools designed specifically to capture and analyze web-scale data. It describes a scalable, easy to understand approach to big data systems that can be built and run by a small team. Following a realistic example, this book guides readers through the theory of big data systems, how to use them in practice, and how to deploy and operate them once they're built. Purchase of the print book comes with an offer of a free PDF, ePub, and Kindle eBook from Manning. Also available is all code from the book.


Michael Franklin. 2015. Making Sense of Big Data with the Berkeley Data Analytics Stack. In Proceedings of the Eighth ACM International Conference on Web Search and Data Mining (WSDM '15). ACM, New York, NY, USA, 1-2. DOI: http://proxy.library.spbu.ru:2083/10.1145/2684822.2685326

@inproceedings{Franklin:2015:MSB:2684822.2685326,
 author = {Franklin, Michael},
 title = {Making Sense of Big Data with the Berkeley Data Analytics Stack},
 booktitle = {Proceedings of the Eighth ACM International Conference on Web Search and Data Mining},
 series = {WSDM '15},
 year = {2015},
 isbn = {978-1-4503-3317-7},
 location = {Shanghai, China},
 pages = {1--2},
 numpages = {2},
 url = {http://proxy.library.spbu.ru:2393/10.1145/2684822.2685326},
 doi = {10.1145/2684822.2685326},
 acmid = {2685326},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {big data},
} 

The Berkeley AMPLab is creating a new approach to data analytics. Launching in early 2011, the lab aims to seamlessly integrate the three main resources available for making sense of data at scale: Algorithms (machine learning and statistical techniques), Machines (in the form of scalable clusters and elastic cloud computing), and People (both individually as analysts and in crowds). The lab is realizing its ideas through the development of a freely-available Open Source software stack called BDAS: the Berkeley Data Analytics Stack. In the four years the lab has been in operation, we've released major components of BDAS. Several of these components have gained significant traction in industry and elsewhere: the Mesos cluster resource manager, the Spark in-memory computation framework, and the Shark query processing system. BDAS features prominently in many industry discussions of the future of the Big Data analytics ecosystem -- a rare degree of impact for an ongoing academic project. Given this initial success, the lab is continuing on its research path, moving "up the stack" to better integrate and support advanced analytics and to make people a full-fledged resource for making sense of data. In this talk, I'll first outline the motivation and insights behind our research approach and describe how we have organized to address the cross-disciplinary nature of Big Data challenges. I will then describe the current state of BDAS with an emphasis on our newest efforts, including some or all of: the GraphX graph processing system, the Velox and MLBase machine learning platforms, and the SampleClean framework for hybrid human/computer data cleaning. Finally I will present our current views of how all the pieces will fit together to form a system that can adaptively bring the right resources to bear on a given data-driven question to meet time, cost and quality requirements throughout the analytics lifecycle.

Matei Zaharia, Reynold S. Xin, Patrick Wendell, Tathagata Das, Michael Armbrust, Ankur Dave, Xiangrui Meng, Josh Rosen, Shivaram Venkataraman, Michael J. Franklin, Ali Ghodsi, Joseph Gonzalez, Scott Shenker, and Ion Stoica. 2016. Apache Spark: a unified engine for big data processing. Commun. ACM 59, 11 (October 2016), 56-65. DOI: https://proxy.library.spbu.ru:3316/10.1145/2934664

@article{Zaharia:2016:ASU:3013530.2934664,
 author = {Zaharia, Matei and Xin, Reynold S. and Wendell, Patrick and Das, Tathagata and Armbrust, Michael and Dave, Ankur and Meng, Xiangrui and Rosen, Josh and Venkataraman, Shivaram and Franklin, Michael J. and Ghodsi, Ali and Gonzalez, Joseph and Shenker, Scott and Stoica, Ion},
 title = {Apache Spark: A Unified Engine for Big Data Processing},
 journal = {Commun. ACM},
 issue_date = {November 2016},
 volume = {59},
 number = {11},
 month = oct,
 year = {2016},
 issn = {0001-0782},
 pages = {56--65},
 numpages = {10},
 url = {http://proxy.library.spbu.ru:2393/10.1145/2934664},
 doi = {10.1145/2934664},
 acmid = {2934664},
 publisher = {ACM},
 address = {New York, NY, USA},
} 
This open source computing framework unifies streaming, batch, and interactive big data workloads to unlock new applications.


Fidge, C.J., 1987. Timestamps in message-passing systems that preserve the partial ordering.

@article{fidge1988timestamps,
  added-at = {2012-01-14T13:04:26.000+0100},
  author = {Fidge, C. J.},
  biburl = {https://www.bibsonomy.org/bibtex/2cc829c490de6ff6203758943b4d3ca84/nosebrain},
  description = {Timestamps in message-passing systems that preserve the partial ordering | Mendeley},
  interhash = {27bbdd2f31ec6e29b97770b3e017cddc},
  intrahash = {cc829c490de6ff6203758943b4d3ca84},
  journal = {Proceedings of the 11th Australian Computer Science Conference},
  keywords = {clock vector},
  number = 1,
  pages = {56–66},
  timestamp = {2012-01-14T13:04:26.000+0100},
  title = {Timestamps in message-passing systems that preserve the partial ordering},
  url = {http://sky.scitech.qut.edu.au/~fidgec/Publications/fidge88a.pdf},
  volume = 10,
  year = 1988
}
Timestamping is a common method of totally ordering events in concurrent programs. However, for applications requiring access to the global state, a total ordering is inappropriate. This paper presents algorithms for timestamping events in both synchronous and asynchronous n1essage-passing programs that allow for access to the partial ordering inherent in a parallel system. The algorithms do not change the con1munications graph or require a central timestamp issuing authority. 


Mattern, F., 1989. Virtual time and global states of distributed systems. Parallel and Distributed Algorithms, 1(23), pp.215-226.

@INPROCEEDINGS{mattern88virtualtime,
    author = {Friedemann Mattern},
    title = {Virtual Time and Global States of Distributed Systems},
    booktitle = {PARALLEL AND DISTRIBUTED ALGORITHMS},
    year = {1988},
    pages = {215--226},
    publisher = {North-Holland}
}
A distributed system can be characterized by the fact that the global state is distributed and that a common time base does not exist. However, the notion of time is an important concept in every day life of our decentralized "real world" and helps to solve problems like getting a consistent population census or determining the potential causality between events. We argue that a linearly ordered structure of time is not (always) adequate for distributed systems and propose a generalized non-standardmodel of time which consists of vectors of clocks. These clock-vectors arepartially orderedand form a lattice. By using timestamps and a simple clock update mechanism the structure of causality is represented in an isomorphic way. The new model of time has a close analogy to Minkowski's relativistic spacetime and leads among others to an interesting characterization of the global state problem. Finally, we present a new algorithm to compute a consistent global snapshot of a distributed system where messages may bereceived out of order.


Derek G. Murray, Frank McSherry, Rebecca Isaacs, Michael Isard, Paul Barham, and Martín Abadi. 2013. Naiad: a timely dataflow system. In Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles (SOSP '13). ACM, New York, NY, USA, 439-455. DOI: https://doi.org/10.1145/2517349.2522738

@inproceedings{Murray:2013:NTD:2517349.2522738,
 author = {Murray, Derek G. and McSherry, Frank and Isaacs, Rebecca and Isard, Michael and Barham, Paul and Abadi, Mart\'{\i}n},
 title = {Naiad: A Timely Dataflow System},
 booktitle = {Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles},
 series = {SOSP '13},
 year = {2013},
 isbn = {978-1-4503-2388-8},
 location = {Farminton, Pennsylvania},
 pages = {439--455},
 numpages = {17},
 url = {http://doi.acm.org/10.1145/2517349.2522738},
 doi = {10.1145/2517349.2522738},
 acmid = {2522738},
 publisher = {ACM},
 address = {New York, NY, USA},
} 
Naiad is a distributed system for executing data parallel, cyclic dataflow programs. It offers the high throughput of batch processors, the low latency of stream processors, and the ability to perform iterative and incremental computations. Although existing systems offer some of these features, applications that require all three have relied on multiple platforms, at the expense of efficiency, maintainability, and simplicity. Naiad resolves the complexities of combining these features in one framework.

A new computational model, timely dataflow, underlies Naiad and captures opportunities for parallelism across a wide class of algorithms. This model enriches dataflow computation with timestamps that represent logical points in the computation and provide the basis for an efficient, lightweight coordination mechanism.

We show that many powerful high-level programming models can be built on Naiad's low-level primitives, enabling such diverse tasks as streaming data analysis, iterative machine learning, and interactive graph mining. Naiad outperforms specialized systems in their target application domains, and its unique features enable the development of new high-performance applications.

@online{amazon:kinesis,
  title = {Amazon Kinesis},
  url = {https://aws.amazon.com/kinesis/}
}
Amazon Kinesis makes it easy to collect, process, and analyze real-time, streaming data so you can get timely insights and react quickly to new information. Amazon Kinesis offers key capabilities to cost effectively process streaming data at any scale, along with the flexibility to choose the tools that best suit the requirements of your application. With Amazon Kinesis, you can ingest real-time data such as application logs, website clickstreams, IoT telemetry data, and more into your databases, data lakes and data warehouses, or build your own real-time applications using this data. Amazon Kinesis enables you to process and analyze data as it arrives and respond in real-time instead of having to wait until all your data is collected before the processing can begin.

@online{apache:storm,
  title = {Apache Storm},
  url = {http://storm.apache.org/}
}
Apache Storm is a free and open source distributed realtime computation system. Storm makes it easy to reliably process unbounded streams of data, doing for realtime processing what Hadoop did for batch processing. Storm is simple, can be used with any programming language, and is a lot of fun to use!

Storm has many use cases: realtime analytics, online machine learning, continuous computation, distributed RPC, ETL, and more. Storm is fast: a benchmark clocked it at over a million tuples processed per second per node. It is scalable, fault-tolerant, guarantees your data will be processed, and is easy to set up and operate.

Storm integrates with the queueing and database technologies you already use. A Storm topology consumes streams of data and processes those streams in arbitrarily complex ways, repartitioning the streams between each stage of the computation however needed. Read more in the tutorial.


Shadi A. Noghabi, Kartik Paramasivam, Yi Pan, Navina Ramesh, Jon Bringhurst, Indranil Gupta, and Roy H. Campbell. 2017. Samza: stateful scalable stream processing at LinkedIn. Proc. VLDB Endow. 10, 12 (August 2017), 1634-1645. DOI: https://doi.org/10.14778/3137765.3137770

@article{Noghabi:2017:SSS:3137765.3137770,
 author = {Noghabi, Shadi A. and Paramasivam, Kartik and Pan, Yi and Ramesh, Navina and Bringhurst, Jon and Gupta, Indranil and Campbell, Roy H.},
 title = {Samza: Stateful Scalable Stream Processing at LinkedIn},
 journal = {Proc. VLDB Endow.},
 issue_date = {August 2017},
 volume = {10},
 number = {12},
 month = aug,
 year = {2017},
 issn = {2150-8097},
 pages = {1634--1645},
 numpages = {12},
 url = {https://doi.org/10.14778/3137765.3137770},
 doi = {10.14778/3137765.3137770},
 acmid = {3137770},
 publisher = {VLDB Endowment},
} 
Distributed stream processing systems need to support stateful processing, recover quickly from failures to resume such processing, and reprocess an entire data stream quickly. We present Apache Samza, a distributed system for stateful and fault-tolerant stream processing. Samza utilizes a partitioned local state along with a low-overhead background changelog mechanism, allowing it to scale to massive state sizes (hundreds of TB) per application. Recovery from failures is sped up by re-scheduling based on Host Affinity. In addition to processing infinite streams of events, Samza supports processing a finite dataset as a stream, from either a streaming source (e.g., Kafka), a database snapshot (e.g., Databus), or a file system (e.g. HDFS), without having to change the application code (unlike the popular Lambda-based architectures which necessitate maintenance of separate code bases for batch and stream path processing).

Samza is currently in use at LinkedIn by hundreds of production applications with more than 10, 000 containers. Samza is an open-source Apache project adopted by many top-tier companies (e.g., LinkedIn, Uber, Netflix, TripAdvisor, etc.). Our experiments show that Samza: a) handles state efficiently, improving latency and throughput by more than 100X compared to using a remote storage; b) provides recovery time independent of state size; c) scales performance linearly with number of containers; and d) supports reprocessing of the data stream quickly and with minimal interference on real-time traffic.


Hunt, P., Konar, M., Junqueira, F.P. and Reed, B., 2010, June. ZooKeeper: Wait-free Coordination for Internet-scale Systems. In USENIX annual technical conference (Vol. 8, p. 9).

@inproceedings{hunt2010zookeeper,
  title={ZooKeeper: Wait-free Coordination for Internet-scale Systems.},
  author={Hunt, Patrick and Konar, Mahadev and Junqueira, Flavio Paiva and Reed, Benjamin},
  booktitle={USENIX annual technical conference},
  volume={8},
  pages={9},
  year={2010},
  organization={Boston, MA, USA}
}
In this paper, we describe ZooKeeper, a service for coordinating
processes of distributed applications. Since
ZooKeeper is part of critical infrastructure, ZooKeeper
aims to provide a simple and high performance kernel
for building more complex coordination primitives at the
client. It incorporates elements from group messaging,
shared registers, and distributed lock services in a replicated,
centralized service. The interface exposed by ZooKeeper
has the wait-free aspects of shared registers with
an event-driven mechanism similar to cache invalidations
of distributed file systems to provide a simple, yet powerful
coordination service.
The ZooKeeper interface enables a high-performance
service implementation. In addition to the wait-free
property, ZooKeeper provides a per client guarantee of
FIFO execution of requests and linearizability for all requests
that change the ZooKeeper state. These design decisions
enable the implementation of a high performance
processing pipeline with read requests being satisfied by
local servers. We show for the target workloads, 2:1
to 100:1 read to write ratio, that ZooKeeper can handle
tens to hundreds of thousands of transactions per second.
This performance allows ZooKeeper to be used extensively
by client applications.


Kreps, J., Narkhede, N. and Rao, J., 2011, June. Kafka: A distributed messaging system for log processing. In Proceedings of the NetDB (pp. 1-7).

@inproceedings{kreps2011kafka,
  title={Kafka: A distributed messaging system for log processing},
  author={Kreps, Jay and Narkhede, Neha and Rao, Jun and others},
  year={2011}
}
Log processing has become a critical component of the data
pipeline for consumer internet companies. We introduce Kafka, a
distributed messaging system that we developed for collecting and
delivering high volumes of log data with low latency. Our system
incorporates ideas from existing log aggregators and messaging
systems, and is suitable for both offline and online message
consumption. We made quite a few unconventional yet practical
design choices in Kafka to make our system efficient and scalable.
Our experimental results show that Kafka has superior
performance when compared to two popular messaging systems.
We have been using Kafka in production for some time and it is
processing hundreds of gigabytes of new data each day.
