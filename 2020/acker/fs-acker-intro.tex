\label {fs-acker-intro}

State-of-the-art distributed stream processing systems such as Flink~\cite{Carbone:2017:SMA:3137765.3137777}, Heron~\cite{Kulkarni:2015:THS:2723372.2742788}, or MillWheel~\cite{Akidau:2013:MFS:2536222.2536229} are able to execute complex dataflows consisted of multiple operators that can be partitioned among workers. Each operator may execute almost arbitrary user-defined code that transforms elements into other ones or filters them out. After multiple transformations, it may not be clear which ingested by a system input element spawned an output. 

To satisfy {\em consistency} and {\em fault tolerance} requirements, modern streaming engines support {\em dependency tracking} between input and output elements. Tracking mechanisms usually provide notifications that some set of ingested elements has already been transformed into outputs by the whole dataflow or its part. Such notifications are required for a plenty of problems: consistent {\bf state snapshotting}~\cite{Akidau:2013:MFS:2536222.2536229, 2015arXiv150608603C} to provide for correct failure-recovery, {\bf transactional processing} to ensure {\em delivery guarantees}~\cite{thepaper, Carbone:2017:SMA:3137765.3137777}, {\bf data producer cleanup}~\cite{Noghabi:2017:SSS:3137765.3137770} to support recovery while persistently storing only a part of input records, and so on. 

Although most of the mentioned problems have been extensively studied for databases~\cite{DBLP:books/mk/WeikumV2002}, classical databases do not commonly face the problem of matching input and output because they mostly work under less strict latency requirements and persistently save information about all applied transformations. For instance, transactional processing methods employed in databases cannot be used as-is in distributed streaming systems without an adaptation of some dependency tracking technique~\cite{2015arXiv150608603C}.

Most streaming systems apply one of the following approaches for dependency tracking: {\em micro-batching} and {\em markers}. In micro-batching, the output element may be originated only by an input one from the same micro-batch. Therefore, the completeness of a micro-batch indicates that all input elements within this batch are transformed into output elements. Markers approach bases on an injection of special input elements into a dataflow. These elements are broadcasted to each partition of the next operator and play the role of separators between ordinary records. Receiving such element from all partitions indicates that all upstream operators have entirely processed a particular set of input elements. 

Both micro-batching and marker approaches have limitations and can induce high overhead on regular processing. Micro-batching has well-known issues with latency-conscious dataflows~\cite{S7530084}. It is hard to track individual records due to the ineffectiveness of very small micro-batches~\cite{Zaharia:2012:DSE:2342763.2342773}. The application of marker-based methods is limited for cyclic dataflows~\cite{Carbone:2017:SMA:3137765.3137777}. Besides, as we show further, markers may induce significant overhead on throughput due to a huge amount of extra network traffic.

In this work, we present \tracker , a dependency tracking technique that provides for individual elements monitoring, while inducing a small overhead on regular processing. On the one hand, \tracker\ is suitable for a precise element-at-time streaming model without the need of micro-batching. On the other hand, it supports cyclic dataflows and requires a small amount of service traffic. \tracker\ can also monitor streaming elements within parts of dataflow as well as marker-based techniques. \tracker\ is deployed as an external agent that can scale out to sustain a high rate of input records. 

This paper complements preliminary publications~\cite{we2018beyondmr, we2018adbis, thepaper}, which describe a stream processing system that uses \tracker\ as a component. In this paper, we detail a formal concept of \tracker , propose its distributed implementation, and evaluate the~\tracker\ performance in a more narrowly focused way. Our main contributions are:
\begin{itemize}
    \item Design of general formal concepts for a dependency tracking mechanism
    \item Transformation of these concepts into the~\tracker\ using ideas from Apache Storm {\em \acker}~\cite{Toshniwal:2014:STO:2588555.2595641}
    \item Demonstration of the \tracker\ practical feasibility and performance insights
\end{itemize}

The rest of the paper is organized as follows: Section~\ref{fs-acker-motivation} gives an overview of existing dependency tracking solutions and practical tasks that require a tracking mechanism. In Section~\ref{fs-acker-design}, we design formal concepts for dependency tracking methods and turn these concepts into the \tracker\ mechanism. Section~\ref{fs-acker-impl} summarizes the implementation of \tracker\ for both centralized and distributed setups with optimizations that can reduce the amount of extra traffic. In Section~\ref{fs-acker-experiments}, we show that the proposed technique is scalable, and can outperform alternatives employed in state-of-the-art stream processing engines. Finally, we discuss our conclusions in Section~\ref{fs-acker-conclusion}.