Jeffrey Dean and Sanjay Ghemawat. 2008. MapReduce: simplified data processing on large clusters. Commun. ACM 51, 1 (January 2008), 107-113. DOI: https://doi.org/10.1145/1327452.1327492

@article{Dean:2008:MSD:1327452.1327492,
 author = {Dean, Jeffrey and Ghemawat, Sanjay},
 title = {MapReduce: Simplified Data Processing on Large Clusters},
 journal = {Commun. ACM},
 issue_date = {January 2008},
 volume = {51},
 number = {1},
 month = jan,
 year = {2008},
 issn = {0001-0782},
 pages = {107--113},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/1327452.1327492},
 doi = {10.1145/1327452.1327492},
 acmid = {1327492},
 publisher = {ACM},
 address = {New York, NY, USA},
} 
MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.


Carbone, P., Katsifodimos, A., Ewen, S., Markl, V., Haridi, S. and Tzoumas, K., 2015. Apache flink: Stream and batch processing in a single engine. Bulletin of the IEEE Computer Society Technical Committee on Data Engineering, 36(4).
Vancouver	

@article{carbone2015apache,
  title={Apache flink: Stream and batch processing in a single engine},
  author={Carbone, Paris and Katsifodimos, Asterios and Ewen, Stephan and Markl, Volker and Haridi, Seif and Tzoumas, Kostas},
  journal={Bulletin of the IEEE Computer Society Technical Committee on Data Engineering},
  volume={36},
  number={4},
  year={2015},
  publisher={IEEE Computer Society}
}
Apache Flink is an open-source system for processing streaming and batch data. Flink is built on the
philosophy that many classes of data processing applications, including real-time analytics, continuous
data pipelines, historic data processing (batch), and iterative algorithms (machine learning, graph
analysis) can be expressed and executed as pipelined fault-tolerant dataflows. In this paper, we present
Flink’s architecture and expand on how a (seemingly diverse) set of use cases can be unified under a
single execution model


Gábor Horváth, Norbert Pataki, and Márton Balassi. 2017. Code Generation in Serializers and Comparators of Apache Flink. In Proceedings of the 12th Workshop on Implementation, Compilation, Optimization of Object-Oriented Languages, Programs and Systems (ICOOOLPS'17). ACM, New York, NY, USA, Article 5, 6 pages. DOI: https://proxy.library.spbu.ru:3316/10.1145/3098572.3098579

@inproceedings{Horvath:2017:CGS:3098572.3098579,
 author = {Horv\'{a}th, G\'{a}bor and Pataki, Norbert and Balassi, M\'{a}rton},
 title = {Code Generation in Serializers and Comparators of Apache Flink},
 booktitle = {Proceedings of the 12th Workshop on Implementation, Compilation, Optimization of Object-Oriented Languages, Programs and Systems},
 series = {ICOOOLPS'17},
 year = {2017},
 isbn = {978-1-4503-5088-4},
 location = {Barcelona, Spain},
 pages = {5:1--5:6},
 articleno = {5},
 numpages = {6},
 doi = {10.1145/3098572.3098579},
 acmid = {3098579},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Flink, Janino, Java, big data, code generation},
} 

here is a shift in the Big Data world. Applications used to be I/O bound. InfiniBand, SSDs reduced the I/O overhead and more sophisticated algorithms were developed. CPU became a bottleneck for some applications. Using state of the art CPUs, reduced CPU usage can lead to reduced electricity costs even when an application is I/O bound.

Apache Flink is an open source framework for processing streams of data and batch jobs. It is using serialization for wide variety of purposes. Not only for sending data over the network, saving it to the hard disk, or for fault tolerance, but also some of the operators can work on the serialized representation of the data instead of Java objects. This approach can improve the performance significantly. Flink has a custom serialization method that enables operators to work on the serialized formats.

Currently, Apache Flink uses reflection to serialize Plain Old Java Objects (POJOs). Reflection in Java is notoriously slow. Moreover, the structure of the code is harder to optimize for the JIT compiler. As a Google Summer of Code project in 2016, we implemented code generation for serializers and comparators for POJOs to improve the performance of Apache Flink. Flink has a delicate type system which provides us with lots of information about the types that need to be serialized. Using this information it is possible to generate specialized code with great performance.

We achieved more than 6X performance improvement in the serialization which was a 20% overall improvement.


Christos Doulkeridis and Kjetil NØrvåg. 2014. A survey of large-scale analytical query processing in MapReduce. The VLDB Journal 23, 3 (June 2014), 355-380. DOI=http://proxy.library.spbu.ru:2083/10.1007/s00778-013-0319-9

@article{Doulkeridis:2014:SLA:2628707.2628782,
 author = {Doulkeridis, Christos and Norvaag, Kjetil},
 title = {A Survey of Large-scale Analytical Query Processing in MapReduce},
 journal = {The VLDB Journal},
 issue_date = {June      2014},
 volume = {23},
 number = {3},
 month = jun,
 year = {2014},
 issn = {1066-8888},
 pages = {355--380},
 numpages = {26},
 publisher = {Springer-Verlag New York, Inc.},
 address = {Secaucus, NJ, USA},
 keywords = {Big Data, Data analysis, Large-scale, MapReduce, Query processing, Survey},
}
 doi = {10.1007/s00778-013-0319-9},
 acmid = {2628782},
 
Enterprises today acquire vast volumes of data from different sources and leverage this information by means of data analysis to support effective decision-making and provide new functionality and services. The key requirement of data analytics is scalability, simply due to the immense volume of data that need to be extracted, processed, and analyzed in a timely fashion. Arguably the most popular framework for contemporary large-scale data analytics is MapReduce, mainly due to its salient features that include scalability, fault-tolerance, ease of programming, and flexibility. However, despite its merits, MapReduce has evident performance limitations in miscellaneous analytical tasks, and this has given rise to a significant body of research that aim at improving its efficiency, while maintaining its desirable properties. This survey aims to review the state of the art in improving the performance of parallel query processing using MapReduce. A set of the most significant weaknesses and limitations of MapReduce is discussed at a high level, along with solving techniques. A taxonomy is presented for categorizing existing research on MapReduce improvements according to the specific problem they target. Based on the proposed taxonomy, a classification of existing research is provided focusing on the optimization objective. Concluding, we outline interesting directions for future parallel data processing systems.


Sattam Alsubaiee, Alexander Behm, Raman Grover, Rares Vernica, Vinayak Borkar, Michael J. Carey, and Chen Li. 2012. ASTERIX: scalable warehouse-style web data integration. In Proceedings of the Ninth International Workshop on Information Integration on the Web (IIWeb '12). ACM, New York, NY, USA, , Article 2 , 4 pages. DOI=http://proxy.library.spbu.ru:2083/10.1145/2331801.2331803

@inproceedings{Alsubaiee:2012:ASW:2331801.2331803,
 author = {Alsubaiee, Sattam and Behm, Alexander and Grover, Raman and Vernica, Rares and Borkar, Vinayak and Carey, Michael J. and Li, Chen},
 title = {ASTERIX: Scalable Warehouse-style Web Data Integration},
 booktitle = {Proceedings of the Ninth International Workshop on Information Integration on the Web},
 series = {IIWeb '12},
 year = {2012},
 isbn = {978-1-4503-1239-4},
 location = {Scottsdale, Arizona, USA},
 pages = {2:1--2:4},
 articleno = {2},
 numpages = {4},
 doi = {10.1145/2331801.2331803},
 acmid = {2331803},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {ASTERIX, cloud computing, data-intensive computing, hyracks, semistructured data},
} 
A growing wealth of digital information is being generated on a daily basis in social networks, blogs, online communities, etc. Organizations and researchers in a wide variety of domains recognize that there is tremendous value and insight to be gained by warehousing this emerging data and making it available for querying, analysis, and other purposes. This new breed of "Big Data" applications poses challenging requirements against data management platforms in terms of scalability, flexibility, manageability, and analysis capabilities. At UC Irvine, we are building a next-generation database system, called ASTERIX, in response to these trends. We present ongoing work that approaches the following questions: How does data get into the system? What primitives should we provide to better cope with dirty/noisy data? How can we support efficient data analysis on spatial data? Using real examples, we show the capabilities of ASTERIX for ingesting data via feeds, supporting set-similarity predicates for fuzzy matching, and answering spatial aggregation queries.


@article{Carbone:2017:SMA:3137765.3137777,
 author = {Carbone, Paris and Ewen, Stephan and F\'{o}ra, Gyula and Haridi, Seif and Richter, Stefan and Tzoumas, Kostas},
 title = {State Management in Apache Flink\&Reg;: Consistent Stateful Distributed Stream Processing},
 journal = {Proc. VLDB},
 issue_date = {August 2017},
 volume = {10},
 number = {12},
 month = aug,
 year = {2017},
 issn = {2150-8097},
 pages = {1718--1729},
 numpages = {12},
 acmid = {3137777},
 publisher = {VLDB Endowment},
} 
 doi = {10.14778/3137765.3137777},

Stream processors are emerging in industry as an apparatus that drives analytical but also mission critical services handling the core of persistent application logic. Thus, apart from scalability and low-latency, a rising system need is first-class support for application state together with strong consistency guarantees, and adaptivity to cluster reconfigurations, software patches and partial failures. Although prior systems research has addressed some of these specific problems, the practical challenge lies on how such guarantees can be materialized in a transparent, non-intrusive manner that relieves the user from unnecessary constraints. Such needs served as the main design principles of state management in Apache Flink, an open source, scalable stream processor.

We present Flink's core pipelined, in-flight mechanism which guarantees the creation of lightweight, consistent, distributed snapshots of application state, progressively, without impacting continuous execution. Consistent snapshots cover all needs for system reconfiguration, fault tolerance and version management through coarse grained rollback recovery. Application state is declared explicitly to the system, allowing efficient partitioning and transparent commits to persistent storage. We further present Flink's backend implementations and mechanisms for high availability, external state queries and output commit. Finally, we demonstrate how these mechanisms behave in practice with metrics and large-deployment insights exhibiting the low performance trade-offs of our approach and the general benefits of exploiting asynchrony in continuous, yet sustainable system deployments.


Zhengping Qian, Yong He, Chunzhi Su, Zhuojie Wu, Hongyu Zhu, Taizhi Zhang, Lidong Zhou, Yuan Yu, and Zheng Zhang. 2013.
TimeStream: reliable stream computation in the cloud. In Proceedings of the 8th ACM European Conference on Computer
Systems (EuroSys '13). ACM, New York, NY, USA, 1-14. DOI=http://dx.doi.org/10.1145/2465351.2465353

@inproceedings{Qian:2013:TRS:2465351.2465353,
 author = {Qian, Zhengping and He, Yong and Su, Chunzhi and Wu, Zhuojie and Zhu, Hongyu and Zhang, Taizhi and Zhou, Lidong and Yu, Yuan and Zhang, Zheng},
 title = {TimeStream: Reliable Stream Computation in the Cloud},
 booktitle = {Proceedings of the 8th ACM European Conference on Computer Systems},
 series = {EuroSys '13},
 year = {2013},
 isbn = {978-1-4503-1994-2},
 location = {Prague, Czech Republic},
 pages = {1--14},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/2465351.2465353},
 doi = {10.1145/2465351.2465353},
 acmid = {2465353},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {StreamInsight, cluster computing, distributed stream processing, dynamic reconfiguration, fault-tolerance, real-time, resilient substitution},
} 
TimeStream is a distributed system designed specifically for low-latency continuous processing of big streaming data on a large cluster of commodity machines. The unique characteristics of this emerging application domain have led to a significantly different design from the popular MapReduce-style batch data processing. In particular, we advocate a powerful new abstraction called resilient substitution that caters to the specific needs in this new computation model to handle failure recovery and dynamic reconfiguration in response to load changes. Several real-world applications running on our prototype have been shown to scale robustly with low latency while at the same time maintaining the simple and concise declarative programming model. TimeStream handles an on-line advertising aggregation pipeline at a rate of 700,000 URLs per second with a 2-second delay, while performing sentiment analysis of Twitter data at a peak rate close to 10,000 tweets per second, with approximately 2-second delay.


Sanjeev Kulkarni, Nikunj Bhagat, Maosong Fu, Vikas Kedigehalli, Christopher Kellogg, Sailesh Mittal, Jignesh M. Patel, Karthik Ramasamy, and Siddarth Taneja. 2015. Twitter Heron: Stream Processing at Scale. In Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data (SIGMOD '15). ACM, New York, NY, USA, 239-250. DOI: https://doi.org/10.1145/2723372.2742788


@inproceedings{Kulkarni:2015:THS:2723372.2742788,
 author = {Kulkarni, Sanjeev and Bhagat, Nikunj and Fu, Maosong and Kedigehalli, Vikas and Kellogg, Christopher and Mittal, Sailesh and Patel, Jignesh M. and Ramasamy, Karthik and Taneja, Siddarth},
 title = {Twitter Heron: Stream Processing at Scale},
 booktitle = {Proc. of the 2015 ACM SIGMOD Intnl. Conf. on Management of Data},
 series = {SIGMOD '15},
 year = {2015},
 isbn = {978-1-4503-2758-9},
 location = {Melbourne, Victoria, Australia},
 pages = {239--250},
 numpages = {12},
 doi = {10.1145/2723372.2742788},
 acmid = {2742788},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {real-time data processing., stream data processing systems},
} 
 url = {http://doi.acm.org/10.1145/2723372.2742788},

Storm has long served as the main platform for real-time analytics at Twitter. However, as the scale of data being processed in real-time at Twitter has increased, along with an increase in the diversity and the number of use cases, many limitations of Storm have become apparent. We need a system that scales better, has better debug-ability, has better performance, and is easier to manage -- all while working in a shared cluster infrastructure. We considered various alternatives to meet these needs, and in the end concluded that we needed to build a new real-time stream data processing system. This paper presents the design and implementation of this new system, called Heron. Heron is now the de facto stream data processing engine inside Twitter, and in this paper we also share our experiences from running Heron in production. In this paper, we also provide empirical evidence demonstrating the efficiency and scalability of Heron.


Matei Zaharia, Tathagata Das, Haoyuan Li, Scott Shenker, and Ion Stoica. 2012. Discretized streams: an efficient and fault-tolerant model for stream processing on large clusters. In Proceedings of the 4th USENIX conference on Hot Topics in Cloud Ccomputing (HotCloud'12). USENIX Association, Berkeley, CA, USA, 10-10.

@inproceedings{Zaharia:2012:DSE:2342763.2342773,
 author = {Zaharia, Matei and Das, Tathagata and Li, Haoyuan and Shenker, Scott and Stoica, Ion},
 title = {Discretized Streams: An Efficient and Fault-tolerant Model for Stream Processing on Large Clusters},
 booktitle = {Proc. of the 4th USENIX Conf. on Hot Topics in Cloud Ccomputing},
 series = {HotCloud'12},
 year = {2012},
 location = {Boston, MA},
 pages = {10--10},
 numpages = {1},
 acmid = {2342773},
 publisher = {USENIX Association},
 address = {Berkeley, CA, USA},
} 
Many important "big data" applications need to process data arriving in real time. However, current programming models for distributed stream processing are relatively low-level, often leaving the user to worry about consistency of state across the system and fault recovery. Furthermore, the models that provide fault recovery do so in an expensive manner, requiring either hot replication or long recovery times. We propose a new programming model, discretized streams (D-Streams), that offers a high-level functional programming API, strong consistency, and efficient fault recovery. D-Streams support a new recovery mechanism that improves efficiency over the traditional replication and upstream backup solutions in streaming databases: parallel recovery of lost state across the cluster. We have prototyped D-Streams in an extension to the Spark cluster computing framework called Spark Streaming, which lets users seamlessly intermix streaming, batch and interactive queries.


Tyler Akidau, Robert Bradshaw, Craig Chambers, Slava Chernyak, Rafael J. Fernández-Moctezuma, Reuven Lax, Sam McVeety, Daniel Mills, Frances Perry, Eric Schmidt, and Sam Whittle. 2015. The dataflow model: a practical approach to balancing correctness, latency, and cost in massive-scale, unbounded, out-of-order data processing. Proc. VLDB Endow. 8, 12 (August 2015), 1792-1803. DOI=http://dx.doi.org/10.14778/2824032.2824076

@article{Akidau:2015:DMP:2824032.2824076,
 author = {Akidau, Tyler and Bradshaw, Robert and Chambers, Craig and Chernyak, Slava and Fern\'{a}ndez-Moctezuma, Rafael J. and Lax, Reuven and McVeety, Sam and Mills, Daniel and Perry, Frances and Schmidt, Eric and Whittle, Sam},
 title = {The Dataflow Model: A Practical Approach to Balancing Correctness, Latency, and Cost in Massive-scale, Unbounded, Out-of-order Data Processing},
 journal = {Proc. VLDB },
 issue_date = {August 2015},
 volume = {8},
 number = {12},
 month = aug,
 year = {2015},
 issn = {2150-8097},
 pages = {1792--1803},
 numpages = {12},
 doi = {10.14778/2824032.2824076},
 acmid = {2824076},
 publisher = {VLDB Endowment},
} 
#   url = {http://dx.doi.org/10.14778/2824032.2824076},

Unbounded, unordered, global-scale datasets are increasingly common in day-to-day business (e.g. Web logs, mobile usage statistics, and sensor networks). At the same time, consumers of these datasets have evolved sophisticated requirements, such as event-time ordering and windowing by features of the data themselves, in addition to an insatiable hunger for faster answers. Meanwhile, practicality dictates that one can never fully optimize along all dimensions of correctness, latency, and cost for these types of input. As a result, data processing practitioners are left with the quandary of how to reconcile the tensions between these seemingly competing propositions, often resulting in disparate implementations and systems.

We propose that a fundamental shift of approach is necessary to deal with these evolved requirements in modern data processing. We as a field must stop trying to groom unbounded datasets into finite pools of information that eventually become complete, and instead live and breathe under the assumption that we will never know if or when we have seen all of our data, only that new data will arrive, old data may be retracted, and the only way to make this problem tractable is via principled abstractions that allow the practitioner the choice of appropriate tradeoffs along the axes of interest: correctness, latency, and cost.

In this paper, we present one such approach, the Dataflow Model, along with a detailed examination of the semantics it enables, an overview of the core principles that guided its design, and a validation of the model itself via the real-world experiences that led to its development.


Tyler Akidau, Alex Balikov, Kaya Bekiroğlu, Slava Chernyak, Josh Haberman, Reuven Lax, Sam McVeety, Daniel Mills, Paul Nordstrom, and Sam Whittle. 2013. MillWheel: fault-tolerant stream processing at internet scale. Proc. VLDB Endow. 6, 11 (August 2013), 1033-1044. DOI: http://dx.doi.org/10.14778/2536222.2536229

@article{Akidau:2013:MFS:2536222.2536229,
 author = {Akidau, Tyler and Balikov, Alex and Bekiro\u{g}lu, Kaya and Chernyak, Slava and Haberman, Josh and Lax, Reuven and McVeety, Sam and Mills, Daniel and Nordstrom, Paul and Whittle, Sam},
 title = {MillWheel: Fault-tolerant Stream Processing at Internet Scale},
 journal = {Proc. VLDB},
 issue_date = {August 2013},
 volume = {6},
 number = {11},
 month = aug,
 year = {2013},
 issn = {2150-8097},
 pages = {1033--1044},
 numpages = {12},
 acmid = {2536229},
 publisher = {VLDB Endowment},
}
 url = {http://dx.doi.org/10.14778/2536222.2536229},
 doi = {10.14778/2536222.2536229},
  
MillWheel is a framework for building low-latency data-processing applications that is widely used at Google. Users specify a directed computation graph and application code for individual nodes, and the system manages persistent state and the continuous flow of records, all within the envelope of the framework's fault-tolerance guarantees.

This paper describes MillWheel's programming model as well as its implementation. The case study of a continuous anomaly detector in use at Google serves to motivate how many of MillWheel's features are used. MillWheel's programming model provides a notion of logical time, making it simple to write time-based aggregations. MillWheel was designed from the outset with fault tolerance and scalability in mind. In practice, we find that MillWheel's unique combination of scalability, fault tolerance, and a versatile programming model lends itself to a wide variety of problems at Google.


Nathan Marz and James Warren. 2015. Big Data: Principles and Best Practices of Scalable Realtime Data Systems (1st ed.). Manning Publications Co., Greenwich, CT, USA.

@book{Marz:2015:BDP:2717065,
 author = {Marz, Nathan and Warren, James},
 title = {Big Data: Principles and Best Practices of Scalable Realtime Data Systems},
 year = {2015},
 isbn = {1617290343, 9781617290343},
 edition = {1st},
 publisher = {Manning Publications Co.},
 address = {Greenwich, CT, USA},
} 
Services like social networks, web analytics, and intelligent e-commerce often need to manage data at a scale too big for a traditional database. As scale and demand increase, so does Complexity. Fortunately, scalability and simplicity are not mutually exclusiverather than using some trendy technology, a different approach is needed. Big data systems use many machines working in parallel to store and process data, which introduces fundamental challenges unfamiliar to most developers. Big Data shows how to build these systems using an architecture that takes advantage of clustered hardware along with new tools designed specifically to capture and analyze web-scale data. It describes a scalable, easy to understand approach to big data systems that can be built and run by a small team. Following a realistic example, this book guides readers through the theory of big data systems, how to use them in practice, and how to deploy and operate them once they're built. Purchase of the print book comes with an offer of a free PDF, ePub, and Kindle eBook from Manning. Also available is all code from the book.


Michael Franklin. 2015. Making Sense of Big Data with the Berkeley Data Analytics Stack. In Proceedings of the Eighth ACM International Conference on Web Search and Data Mining (WSDM '15). ACM, New York, NY, USA, 1-2. DOI: http://proxy.library.spbu.ru:2083/10.1145/2684822.2685326

@inproceedings{Franklin:2015:MSB:2684822.2685326,
 author = {Franklin, Michael},
 title = {Making Sense of Big Data with the Berkeley Data Analytics Stack},
 booktitle = {Proceedings of the Eighth ACM International Conference on Web Search and Data Mining},
 series = {WSDM '15},
 year = {2015},
 isbn = {978-1-4503-3317-7},
 location = {Shanghai, China},
 pages = {1--2},
 numpages = {2},
 doi = {10.1145/2684822.2685326},
 acmid = {2685326},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {big data},
} 

The Berkeley AMPLab is creating a new approach to data analytics. Launching in early 2011, the lab aims to seamlessly integrate the three main resources available for making sense of data at scale: Algorithms (machine learning and statistical techniques), Machines (in the form of scalable clusters and elastic cloud computing), and People (both individually as analysts and in crowds). The lab is realizing its ideas through the development of a freely-available Open Source software stack called BDAS: the Berkeley Data Analytics Stack. In the four years the lab has been in operation, we've released major components of BDAS. Several of these components have gained significant traction in industry and elsewhere: the Mesos cluster resource manager, the Spark in-memory computation framework, and the Shark query processing system. BDAS features prominently in many industry discussions of the future of the Big Data analytics ecosystem -- a rare degree of impact for an ongoing academic project. Given this initial success, the lab is continuing on its research path, moving "up the stack" to better integrate and support advanced analytics and to make people a full-fledged resource for making sense of data. In this talk, I'll first outline the motivation and insights behind our research approach and describe how we have organized to address the cross-disciplinary nature of Big Data challenges. I will then describe the current state of BDAS with an emphasis on our newest efforts, including some or all of: the GraphX graph processing system, the Velox and MLBase machine learning platforms, and the SampleClean framework for hybrid human/computer data cleaning. Finally I will present our current views of how all the pieces will fit together to form a system that can adaptively bring the right resources to bear on a given data-driven question to meet time, cost and quality requirements throughout the analytics lifecycle.

Matei Zaharia, Reynold S. Xin, Patrick Wendell, Tathagata Das, Michael Armbrust, Ankur Dave, Xiangrui Meng, Josh Rosen, Shivaram Venkataraman, Michael J. Franklin, Ali Ghodsi, Joseph Gonzalez, Scott Shenker, and Ion Stoica. 2016. Apache Spark: a unified engine for big data processing. Commun. ACM 59, 11 (October 2016), 56-65. DOI: https://proxy.library.spbu.ru:3316/10.1145/2934664

@article{Zaharia:2016:ASU:3013530.2934664,
 author = {Zaharia, Matei and Xin, Reynold S. and Wendell, Patrick and Das, Tathagata and Armbrust, Michael and Dave, Ankur and Meng, Xiangrui and Rosen, Josh and Venkataraman, Shivaram and Franklin, Michael J. and Ghodsi, Ali and Gonzalez, Joseph and Shenker, Scott and Stoica, Ion},
 title = {Apache Spark: A Unified Engine for Big Data Processing},
 journal = {Commun. ACM},
 issue_date = {November 2016},
 volume = {59},
 number = {11},
 month = oct,
 year = {2016},
 issn = {0001-0782},
 pages = {56--65},
 numpages = {10},
 doi = {10.1145/2934664},
 acmid = {2934664},
 publisher = {ACM},
 address = {New York, NY, USA},
} 
This open source computing framework unifies streaming, batch, and interactive big data workloads to unlock new applications.


Fidge, C.J., 1987. Timestamps in message-passing systems that preserve the partial ordering.

@article{fidge1988timestamps,
  added-at = {2012-01-14T13:04:26.000+0100},
  author = {Fidge, C. J.},
  biburl = {https://www.bibsonomy.org/bibtex/2cc829c490de6ff6203758943b4d3ca84/nosebrain},
  description = {Timestamps in message-passing systems that preserve the partial ordering | Mendeley},
  interhash = {27bbdd2f31ec6e29b97770b3e017cddc},
  intrahash = {cc829c490de6ff6203758943b4d3ca84},
  journal = {Proceedings of the 11th Australian Computer Science Conference},
  keywords = {clock vector},
  number = 1,
  pages = {56--66},
  timestamp = {2012-01-14T13:04:26.000+0100},
  title = {Timestamps in message-passing systems that preserve the partial ordering},
  url = {http://sky.scitech.qut.edu.au/~fidgec/Publications/fidge88a.pdf},
  volume = 10,
  year = 1988
}
Timestamping is a common method of totally ordering events in concurrent programs. However, for applications requiring access to the global state, a total ordering is inappropriate. This paper presents algorithms for timestamping events in both synchronous and asynchronous n1essage-passing programs that allow for access to the partial ordering inherent in a parallel system. The algorithms do not change the con1munications graph or require a central timestamp issuing authority. 


Mattern, F., 1989. Virtual time and global states of distributed systems. Parallel and Distributed Algorithms, 1(23), pp.215-226.

@INPROCEEDINGS{mattern88virtualtime,
    author = {Friedemann Mattern},
    title = {Virtual Time and Global States of Distributed Systems},
    booktitle = {PARALLEL AND DISTRIBUTED ALGORITHMS},
    year = {1988},
    pages = {215--226},
    publisher = {North-Holland}
}
A distributed system can be characterized by the fact that the global state is distributed and that a common time base does not exist. However, the notion of time is an important concept in every day life of our decentralized "real world" and helps to solve problems like getting a consistent population census or determining the potential causality between events. We argue that a linearly ordered structure of time is not (always) adequate for distributed systems and propose a generalized non-standardmodel of time which consists of vectors of clocks. These clock-vectors arepartially orderedand form a lattice. By using timestamps and a simple clock update mechanism the structure of causality is represented in an isomorphic way. The new model of time has a close analogy to Minkowski's relativistic spacetime and leads among others to an interesting characterization of the global state problem. Finally, we present a new algorithm to compute a consistent global snapshot of a distributed system where messages may bereceived out of order.


Derek G. Murray, Frank McSherry, Rebecca Isaacs, Michael Isard, Paul Barham, and Martín Abadi. 2013. Naiad: a timely dataflow system. In Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles (SOSP '13). ACM, New York, NY, USA, 439-455. DOI: https://doi.org/10.1145/2517349.2522738

@inproceedings{Murray:2013:NTD:2517349.2522738,
 author = {Murray, Derek G. and McSherry, Frank and Isaacs, Rebecca and Isard, Michael and Barham, Paul and Abadi, Mart\'{\i}n},
 title = {Naiad: A Timely Dataflow System},
 booktitle = {Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles},
 series = {SOSP '13},
 year = {2013},
 isbn = {978-1-4503-2388-8},
 location = {Farminton, Pennsylvania},
 pages = {439--455},
 numpages = {17},
 url = {http://doi.acm.org/10.1145/2517349.2522738},
 doi = {10.1145/2517349.2522738},
 acmid = {2522738},
 publisher = {ACM},
 address = {New York, NY, USA},
} 
Naiad is a distributed system for executing data parallel, cyclic dataflow programs. It offers the high throughput of batch processors, the low latency of stream processors, and the ability to perform iterative and incremental computations. Although existing systems offer some of these features, applications that require all three have relied on multiple platforms, at the expense of efficiency, maintainability, and simplicity. Naiad resolves the complexities of combining these features in one framework.

A new computational model, timely dataflow, underlies Naiad and captures opportunities for parallelism across a wide class of algorithms. This model enriches dataflow computation with timestamps that represent logical points in the computation and provide the basis for an efficient, lightweight coordination mechanism.

We show that many powerful high-level programming models can be built on Naiad's low-level primitives, enabling such diverse tasks as streaming data analysis, iterative machine learning, and interactive graph mining. Naiad outperforms specialized systems in their target application domains, and its unique features enable the development of new high-performance applications.

@online{amazon:kinesis,
  title = {Amazon Kinesis},
  url = {https://aws.amazon.com/kinesis/}
}
Amazon Kinesis makes it easy to collect, process, and analyze real-time, streaming data so you can get timely insights and react quickly to new information. Amazon Kinesis offers key capabilities to cost effectively process streaming data at any scale, along with the flexibility to choose the tools that best suit the requirements of your application. With Amazon Kinesis, you can ingest real-time data such as application logs, website clickstreams, IoT telemetry data, and more into your databases, data lakes and data warehouses, or build your own real-time applications using this data. Amazon Kinesis enables you to process and analyze data as it arrives and respond in real-time instead of having to wait until all your data is collected before the processing can begin.

@online{apache:flink:state,
  month = feb,
  year = {2018},
  title = {Apache Flink documentation, Working with State},
  url = {https: //ci.apache.org/projects/flink/flink-docs-release-1.4/dev/stream/state/state.html},
  key = {flink-state}
}

@online{apache:storm:state,
  month = feb,
  year = {2018},
  title = {Apache Storm documentation, Storm State Management},
  url = {http://storm.apache.org/releases/1.2.1/State-checkpointing.html},
  key={storm-site}
}

@online{apache:storm:acker,
  month = feb,
  year = {2018},
  title = {Apache Storm documentation, Guaranteeing Message Processing},
  url = {https://storm.apache.org/releases/current/Guaranteeing-message-processing.html},
  key={storm-site}
}

@online{samza:state,
  month = mar,
  year = {2018},
  title = {Samza documentation, State Management},
  url = {https://samza.apache.org/learn/documentation/0.7.0/container/state-management.html}
}

@online{hadoop2009hadoop,
  month = oct,
  year = {2020},
  title={Apache Hadoop},
  key={Apache Hadoop},
  url = {http://hadoop.apache.org/}
}

@online{apache:storm:trident,
  month = mar,
  title={Trident},
  year = {2018},
  key ={ storm Trident},
  url = {http://storm.apache.org/releases/current/Trident-tutorial.html}
}

@online{apache:storm,
  month = oct,
  year = {2017},
  title={Apache Storm},
  key ={Apache Storm},
  url = {http://storm.apache.org/}
}
Apache Storm is a free and open source distributed realtime computation system. Storm makes it easy to reliably process unbounded streams of data, doing for realtime processing what sp did for batch processing. Storm is simple, can be used with any programming language, and is a lot of fun to use!

Storm has many use cases: realtime analytics, online machine learning, continuous computation, distributed RPC, ETL, and more. Storm is fast: a benchmark clocked it at over a million tuples processed per second per node. It is scalable, fault-tolerant, guarantees your data will be processed, and is easy to set up and operate.

Storm integrates with the queueing and database technologies you already use. A Storm topology consumes streams of data and processes those streams in arbitrarily complex ways, repartitioning the streams between each stage of the computation however needed. Read more in the tutorial.


Shadi A. Noghabi, Kartik Paramasivam, Yi Pan, Navina Ramesh, Jon Bringhurst, Indranil Gupta, and Roy H. Campbell. 2017. Samza: stateful scalable stream processing at LinkedIn. Proc. VLDB Endow. 10, 12 (August 2017), 1634-1645. DOI: https://doi.org/10.14778/3137765.3137770

@article{Noghabi:2017:SSS:3137765.3137770,
 author = {Noghabi, Shadi A. and Paramasivam, Kartik and Pan, Yi and Ramesh, Navina and Bringhurst, Jon and Gupta, Indranil and Campbell, Roy H.},
 title = {Samza: Stateful Scalable Stream Processing at LinkedIn},
 journal = {Proc. VLDB Endow.},
 issue_date = {August 2017},
 volume = {10},
 number = {12},
 month = aug,
 year = {2017},
 issn = {2150-8097},
 pages = {1634--1645},
 numpages = {12},
 publisher = {VLDB Endowment},
} 
 doi = {10.14778/3137765.3137770},
 acmid = {3137770},
 
Distributed stream processing systems need to support stateful processing, recover quickly from failures to resume such processing, and reprocess an entire data stream quickly. We present Apache Samza, a distributed system for stateful and fault-tolerant stream processing. Samza utilizes a partitioned local state along with a low-overhead background changelog mechanism, allowing it to scale to massive state sizes (hundreds of TB) per application. Recovery from failures is sped up by re-scheduling based on Host Affinity. In addition to processing infinite streams of events, Samza supports processing a finite dataset as a stream, from either a streaming source (e.g., Kafka), a database snapshot (e.g., Databus), or a file system (e.g. HDFS), without having to change the application code (unlike the popular Lambda-based architectures which necessitate maintenance of separate code bases for batch and stream path processing).

Samza is currently in use at LinkedIn by hundreds of production applications with more than 10, 000 containers. Samza is an open-source Apache project adopted by many top-tier companies (e.g., LinkedIn, Uber, Netflix, TripAdvisor, etc.). Our experiments show that Samza: a) handles state efficiently, improving latency and throughput by more than 100X compared to using a remote storage; b) provides recovery time independent of state size; c) scales performance linearly with number of containers; and d) supports reprocessing of the data stream quickly and with minimal interference on real-time traffic.


Hunt, P., Konar, M., Junqueira, F.P. and Reed, B., 2010, June. ZooKeeper: Wait-free Coordination for Internet-scale Systems. In USENIX annual technical conference (Vol. 8, p. 9).

@inproceedings{hunt2010zookeeper,
  title={ZooKeeper: Wait-free Coordination for Internet-scale Systems.},
  author={Hunt, Patrick and Konar, Mahadev and Junqueira, Flavio Paiva and Reed, Benjamin},
  booktitle={USENIX annual technical conference},
  volume={8},
  pages={9},
  year={2010},
  organization={Boston, MA, USA}
}
In this paper, we describe ZooKeeper, a service for coordinating
processes of distributed applications. Since
ZooKeeper is part of critical infrastructure, ZooKeeper
aims to provide a simple and high performance kernel
for building more complex coordination primitives at the
client. It incorporates elements from group messaging,
shared registers, and distributed lock services in a replicated,
centralized service. The interface exposed by ZooKeeper
has the wait-free aspects of shared registers with
an event-driven mechanism similar to cache invalidations
of distributed file systems to provide a simple, yet powerful
coordination service.
The ZooKeeper interface enables a high-performance
service implementation. In addition to the wait-free
property, ZooKeeper provides a per client guarantee of
FIFO execution of requests and linearizability for all requests
that change the ZooKeeper state. These design decisions
enable the implementation of a high performance
processing pipeline with read requests being satisfied by
local servers. We show for the target workloads, 2:1
to 100:1 read to write ratio, that ZooKeeper can handle
tens to hundreds of thousands of transactions per second.
This performance allows ZooKeeper to be used extensively
by client applications.


Kreps, J., Narkhede, N. and Rao, J., 2011, June. Kafka: A distributed messaging system for log processing. In Proceedings of the NetDB (pp. 1-7).

@inproceedings{kreps2011kafka,
  title={Kafka: A distributed messaging system for log processing},
  author={Kreps, Jay and Narkhede, Neha and Rao, Jun and others},
  booktitle={Proceedings of the NetDB},
  pages={1--7},
  year={2011}
}
Log processing has become a critical component of the data
pipeline for consumer internet companies. We introduce Kafka, a
distributed messaging system that we developed for collecting and
delivering high volumes of log data with low latency. Our system
incorporates ideas from existing log aggregators and messaging
systems, and is suitable for both offline and online message
consumption. We made quite a few unconventional yet practical
design choices in Kafka to make our system efficient and scalable.
Our experimental results show that Kafka has superior
performance when compared to two popular messaging systems.
We have been using Kafka in production for some time and it is
processing hundreds of gigabytes of new data each day.


Michael Isard, Mihai Budiu, Yuan Yu, Andrew Birrell, and Dennis Fetterly. 2007. Dryad: distributed data-parallel programs from sequential building blocks. In Proceedings of the 2nd ACM SIGOPS/EuroSys European Conference on Computer Systems 2007 (EuroSys '07). ACM, New York, NY, USA, 59-72. DOI=http://dx.doi.org/10.1145/1272996.1273005
@inproceedings{Isard:2007:DDD:1272996.1273005,
 author = {Isard, Michael and Budiu, Mihai and Yu, Yuan and Birrell, Andrew and Fetterly, Dennis},
 title = {Dryad: Distributed Data-parallel Programs from Sequential Building Blocks},
 booktitle = {Proceedings of the 2Nd ACM SIGOPS/EuroSys European Conference on Computer Systems 2007},
 series = {EuroSys '07},
 year = {2007},
 isbn = {978-1-59593-636-3},
 location = {Lisbon, Portugal},
 pages = {59--72},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1272996.1273005},
 doi = {10.1145/1272996.1273005},
 acmid = {1273005},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cluster computing, concurrency, dataflow, distributed programming},
} 
Dryad is a general-purpose distributed execution engine for coarse-grain data-parallel applications. A Dryad application combines computational "vertices" with communication "channels" to form a dataflow graph. Dryad runs the application by executing the vertices of this graph on a set of available computers, communicating as appropriate through flies, TCP pipes, and shared-memory FIFOs.

The vertices provided by the application developer are quite simple and are usually written as sequential programs with no thread creation or locking. Concurrency arises from Dryad scheduling vertices to run simultaneously on multiple computers, or on multiple CPU cores within a computer. The application can discover the size and placement of data at run time, and modify the graph as the computation progresses to make efficient use of the available resources.

Dryad is designed to scale from powerful multi-core single computers, through small clusters of computers, to data centers with thousands of computers. The Dryad execution engine handles all the difficult problems of creating a large distributed, concurrent application: scheduling the use of computers and their CPUs, recovering from communication or computer failures, and transporting data between vertices.

@INPROCEEDINGS{4279071, 
author={M. Li and M. Liu and L. Ding and E. A. Rundensteiner and M. Mani}, 
booktitle={Distributed Computing Systems Workshops, 2007. ICDCSW '07. 27th International Conference on}, 
title={Event Stream Processing with Out-of-Order Data Arrival}, 
year={2007}, 
volume={}, 
number={}, 
pages={67-67}, 
keywords={algebra;data handling;distributed processing;mathematical operators;RFID tracking;core stream algebra operators;distributed computing environment;even machine failure;event pattern queries;event stream processing;out-of-order data arrival;real-time intrusion detection;supply chain management;Algebra;Data analysis;Data mining;Delay;Engines;Face;Intrusion detection;Out of order;Radiofrequency identification;Supply chain management}, 
doi={10.1109/ICDCSW.2007.35}, 
ISSN={1545-0678}, 
month={June},}
Complex event processing has become increasingly important in modern applications, ranging from supply chain management for RFID tracking to real-time intrusion detection. The goal is to extract patterns from such event streams in order to make informed decisions in real-time. However, networking latencies and even machine failure may cause events to arrive out-of-order at the event stream processing engine. In this work, we address the problem of processing event pattern queries specified over event streams that may contain out-of-order data. First, we analyze the problems state-of-the-art event stream processing technology would experience when faced with out-of-order data arrival. We then propose a new solution of physical implementation strategies for the core stream algebra operators such as sequence scan and pattern construction, including stack- based data structures and associated purge algorithms. Optimizations for sequence scan and construction as well as state purging to minimize CPU cost and memory consumption are also introduced. Lastly, we conduct an experimental study demonstrating the effectiveness of our approach.

@inproceedings{Wei:2009:SSO:1559845.1559973,
 author = {Wei, Mingzhu and Liu, Mo and Li, Ming and Golovnya, Denis and Rundensteiner, Elke A. and Claypool, Kajal},
 title = {Supporting a Spectrum of Out-of-order Event Processing Technologies: From Aggressive to Conservative Methodologies},
 booktitle = {Proc. of the 2009 ACM SIGMOD Intnl. Conf. on Management of Data},
 year = {2009},
 isbn = {978-1-60558-551-2},
 location = {Providence, Rhode Island, USA},
 pages = {1031--1034},
 numpages = {4},
 acmid = {1559973},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {complex event processing, out-of-order data, stream processing},
} 
 url = {http://doi.acm.org/10.1145/1559845.1559973},
  series = {SIGMOD '09},
 doi = {10.1145/1559845.1559973},


This demonstration presents a complex event processing system which focuses on out-of-order handling. State-of-the-art event stream processing technology experiences significant challenges when faced with out-of-order data arrival including huge system latencies, missing results, and incorrect result generation. We propose two out-of-order handling techniques, conservative and aggressive strategies. We will show the efficiency of our techniques and how they can satisfy various QoS requirements of different applications.

@article{Mutschler:2014:ASP:2659232.2633686,
 author = {Mutschler, Christopher and Philippsen, Michael},
 title = {Adaptive Speculative Processing of Out-of-Order Event Streams},
 journal = {ACM Trans. Internet Technol.},
 issue_date = {July 2014},
 volume = {14},
 number = {1},
 month = aug,
 year = {2014},
 issn = {1533-5399},
 pages = {4:1--4:24},
 articleno = {4},
 numpages = {24},
 url = {http://doi.acm.org/10.1145/2633686},
 doi = {10.1145/2633686},
 acmid = {2633686},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Distributed event processing, low latency, message-oriented middleware, out-of-order event processing, publish/subscribe, speculative processing},
} 
Distributed event-based systems are used to detect meaningful events with low latency in high data-rate event streams that occur in surveillance, sports, finances, etc. However, both known approaches to dealing with the predominant out-of-order event arrival at the distributed detectors have their shortcomings: buffering approaches introduce latencies for event ordering, and stream revision approaches may result in system overloads due to unbounded retraction cascades.
This article presents an adaptive speculative processing technique for out-of-order event streams that enhances typical buffering approaches. In contrast to other stream revision approaches developed so far, our novel technique encapsulates the event detector, uses the buffering technique to delay events but also speculatively processes a portion of it, and adapts the degree of speculation at runtime to fit the available system resources so that detection latency becomes minimal.
Our technique outperforms known approaches on both synthetical data and real sensor data from a realtime locating system (RTLS) with several thousands of out-of-order sensor events per second. Speculative buffering exploits system resources and reduces latency by 40 percent on average.

@article{Li:2008:OPN:1453856.1453890,
 author = {Li, Jin and Tufte, Kristin and Shkapenyuk, Vladislav and Papadimos, Vassilis and Johnson, Theodore and Maier, David},
 title = {Out-of-order Processing: A New Architecture for High-performance Stream Systems},
 journal = {Proc. VLDB Endow.},
 issue_date = {August 2008},
 volume = {1},
 number = {1},
 month = aug,
 year = {2008},
 issn = {2150-8097},
 pages = {274--288},
 numpages = {15},
 publisher = {VLDB Endowment},
} 
 url = {http://dx.doi.org/10.14778/1453856.1453890},
 doi = {10.14778/1453856.1453890},
 acmid = {1453890},


Many stream-processing systems enforce an order on data streams during query evaluation to help unblock blocking operators and purge state from stateful operators. Such in-order processing (IOP) systems not only must enforce order on input streams, but also require that query operators preserve order. This order-preserving requirement constrains the implementation of stream systems and incurs significant performance penalties, particularly for memory consumption. Especially for high-performance, potentially distributed stream systems, the cost of enforcing order can be prohibitive. We introduce a new architecture for stream systems, out-of-order processing (OOP), that avoids ordering constraints. The OOP architecture frees stream systems from the burden of order maintenance by using explicit stream progress indicators, such as punctuation or heartbeats, to unblock and purge operators. We describe the implementation of OOP stream systems and discuss the benefits of this architecture in depth. For example, the OOP approach has proven useful for smoothing workload bursts caused by expensive end-of-window operations, which can overwhelm internal communication paths in IOP approaches. We have implemented OOP in two stream systems, Gigascope and NiagaraST. Our experimental study shows that the OOP approach can significantly outperform IOP in a number of aspects, including memory, throughput and latency.

@inproceedings{Cranor:2003:GSD:872757.872838,
 author = {Cranor, Chuck and Johnson, Theodore and Spataschek, Oliver and Shkapenyuk, Vladislav},
 title = {Gigascope: A Stream Database for Network Applications},
 booktitle = {Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data},
 series = {SIGMOD '03},
 year = {2003},
 isbn = {1-58113-634-X},
 location = {San Diego, California},
 pages = {647--651},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/872757.872838},
 doi = {10.1145/872757.872838},
 acmid = {872838},
 publisher = {ACM},
 address = {New York, NY, USA},
} 
We have developed Gigascope, a stream database for network applications including traffic analysis, intrusion detection, router configuration analysis, network research, network monitoring, and performance monitoring and debugging. Gigascope is undergoing installation at many sites within the AT&T network, including at OC48 routers, for detailed monitoring. In this paper we describe our motivation for and constraints in developing Gigascope, the Gigascope architecture and query language, and performance issues. We conclude with a discussion of stream database research problems we have found in our application.

@article{hammad2004optimizing,
  title={Optimizing in-order execution of continuous queries over streamed sensor data},
  author={Hammad, Moustafa and Aref, Walid and Elmagarmid, Ahmed},
  year={2004},
  publisher={University of Calgary}
}
The widespread use of sensor networks in scientific and
engineering applications leads to increased demand on the
efficient computation of the collected sensor data. Recent
research in sensor and stream data systems adopts the notion
of sliding windows to process continuous queries over
infinite sensor readings. Ordered processing of input data is
essential during query execution for many application scenarios.
In this paper we present three approaches for ordered
execution of continuous sliding window queries over
sensor data. The first approach enforces ordered processing
at the input side of the query execution plan. In the second
approach we utilize the advantage of out-of-order execution
to optimize query operators and enforce an ordered
release of the output results. The third approach is adaptive
and switches between the first and second approaches
to achieve the best overall performance with current input
arrival rates and level of multiprogramming. We study the
performance of the proposed approaches both analytically
and experimentally and under a variety of conditions such
as the asynchronous arrival of input data, and various levels
of multiprogramming. Our performance study is based
on an extensive set of experiments using a realization of the
proposed approaches in a prototype stream query processing
system.

@article{Tucker:2003:EPS:776752.776780,
 author = {Tucker, Peter A. and Maier, David and Sheard, Tim and Fegaras, Leonidas},
 title = {Exploiting Punctuation Semantics in Continuous Data Streams},
 journal = {IEEE Trans. on Knowl. and Data Eng.},
 issue_date = {March 2003},
 volume = {15},
 number = {3},
 month = mar,
 year = {2003},
 issn = {1041-4347},
 pages = {555--568},
 numpages = {14},
 url = {http://dx.doi.org/10.1109/TKDE.2003.1198390},
 doi = {10.1109/TKDE.2003.1198390},
 acmid = {776780},
 publisher = {IEEE Educational Activities Department},
 address = {Piscataway, NJ, USA},
 keywords = {Continuous queries, stream semantics, continuous data streams, query operators, stream iterators.},
}
As most current query processing architectures are already pipelined, it seems logical to apply them to data streams. However, two classes of query operators are impractical for processing long or infinite data streams. Unbounded stateful operators maintain state with no upper bound in size and, so, run out of memory. Blocking operators read an entire input before emitting a single output and, so, might never produce a result. We believe that a priori knowledge of a data stream can permit the use of such operators in some cases. We discuss a kind of stream semantics called punctuated streams. Punctuations in a stream mark the end of substreams allowing us to view an infinite stream as a mixture of finite streams. We introduce three kinds of invariants to specify the proper behavior of operators in the presence of punctuation. Pass invariants define when results can be passed on. Keep invariants define what must be kept in local state to continue successful operation. Propagation invariants define when punctuation can be passed on. We report on our initial implementation and show a strategy for proving implementations of these invariants are faithful to their relational counterparts.

@inproceedings{Srivastava:2004:FTM:1055558.1055596,
 author = {Srivastava, Utkarsh and Widom, Jennifer},
 title = {Flexible Time Management in Data Stream Systems},
 booktitle = {Proc. PODS},
 year = {2004},
 isbn = {158113858X},
 location = {Paris, France},
 pages = {263--274},
 numpages = {12},
 publisher = {ACM},
 address = {New York, NY, USA},
} 
 booktitle = {Proceedings of the Twenty-third ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems},
 series = {PODS '04},
 url = {http://doi.acm.org/10.1145/1055558.1055596},
 doi = {10.1145/1055558.1055596},
 acmid = {1055596},


Continuous queries in a Data Stream Management System (DSMS) rely on time as a basis for windows on streams and for defining a consistent semantics for multiple streams and updatable relations. The system clock in a centralized DSMS provides a convenient and well-behaved notion of time, but often it is more appropriate for a DSMS application to define its own notion of time---its own clock(s), sequence numbers, or other forms of ordering and times-tamping. Flexible application-defined time poses challenges to the DSMS, since streams may be out of order and uncoordinated with each other, they may incur latency reaching the DSMS, and they may pause or stop. We formalize these challenges and specify how to generate heartbeats so that queries can be evaluated correctly and continuously in an application-defined time domain. Our heartbeat generation algorithm is based on parameters capturing skew between streams, unordering within streams, and latency in streams reaching the DSMS. We also describe how to estimate these parameters at run-time, and we discuss how heartbeats can be used for processing continuous queries.

@article{Babu:2004:EKC:1016028.1016032,
 author = {Babu, Shivnath and Srivastava, Utkarsh and Widom, Jennifer},
 title = {Exploiting K-constraints to Reduce Memory Overhead in Continuous Queries over Data Streams},
 journal = {ACM Trans. Database Syst.},
 issue_date = {September 2004},
 volume = {29},
 number = {3},
 month = sep,
 year = {2004},
 issn = {0362-5915},
 pages = {545--580},
 numpages = {36},
 url = {http://doi.acm.org/10.1145/1016028.1016032},
 doi = {10.1145/1016028.1016032},
 acmid = {1016032},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Continuous queries, constraints, data streams},
} 
Continuous queries often require significant run-time state over arbitrary data streams. However, streams may exhibit certain data or arrival patterns, or constraints, that can be detected and exploited to reduce state considerably without compromising correctness. Rather than requiring constraints to be satisfied precisely, which can be unrealistic in a data streams environment, we introduce k-constraints, where k is an adherence parameter specifying how closely a stream adheres to the constraint. (Smaller k's are closer to strict adherence and offer better memory reduction.) We present a query processing architecture, called k-Mon, that detects useful k-constraints automatically and exploits the constraints to reduce run-time state for a wide range of continuous queries. Experimental results showed dramatic state reduction, while only modest computational overhead was incurred for our constraint monitoring and query execution algorithms.

@inproceedings{Li:2007:ESP:1270388.1270975,
 author = {Li, Ming and Liu, Mo and Ding, Luping and Rundensteiner, Elke A. and Mani, Murali},
 title = {Event Stream Processing with Out-of-Order Data Arrival},
 booktitle = {Proceedings of the 27th International Conference on Distributed Computing Systems Workshops},
 series = {ICDCSW '07},
 year = {2007},
 isbn = {0-7695-2838-4},
 pages = {67--},
 url = {http://dx.doi.org/10.1109/ICDCSW.2007.35},
 doi = {10.1109/ICDCSW.2007.35},
 acmid = {1270975},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 
Complex event processing has become increasingly important in modern applications, ranging from supply chain management for RFID tracking to real-time intrusion detection. The goal is to extract patterns from such event streams in order to make informed decisions in real-time. However, networking latencies and even machine failure may cause events to arrive out-of-order at the event stream processing engine. In this work, we address the problem of processing event pattern queries specified over event streams that may contain out-of-order data. First, we analyze the problems state-of-the-art event stream processing technology would experience when faced with out-of-order data arrival. We then propose a new solution of physical implementation strategies for the core stream algebra operators such as sequence scan and pattern construction, including stack-based data structures and associated purge algorithms. Optimizations for sequence scan and construction as well as state purging to minimize CPU cost and memory consumption are also introduced. Lastly, we conduct an experimental study demonstrating the effectiveness of our approach.

@article{Abadi:2003:ANM:950481.950485,
 author = {Abadi, Daniel J. and Carney, Don and \c{C}etintemel, Ugur and Cherniack, Mitch and Convey, Christian and Lee, Sangdon and Stonebraker, Michael and Tatbul, Nesime and Zdonik, Stan},
 title = {Aurora: A New Model and Architecture for Data Stream Management},
 journal = {The VLDB Journal},
 issue_date = {August 2003},
 volume = {12},
 number = {2},
 month = aug,
 year = {2003},
 issn = {1066-8888},
 pages = {120--139},
 numpages = {20},
 publisher = {Springer-Verlag New York, Inc.},
 address = {Secaucus, NJ, USA},
 keywords = {Continuous queries, Data stream management, Database triggers, Quality-of-service, Real-time systems},
} 
 url = {http://dx.doi.org/10.1007/s00778-003-0095-z},
 doi = {10.1007/s00778-003-0095-z},
 acmid = {950485},

Abstract.This paper describes the basic processing model and architecture of Aurora, a new system to manage data streams for monitoring applications. Monitoring applications differ substantially from conventional business data processing. The fact that a software system must process and react to continual inputs from many sources (e.g., sensors) rather than from human operators requires one to rethink the fundamental architecture of a DBMS for this application area. In this paper, we present Aurora, a new DBMS currently under construction at Brandeis University, Brown University, and M.I.T. We first provide an overview of the basic Aurora model and architecture and then describe in detail a stream-oriented set of operators.

@article{Arasu:2006:CCQ:1146461.1146463,
 author = {Arasu, Arvind and Babu, Shivnath and Widom, Jennifer},
 title = {The CQL Continuous Query Language: Semantic Foundations and Query Execution},
 journal = {The VLDB Journal},
 issue_date = {June 2006},
 volume = {15},
 number = {2},
 month = jun,
 year = {2006},
 issn = {1066-8888},
 pages = {121--142},
 numpages = {22},
 publisher = {Springer-Verlag New York, Inc.},
 address = {Secaucus, NJ, USA},
 keywords = {Continuous queries, Data streams, Query language, Query processing},
} 
 url = {http://dx.doi.org/10.1007/s00778-004-0147-z},
 doi = {10.1007/s00778-004-0147-z},
 acmid = {1146463},

CQL, a continuous query language, is supported by the STREAM prototype data stream management system (DSMS) at Stanford. CQL is an expressive SQL-based declarative language for registering continuous queries against streams and stored relations. We begin by presenting an abstract semantics that relies only on “black-box” mappings among streams and relations. From these mappings we define a precise and general interpretation for continuous queries. CQL is an instantiation of our abstract semantics using SQL to map from relations to relations, window specifications derived from SQL-99 to map from streams to relations, and three new operators to map from relations to streams. Most of the CQL language is operational in the STREAM system. We present the structure of CQL's query execution plans as well as details of the most important components: operators, interoperator queues, synopses, and sharing of components among multiple operators and queries. Examples throughout the paper are drawn from the Linear Road benchmark recently proposed for DSMSs. We also curate a public repository of data stream applications that includes a wide variety of queries expressed in CQL. The relative ease of capturing these applications in CQL is one indicator that the language contains an appropriate set of constructs for data stream processing.

@inproceedings{Ding:2004:EWJ:1031171.1031189,
 author = {Ding, Luping and Rundensteiner, Elke A.},
 title = {Evaluating Window Joins over Punctuated Streams},
 booktitle = {Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management},
 series = {CIKM '04},
 year = {2004},
 isbn = {1-58113-874-1},
 location = {Washington, D.C., USA},
 pages = {98--107},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1031171.1031189},
 doi = {10.1145/1031171.1031189},
 acmid = {1031189},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {join algorithm, punctuation, sliding window, streaming data processing},
} 
We explore join optimizations in the presence of both time-based constraints (sliding windows) and value-based constraints (punctuations). We present the first join solution named PWJoin that exploits such combined constraints to shrink the runtime join state and to propagate punctuations to benefit downstream operators. We design a state structure for PWJoin that facilitates the exploitation of both constraint types. We also explore optimizations enabled by the interactions between window and punctuation, e.g., early punctuation propagation. The costs of the PWJoin are analyzed using a cost model. We also conduct an experimental study using CAPE continuous query system. The experimental results show that in most cases, by exploiting punctuations, PWJoin outperforms the pure window join with regard to both memory overhead and throughput. Our technique complements the joins in the literature, such as symmetric hash join or window join, to now require less runtime resources without compromising the accuracy of the result.

@inproceedings{Hammad:2005:OIE:1116877.1116897,
 author = {Hammad, Moustafa A. and Aref, Walid G. and Elmagarmid, Ahmed K.},
 title = {Optimizing In-order Execution of Continuous Queries over Streamed Sensor Data},
 booktitle = {Proceedings of the 17th International Conference on Scientific and Statistical Database Management},
 series = {SSDBM'2005},
 year = {2005},
 isbn = {1-88888-111-X},
 location = {Santa Barbara, CA},
 pages = {143--146},
 numpages = {4},
 url = {http://dl.acm.org/citation.cfm?id=1116877.1116897},
 acmid = {1116897},
 publisher = {Lawrence Berkeley Laboratory},
 address = {Berkeley, CA, US},
} 

@inproceedings{Hammad:2003:SSW:1315451.1315478,
 author = {Hammad, Moustafa A. and Franklin, Michael J. and Aref, Walid G. and Elmagarmid, Ahmed K.},
 title = {Scheduling for Shared Window Joins over Data Streams},
 booktitle = {Proceedings of the 29th International Conference on Very Large Data Bases - Volume 29},
 series = {VLDB '03},
 year = {2003},
 isbn = {0-12-722442-4},
 location = {Berlin, Germany},
 pages = {297--308},
 numpages = {12},
 publisher = {VLDB Endowment},
} 
 url = {http://dl.acm.org/citation.cfm?id=1315451.1315478},
 acmid = {1315478},

Continuous Query (CQ) systems typically exploit commonality among query expressions to achieve improved efficiency through shared processing. Recently proposed CQ systems have introduced window specifications in order to support unbounded data streams. There has been, however, little investigation of sharing for windowed query operators. In this paper, we address the shared execution of windowed joins, a core operator for CQ systems. We show that the strategy used in systems to date has a previously unreported performance flaw that can negatively impact queries with relatively small windows. We then propose two new execution strategies for shared joins. We evaluate the alternatives using both analytical models and implementation in a DBMS. The results show that one strategy, called MQT, provides the best performance over a range of workload settings.

@Article{Li2011,
author="Li, Chuan-Wen
and Gu, Yu
and Yu, Ge
and Hong, Bonghee",
title="Aggressive Complex Event Processing with Confidence over Out-of-Order Streams",
journal="Journal of Computer Science and Technology",
year="2011",
month="Jul",
day="01",
volume="26",
number="4",
pages="685--696",
abstract="In recent years, there has been a growing need for complex event processing (CEP), ranging from supply chain management to security monitoring. In many scenarios events are generated in different sources but arrive at the central server out of order, due to the differences of network latencies. Most state-of-the-art techniques process out-of-order events by buffering the events until the total event order within a specified range can be guaranteed. Their main problems are leading to increasing response time and reducing system throughput. This paper aims to build a high performance out-of-order event processing mechanism, which can match events as soon as they arrive instead of buffering them till all arrive. A suffix-automaton-based event matching algorithm is proposed to speed up query processing, and a confidence-based accuracy evaluation is proposed to control the query result quality. The performance of our approach is evaluated through detailed accuracy and response time analysis. As experimental results show, our approach can obviously speed up the query matching time and produce reasonable query results.",
issn="1860-4749",
doi="10.1007/s11390-011-1168-x",
url="https://doi.org/10.1007/s11390-011-1168-x"
}
In recent years, there has been a growing need for complex event processing (CEP), ranging from supply chain management to security monitoring. In many scenarios events are generated in different sources but arrive at the central server out of order, due to the differences of network latencies. Most state-of-the-art techniques process out-of-order events by buffering the events until the total event order within a specified range can be guaranteed. Their main problems are leading to increasing response time and reducing system throughput. This paper aims to build a high performance out-of-order event processing mechanism, which can match events as soon as they arrive instead of buffering them till all arrive. A suffix-automaton-based event matching algorithm is proposed to speed up query processing, and a confidence-based accuracy evaluation is proposed to control the query result quality. The performance of our approach is evaluated through detailed accuracy and response time analysis. As experimental results show, our approach can obviously speed up the query matching time and produce reasonable query results.

@INPROCEEDINGS{S7530084, 
author={S. Chintapalli and D. Dagit and B. Evans and R. Farivar and T. Graves and M. Holderbaugh and Z. Liu and K. Nusbaum and K. Patil and B. J. Peng and P. Poulosky}, 
booktitle={2016 IEEE Intnl. Parallel and Distributed Processing Symp. Workshops (IPDPSW)}, 
title={Benchmarking Streaming Computation Engines: Storm, Flink and Spark Streaming}, 
year={2016}, 
volume={}, 
number={}, 
pages={1789-1792}, 
abstract={Streaming data processing has been gaining attention due to its application into a wide range of scenarios. To serve the booming demands of streaming data processing, many computation engines have been developed. However, there is still a lack of real-world benchmarks that would be helpful when choosing the most appropriate platform for serving real-time streaming needs. In order to address this problem, we developed a streaming benchmark for three representative computation engines: Flink, Storm and Spark Streaming. Instead of testing speed-of-light event processing, we construct a full data pipeline using Kafka and Redis in order to more closely mimic the real-world production scenarios. Based on our experiments, we provide a performance comparison of the three data engines in terms of 99th percentile latency and throughput for various configurations.}, 
keywords={data analysis;pipeline processing;Flink streaming;Kafka;Redis;Spark streaming;Storm streaming;computation engines;data engines;data pipeline;streaming data processing;Benchmark testing;Data processing;Engines;Pipelines;Sparks;Storms;Throughput;Benchmark;Flink;Low Latency;Spark;Storm;Streaming processing}, 
doi={10.1109/IPDPSW.2016.138}, 
ISSN={}, 
month={May},}

@inproceedings{Zacheilas:2017:MDS:3093742.3093921,
 author = {Zacheilas, Nikos and Kalogeraki, Vana and Nikolakopoulos, Yiannis and Gulisano, Vincenzo and Papatriantafilou, Marina and Tsigas, Philippas},
 title = {Maximizing Determinism in Stream Processing Under Latency Constraints},
 booktitle = {Proc.  of the 11th ACM Intnl. Conf. on Distributed and Event-based Systems},
 series = {DEBS '17},
 year = {2017},
 isbn = {978-1-4503-5065-5},
 location = {Barcelona, Spain},
 pages = {112--123},
 numpages = {12},
 doi = {10.1145/3093742.3093921},
 acmid = {3093921},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Complex Event Processing, Deterministic Processing, Stream Processing},
}
 url = {http://doi.acm.org/10.1145/3093742.3093921},
 
 
The problem of coping with the demands of determinism and meeting latency constraints is challenging in distributed data stream processing systems that have to process high volume data streams that arrive from different unsynchronized input sources. In order to deterministically process the streaming data, they need mechanisms that synchronize the order in which tuples are processed by the operators. On the other hand, achieving real-time response in such a system requires careful tradeoff between determinism and low latency performance. We build on a recently proposed approach to handle data exchange and synchronization in stream processing, namely ScaleGate, which comes with guarantees for determinism and an efficient lock-free implementation, enabling high scalability. Considering the challenge and trade-offs implied by real-time constraints, we propose a system which comprises (a) a novel data structure called Slack-ScaleGate (SSG), along with its algorithmic implementation; SSG enables us to guarantee the deterministic processing of tuples as long as they are able to meet their latency constraints, and (b) a method to dynamically tune the maximum amount of time that a tuple can wait in the SSG data-structure, relaxing the determinism guarantees when needed, in order to satisfy the latency constraints. Our detailed experimental evaluation using a traffic monitoring application deployed in the city of Dublin, illustrates the working and benefits of our approach.

@ARTICLE{2015arXiv150608603C,
   author = {{Carbone}, P. and {F{\'o}ra}, G. and {Ewen}, S. and {Haridi}, S. and 
	{Tzoumas}, K.},
    title = "{Lightweight Asynchronous Snapshots for Distributed Dataflows}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1506.08603},
 primaryClass = "cs.DC",
 keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
     year = 2015,
    month = jun,
   adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150608603C},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
Distributed stateful stream processing enables the deployment and execution of large scale continuous computations in the cloud, targeting both low latency and high throughput. One of the most fundamental challenges of this paradigm is providing processing guarantees under potential failures. Existing approaches rely on periodic global state snapshots that can be used for failure recovery. Those approaches suffer from two main drawbacks. First, they often stall the overall computation which impacts ingestion. Second, they eagerly persist all records in transit along with the operation states which results in larger snapshots than required. In this work we propose Asynchronous Barrier Snapshotting (ABS), a lightweight algorithm suited for modern dataflow execution engines that minimises space requirements. ABS persists only operator states on acyclic execution topologies while keeping a minimal record log on cyclic dataflows. We implemented ABS on Apache Flink, a distributed analytics engine that supports stateful stream processing. Our evaluation shows that our algorithm does not have a heavy impact on the execution, maintaining linear scalability and performing well with frequent snapshots.

@article{Stonebraker:2005:RRS:1107499.1107504,
 author = {Stonebraker, Michael and \c{C}etintemel, U\v{g}ur and Zdonik, Stan},
 title = {The 8 Requirements of Real-time Stream Processing},
 journal = {SIGMOD Rec.},
 volume = {34},
 number = {4},
 month = dec,
 year = {2005},
 issn = {0163-5808},
 pages = {42--47},
 numpages = {6},
 doi = {10.1145/1107499.1107504},
 acmid = {1107504},
 publisher = {ACM},
 address = {New York, NY, USA},
}
 url = {http://doi.acm.org/10.1145/1107499.1107504},
issue_date = {December 2005},
 
Applications that require real-time processing of high-volume data steams are pushing the limits of traditional data processing infrastructures. These stream-based applications include market feed processing and electronic trading on Wall Street, network and infrastructure monitoring, fraud detection, and command and control in military environments. Furthermore, as the "sea change" caused by cheap micro-sensor technology takes hold, we expect to see everything of material significance on the planet get "sensor-tagged" and report its state or location in real time. This sensorization of the real world will lead to a "green field" of novel monitoring and control applications with high-volume and low-latency processing requirements.Recently, several technologies have emerged---including off-the-shelf stream processing engines---specifically to address the challenges of processing high-volume, real-time data without requiring the use of custom code. At the same time, some existing software technologies, such as main memory DBMSs and rule engines, are also being "repurposed" by marketing departments to address these applications.In this paper, we outline eight requirements that a system software should meet to excel at a variety of real-time stream processing applications. Our goal is to provide high-level guidance to information technologists so that they will know what to look for when evaluation alternative stream processing solutions. As such, this paper serves a purpose comparable to the requirements papers in relational DBMSs and on-line analytical processing. We also briefly review alternative system software technologies in the context of our requirements.The paper attempts to be vendor neutral, so no specific commercial products are mentioned.

@article{Cahill:2009:SIS:1620585.1620587,
 author = {Cahill, Michael J. and R\"{o}hm, Uwe and Fekete, Alan D.},
 title = {Serializable Isolation for Snapshot Databases},
 journal = {ACM Trans. Database Syst.},
 issue_date = {December 2009},
 volume = {34},
 number = {4},
 month = dec,
 year = {2009},
 issn = {0362-5915},
 pages = {20:1--20:42},
 articleno = {20},
 numpages = {42},
 url = {http://doi.acm.org/10.1145/1620585.1620587},
 doi = {10.1145/1620585.1620587},
 acmid = {1620587},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Multiversion concurrency control, serializability, snapshot isolation},
}
Many popular database management systems implement a multiversion concurrency control algorithm called snapshot isolation rather than providing full serializability based on locking. There are well-known anomalies permitted by snapshot isolation that can lead to violations of data consistency by interleaving transactions that would maintain consistency if run serially. Until now, the only way to prevent these anomalies was to modify the applications by introducing explicit locking or artificial update conflicts, following careful analysis of conflicts between all pairs of transactions.
This article describes a modification to the concurrency control algorithm of a database management system that automatically detects and prevents snapshot isolation anomalies at runtime for arbitrary applications, thus providing serializable isolation. The new algorithm preserves the properties that make snapshot isolation attractive, including that readers do not block writers and vice versa. An implementation of the algorithm in a relational DBMS is described, along with a benchmark and performance study, showing that the throughput approaches that of snapshot isolation in most cases.

@INPROCEEDINGS{7530084, 
author={S. Chintapalli and D. Dagit and B. Evans and R. Farivar and T. Graves and M. Holderbaugh and Z. Liu and K. Nusbaum and K. Patil and B. J. Peng and P. Poulosky}, 
booktitle={2016 IEEE Intnl. Parallel and Distributed Processing Symposium Workshops (IPDPSW)}, 
title={Benchmarking Streaming Computation Engines: Storm, Flink and Spark Streaming}, 
year={2016}, 
volume={}, 
number={}, 
pages={1789-1792}, 
abstract={Streaming data processing has been gaining attention due to its application into a wide range of scenarios. To serve the booming demands of streaming data processing, many computation engines have been developed. However, there is still a lack of real-world benchmarks that would be helpful when choosing the most appropriate platform for serving real-time streaming needs. In order to address this problem, we developed a streaming benchmark for three representative computation engines: Flink, Storm and Spark Streaming. Instead of testing speed-of-light event processing, we construct a full data pipeline using Kafka and Redis in order to more closely mimic the real-world production scenarios. Based on our experiments, we provide a performance comparison of the three data engines in terms of 99th percentile latency and throughput for various configurations.}, 
keywords={data analysis;pipeline processing;Flink streaming;Kafka;Redis;Spark streaming;Storm streaming;computation engines;data engines;data pipeline;streaming data processing;Benchmark testing;Data processing;Engines;Pipelines;Sparks;Storms;Throughput;Benchmark;Flink;Low Latency;Spark;Storm;Streaming processing}, 
doi={10.1109/IPDPSW.2016.138}, 
ISSN={}, 
month={May},}

@INPROCEEDINGS{7474816, 
author={S. Qian and G. Wu and J. Huang and T. Das}, 
booktitle={2016 IEEE International Conference on Industrial Technology (ICIT)}, 
title={Benchmarking modern distributed streaming platforms}, 
year={2016}, 
volume={}, 
number={}, 
pages={592-598}, 
abstract={The prevalence of big data technology has generated increasing demands in large-scale streaming data processing. However, for certain tasks it is still challenging to appropriately select a platform due to the diversity of choices and the complexity of configurations. This paper focuses on benchmarking some principal streaming platforms. We achieve our goals on StreamBench, a streaming benchmark tool based on which we introduce proper modifications and extensions. We then accomplish performance comparisons among different big data platforms, including Apache Spark, Apache Storm and Apache Samza. In terms of performance criteria, we consider both computational capability and fault-tolerance ability. Finally, we give a summary on some key knobs for performance tuning as well as on hardware utilization.}, 
keywords={Big Data;distributed processing;Apache Samza;Apache Spark;Apache Storm;Big Data technology;StreamBench tool;distributed streaming platforms;large-scale streaming data processing;Benchmark testing;Fault tolerance;Fault tolerant systems;Hardware;Sparks;Storms;Throughput;benchmark;big data;distributed streaming computing;spark streaming;storm}, 
doi={10.1109/ICIT.2016.7474816}, 
ISSN={}, 
month={March},}

#inproceedings{DBLP:conf/cidr/AbadiABCCHLMRRTXZ05,

@inproceedings{abadi2005design,
  author    = {Daniel J. Abadi and
               Yanif Ahmad and
               Magdalena Balazinska and
               Ugur {\c{C}}etintemel and
               Mitch Cherniack and
               Jeong{-}Hyon Hwang and
               Wolfgang Lindner and
               Anurag Maskey and
               Alex Rasin and
               Esther Ryvkina and
               Nesime Tatbul and
               Ying Xing and
               Stanley B. Zdonik},
  title     = {The Design of the Borealis Stream Processing Engine},
  booktitle = {{CIDR} 2005, Second Biennial Conference on Innovative Data Systems
               Research, Asilomar, CA, USA, January 4-7, 2005, Online Proceedings},
  pages     = {277--289},
  year      = {2005},
  crossref  = {DBLP:conf/cidr/2005},
  url       = {http://cidrdb.org/cidr2005/papers/P23.pdf},
  timestamp = {Sun, 05 Aug 2018 22:58:23 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/cidr/AbadiABCCHLMRRTXZ05},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@proceedings{DBLP:conf/cidr/2005,
  title     = {{CIDR} 2005, Second Biennial Conference on Innovative Data Systems
               Research, Asilomar, CA, USA, January 4-7, 2005, Online Proceedings},
  publisher = {www.cidrdb.org},
  year      = {2005},
  url       = {http://cidrdb.org/cidr2005/index.html},
  timestamp = {Thu, 02 Feb 2017 11:12:57 +0100},
  biburl    = {https://dblp.org/rec/bib/conf/cidr/2005},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

Borealis is a second-generation distributed stream processing engine that is being developed at Brandeis University, Brown University, and MIT. Borealis inherits core stream processing functionality from Aurora [14] and distribution functionality from Medusa [51]. Borealis modifies and extends both systems in non-trivial and critical ways to provide advanced capabilities that are commonly required by newly-emerging stream processing applications. In this paper, we outline the basic design and functionality of Borealis. Through sample real-world applications, we motivate the need for dynamically revising query results and modifying query specifications. We then describe how Borealis addresses these challenges through an innovative set of features, including revision records, time travel, and control lines. Finally, we present a highly flexible and scalable QoS-based optimization model that operates across server and sensor networks and a new fault-tolerance model with flexible consistency-availability trade-offs.

@article{Chandy:1985:DSD:214451.214456,
 author = {Chandy, K. Mani and Lamport, Leslie},
 title = {Distributed Snapshots: Determining Global States of Distributed Systems},
 journal = {ACM Trans. Comput. Syst.},
 issue_date = {Feb. 1985},
 volume = {3},
 number = {1},
 month = feb,
 year = {1985},
 issn = {0734-2071},
 pages = {63--75},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/214451.214456},
 doi = {10.1145/214451.214456},
 acmid = {214456},
 publisher = {ACM},
 address = {New York, NY, USA},
} 
This paper presents an algorithm by which a process in a distributed system determines a global state of the system during a computation. Many problems in distributed systems can be cast in terms of the problem of detecting global states. For instance, the global state detection algorithm helps to solve an important class of problems: stable property detection. A stable property is one that persists: once a stable property becomes true it remains true thereafter. Examples of stable properties are “computation has terminated,” “ the system is deadlocked” and “all tokens in a token ring have disappeared.” The stable property detection problem is that of devising algorithms to detect a given stable property. Global state detection can also be used for checkpointing.

@article{jacques2016consistent,
  title={Consistent regions: Guaranteed tuple processing in ibm streams},
  author={Jacques-Silva, Gabriela and Zheng, Fang and Debrunner, Daniel and Wu, Kun-Lung and Dogaru, Victor and Johnson, Eric and Spicer, Michael and Sariy{\"u}ce, Ahmet Erdem},
  journal={Proceedings of the VLDB Endowment},
  volume={9},
  number={13},
  pages={1341--1352},
  year={2016},
  publisher={VLDB Endowment}
}
Guaranteed tuple processing has become critically important for many streaming applications. This paper describes how we enabled IBM Streams, an enterprise-grade stream processing system, to provide data processing guarantees. Our solution goes from language-level abstractions to a runtime protocol. As a result, with a couple of simple annotations at the source code level, IBM Streams developers can define consistent regions, allowing any subgraph of their streaming application to achieve guaranteed tuple processing. At runtime, a consistent region periodically executes a variation of the Chandy-Lamport snapshot algorithm to establish a consistent global state for that region. The coupling of consistent states with data replay enables guaranteed tuple processing.

@inproceedings{borthakur2011apache,
  title={Apache Hadoop goes realtime at Facebook},
  author={Borthakur, Dhruba and Gray, Jonathan and Sarma, Joydeep Sen and Muthukkaruppan, Kannan and Spiegelberg, Nicolas and Kuang, Hairong and Ranganathan, Karthik and Molkov, Dmytro and Menon, Aravind and Rash, Samuel and others},
  booktitle={Proc. of the 2011 ACM SIGMOD Intnl. Conf. on Management of data},
  pages={1071--1080},
  year={2011},
  organization={ACM}
}
Facebook recently deployed Facebook Messages, its first ever user-facing application built on the Apache Hadoop platform. Apache HBase is a database-like layer built on Hadoop designed to support billions of messages per day. This paper describes the reasons why Facebook chose Hadoop and HBase over other systems such as Apache Cassandra and Voldemort and discusses the application's requirements for consistency, availability, partition tolerance, data model and scalability. We explore the enhancements made to Hadoop to make it a more effective realtime system, the tradeoffs we made while configuring the system, and how this solution has significant advantages over the sharded MySQL database scheme used in other applications at Facebook and many other web-scale companies. We discuss the motivations behind our design choices, the challenges that we face in day-to-day operations, and future capabilities and improvements still under development. We offer these observations on the deployment as a model for other companies who are contemplating a Hadoop-based solution over traditional sharded RDBMS deployments.

@inproceedings{we2018seim,
title = {An optimistic approach to handle out-of-order events within analytical stream processing},
author = {Igor Kuralenok and Nikita Marshalkin and Artem Trofimov and Boris Novikov},
pages = {22--29},
url = {http://ceur-ws.org/Vol-2135/#SEIM_2018_paper_16},
booktitle = {Third Conference on Software Engineering and Information Management (SEIM-2018) (full papers)},
year = 2018,
editor = {Yurii Litvinov and Marat Akhin and Boris Novikov and Vladimir Itsykson},
number = 2135,
series = {CEUR Workshop Proceedings},
address = {Aachen},
issn = {1613-0073},
venue = {Saint Petersburg, Russia},
eventdate = {2018-04-14},
}


@online{rocksdb,
  month = mar,
  year = {2018},
  title = {RocksDB},
  key = {RocksDB},
  url = {http://rocksdb.org/}
}

@article{Zou:2010:SRQ:1920841.1921012,
 author = {Zou, Qiong and Wang, Huayong and Soul{\'e}, Robert and Hirzel, Martin and Andrade, Henrique and Gedik, Bu\v{g}ra and Wu, Kun-Lung},
 title = {From a Stream of Relational Queries to Distributed Stream Processing},
 journal = {Proc. VLDB Endow.},
 issue_date = {September 2010},
 volume = {3},
 number = {1-2},
 month = sep,
 year = {2010},
 issn = {2150-8097},
 pages = {1394--1405},
 numpages = {12},
 publisher = {VLDB Endowment},
 keywords = {ODBC, continuous processing, streaming processing},
} 
 url = {http://proxy.library.spbu.ru:2083/10.14778/1920841.1921012},
 doi = {10.14778/1920841.1921012},
 acmid = {1921012},


@article{Garcia-Molina:1983:USK:319983.319985,
 author = {Garcia-Molina, Hector},
 title = {Using Semantic Knowledge for Transaction Processing in a Distributed Database},
 journal = {ACM Trans. Database Syst.},
 issue_date = {June 1983},
 volume = {8},
 number = {2},
 month = jun,
 year = {1983},
 issn = {0362-5915},
 pages = {186--213},
 numpages = {28},
 url = {http://doi.acm.org/10.1145/319983.319985},
 doi = {10.1145/319983.319985},
 acmid = {319985},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrency control, consistency, locking, schedule, semantic knowledge, serializability},
} 
This paper investigates how the semantic knowledge of an application can be used in a distributed database to process transactions efficiently and to avoid some of the delays associated with failures. The main idea is to allow nonserializable schedules which preserve consistency and which are acceptable to the system users. To produce such schedules, the transaction processing mechanism receives semantic information from the users in the form of transaction semantic types, a division of transactions into steps, compatibility sets, and countersteps. Using these notions, we propose a mechanism which allows users to exploit their semantic knowledge in an organized fashion. The strengths and weaknesses of this approach are discussed.

@inproceedings{Rodriguez:2008:ITA:1463434.1463480,
 author = {Rodr\'{\i}guez, M. Andrea and Bertossi, Leopoldo and Caniup\'{a}n, Monica},
 title = {An Inconsistency Tolerant Approach to Querying Spatial Databases},
 booktitle = {Proceedings of the 16th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
 series = {GIS '08},
 year = {2008},
 isbn = {978-1-60558-323-5},
 location = {Irvine, California},
 pages = {36:1--36:10},
 articleno = {36},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1463434.1463480},
 doi = {10.1145/1463434.1463480},
 acmid = {1463480},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {consistency, inconsistency tolerance, repair semantics},
} 
In order to deal with inconsistent databases, a repair semantics defines a set of admissible database instances that restore consistency, while staying close to the original instance. This set can be used to characterize consistent data and consistent query answers in inconsistent databases. In this work we present a repair semantics for spatial databases and spatial integrity constraints, i.e. constraints that combine semantic and topological aspects of spatial data. We also propose the notion of consistent answer to a spatial conjunctive query. This introduces the idea of inconsistency tolerance in the spatial domain, shifting the goal from the consistency of a spatial database to the consistency of query answers.

@inproceedings{Guo:2010:CMS:1822018.1822052,
 author = {Guo, Jingzhi and Lam, Iok Ham and Chan, Chun and Xiao, Guangyi},
 title = {Collaboratively Maintaining Semantic Consistency of Heterogeneous Concepts Towards a Common Concept Set},
 booktitle = {Proceedings of the 2Nd ACM SIGCHI Symposium on Engineering Interactive Computing Systems},
 series = {EICS '10},
 year = {2010},
 isbn = {978-1-4503-0083-4},
 location = {Berlin, Germany},
 pages = {213--218},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/1822018.1822052},
 doi = {10.1145/1822018.1822052},
 acmid = {1822052},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {collaborative editing, electronic business, semantic consistency maintenance},
} 
In e-business, creating a common concept set for business integration, interoperation and interaction has to consider the heterogeneity reality of different interpretations from multiple concept providers. Maintaining semantic consistency between multiple concept providers is a difficult problem. To solve this problem, this paper first reviewed the existing technologies of collaborative editing systems and consistency maintenance in the areas of both CSCW and e-business. Based on the discussion of existing technologies, it then proposes a novel CHCES approach, which divides a collaborative editing system into two layers in topology and introduces four strategies to edit common concepts between the two layers. A set of operations is designed, which demonstrates the solution.

@inproceedings{Mihaila:2008:AIO:1458082.1458132,
 author = {Mihaila, George A. and Stanoi, Ioana and Lang, Christian A.},
 title = {Anomaly-free Incremental Output in Stream Processing},
 booktitle = {Proceedings of the 17th ACM Conference on Information and Knowledge Management},
 series = {CIKM '08},
 year = {2008},
 isbn = {978-1-59593-991-3},
 location = {Napa Valley, California, USA},
 pages = {359--368},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1458082.1458132},
 doi = {10.1145/1458082.1458132},
 acmid = {1458132},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {consistency, stream processing},
} 
Continuous queries enable alerts, predictions, and early warning in various domains such as health care, business process monitoring, financial applications, and environment protection. Currently, the consistency of the result cannot be assessed by the application, since only the query processor has enough internal information to determine when the output has reached a consistent state. To our knowledge, this is the first paper that addresses the problem of consistency under the assumptions and constraints of a continuous query model. In addition to defining an appropriate consistency notion, we propose techniques for guaranteeing consistency. We implemented the proposed techniques in our existing stream engine, and we report on the characteristics of the observed performance. As we show, these methods are practical as they impose only a small overhead on the system.

@inproceedings{Fischer:2010:SSP:1739041.1739068,
 author = {Fischer, Peter M. and Esmaili, Kyumars Sheykh and Miller, Ren{\'e}e J.},
 title = {Stream Schema: Providing and Exploiting Static Metadata for Data Stream Processing},
 booktitle = {Proceedings of the 13th International Conference on Extending Database Technology},
 series = {EDBT '10},
 year = {2010},
 isbn = {978-1-60558-945-9},
 location = {Lausanne, Switzerland},
 pages = {207--218},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1739041.1739068},
 doi = {10.1145/1739041.1739068},
 acmid = {1739068},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {semantic optimization, stream constraints, stream databases},
} 
Schemas, and more generally metadata specifying structural and semantic constraints, are invaluable in data management. They facilitate conceptual design and enable checking of data consistency. They also play an important role in permitting semantic query optimization, that is, optimization and processing strategies that are often highly effective, but only correct for data conforming to a given schema. While the use of metadata is well-established in relational and XML databases, the same is not true for data streams. The existing work mostly focuses on the specification of dynamic information. In this paper, we consider the specification of static metadata for streams in a model called Stream Schema. We show how Stream Schema can be used to validate the consistency of streams. By explicitly modeling stream constraints, we show that stream queries can be simplified by removing predicates or subqueries that check for consistency. This can greatly enhance pro-grammability of stream processing systems. We also present a set of semantic query optimization strategies that both permit compile-time checking of queries (for example, to detect empty queries) and new runtime processing options, options that would not have been possible without a Stream Schema specification. Case studies on two stream processing platforms (covering different applications and underlying stream models), along with an experimental evaluation, show the benefits of Stream Schema.

@inproceedings{we2018beyondmr,
 author = {Kuralenok, Igor E. and Trofimov, Artem and Marshalkin, Nikita and Novikov, Boris},
 title = {FlameStream: Model and Runtime for Distributed Stream Processing},
 booktitle = {Proceedings of the 5th ACM SIGMOD Workshop on Algorithms and Systems for MapReduce and Beyond},
 series = {BeyondMR'18},
 year = {2018},
 isbn = {978-1-4503-5703-6},
 location = {Houston, TX, USA},
 pages = {8:1--8:2},
 articleno = {8},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/3206333.3209273},
 doi = {10.1145/3206333.3209273},
 acmid = {3209273},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Data streams, drifting state, exactly-once, optimistic OOP},
} 

@inproceedings{hiddenSeim,
 title = {Omitted due to ongoing blind review; available on request}
} 

@inproceedings{hiddenBeyondMR,
 title = {Omitted due to ongoing blind review; available on request}
} 

@book{hambling2013user,
  title={User acceptance testing: A step-by-step guide},
  author={Hambling, Brian and Van Goethem, Pauline},
  year={2013},
  publisher={BCS Learning \& Development}
}

@inproceedings{Baylor:2017:TTP:3097983.3098021,
 author = {Baylor, Denis and Breck, Eric and Cheng, Heng-Tze and Fiedel, Noah and Foo, Chuan Yu and Haque, Zakaria and Haykal, Salem and Ispir, Mustafa and Jain, Vihan and Koc, Levent and Koo, Chiu Yuen and Lew, Lukasz and Mewald, Clemens and Modi, Akshay Naresh and Polyzotis, Neoklis and Ramesh, Sukriti and Roy, Sudip and Whang, Steven Euijong and Wicke, Martin and Wilkiewicz, Jarek and Zhang, Xin and Zinkevich, Martin},
 title = {TFX: A TensorFlow-Based Production-Scale Machine Learning Platform},
 booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 series = {KDD '17},
 year = {2017},
 isbn = {978-1-4503-4887-4},
 location = {Halifax, NS, Canada},
 pages = {1387--1395},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/3097983.3098021},
 doi = {10.1145/3097983.3098021},
 acmid = {3098021},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {continuous training, end-to-end platform, large-scale machine learning},
} 

@misc{JerryPengStreamIO,
  title = {Exactly once is NOT exactly the same},
  key  = {Exactly once is NOT exactly the same},
  howpublished = {\url{https://streaml.io/blog/exactly-once}},
  year = {2017},
  note = {Accessed: 2018-10-08}
}

@misc{PaperTrail,
key = {PaperTrail},
  title = {Exactly-once or not, atomic broadcast is still impossible in Kafka - or anywhere},
  howpublished = {\url{https://www.the-paper-trail.org/post/2017-07-28-exactly-not-atomic-broadcast-still-impossible-kafka/}},
  year = {2017},
  note = {Accessed: 2018-10-08}
}

@InProceedings{we2018adbis,
author="Kuralenok, Igor E.
and Trofimov, Artem
and Marshalkin, Nikita
and Novikov, Boris",
editor="Bencz{\'u}r, Andr{\'a}s
and Thalheim, Bernhard
and Horv{\'a}th, Tom{\'a}{\v{s}}",
title="Deterministic Model for Distributed Speculative Stream Processing",
booktitle="Advances in Databases and Information Systems",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="233--246",
abstract="Users of modern distributed stream processing systems have to choose between non-deterministic computations and high latency due to a need in excessive buffering. We introduce a speculative model based on MapReduce-complete set of operations that allows us to achieve determinism and low-latency. Experiments show that our prototype can outperform existing solutions due to low overhead of optimistic synchronization.",
isbn="978-3-319-98398-1"
}

@article{lin2017lambda,
  title={The lambda and the kappa},
  author={Lin, Jimmy},
  journal={IEEE Internet Computing},
  volume={21},
  number={5},
  pages={60--66},
  year={2017},
  publisher={IEEE}
}

@inproceedings{karimov2018benchmarking,
  title={Benchmarking distributed stream data processing systems},
  author={Karimov, Jeyhun and Rabl, Tilmann and Katsifodimos, Asterios and Samarev, Roman and Heiskanen, Henri and Markl, Volker},
  booktitle={2018 IEEE 34th International Conference on Data Engineering (ICDE)},
  pages={1507--1518},
  year={2018},
  organization={IEEE}
}

@article{tampakas2005,
author = {Ikonomakis, Emmanouil and Kotsiantis, Sotiris and Tampakas, V},
year = {2005},
month = {08},
pages = {966-974},
title = {Text Classification Using Machine Learning Techniques},
volume = {4},
isbn = {1109-2750},
journal = {WSEAS transactions on computers}
}

@inproceedings{sklearn_api,
  author    = {Lars Buitinck and Gilles Louppe and Mathieu Blondel and
               Fabian Pedregosa and Andreas Mueller and Olivier Grisel and
               Vlad Niculae and Peter Prettenhofer and Alexandre Gramfort
               and Jaques Grobler and Robert Layton and Jake VanderPlas and
               Arnaud Joly and Brian Holt and Ga{\"{e}}l Varoquaux},
  title     = {{API} design for machine learning software: experiences from the scikit-learn
               project},
  booktitle = {ECML PKDD Workshop: Languages for Data Mining and Machine Learning},
  year      = {2013},
  pages = {108--122},
}

@book{bird2009natural,
  title={Natural language processing with Python: analyzing text with the natural language toolkit},
  author={Bird, Steven and Klein, Ewan and Loper, Edward},
  year={2009},
  publisher={" O'Reilly Media, Inc."}
}

@inproceedings{semberecki2016distributed,
  title={Distributed classification of text documents on Apache Spark platform},
  author={Semberecki, Piotr and Maciejewski, Henryk},
  booktitle={International Conference on Artificial Intelligence and Soft Computing},
  pages={621--630},
  year={2016},
  organization={Springer}
}

@inproceedings{zhang2008one,
  title={One-class classification of text streams with concept drift},
  author={Zhang, Yang and Li, Xue and Orlowska, Maria},
  booktitle={Data Mining Workshops, 2008. ICDMW'08. IEEE International Conference on},
  pages={116--125},
  year={2008},
  organization={IEEE}
}

@incollection{alur2018interfaces,
  title={Interfaces for Stream Processing Systems},
  author={Alur, Rajeev and Mamouras, Konstantinos and Stanford, Caleb and Tannen, Val},
  booktitle={Principles of Modeling},
  pages={38--60},
  year={2018},
  publisher={Springer}
}

@inproceedings{halle2014formalization,
  title={A formalization of complex event stream processing},
  author={Hall{\'e}, Sylvain and Varvaressos, Simon},
  booktitle={2014 IEEE 18th International Enterprise Distributed Object Computing Conference},
  pages={2--11},
  year={2014},
  organization={IEEE}
}

@article{beck2018lars,
  title={LARS: A Logic-based framework for Analytic Reasoning over Streams},
  author={Beck, Harald and Dao-Tran, Minh and Eiter, Thomas},
  journal={Artificial Intelligence},
  volume={261},
  pages={16--70},
  year={2018},
  publisher={Elsevier}
}

@INPROCEEDINGS{8029336,
author={B. {Yan} and Z. {Yang} and Y. {Ren} and X. {Tan} and E. {Liu}},
booktitle={2017 IEEE International Congress on Big Data (BigData Congress)},
title={Microblog Sentiment Classification Using Parallel SVM in Apache Spark},
year={2017},
volume={},
number={},
pages={282-288},
keywords={pattern classification;radial basis function networks;sentiment analysis;social networking (online);support vector machines;microblog sentiment classification;Internet topics;parallel support vector machine;Apache Spark parallel SVM;text classification;radial basis function kernel function parameter selection;RBF kernel function;machine learning;information gain approach;IG approach;Support vector machines;Sparks;Training;Big Data;Kernel;Classification algorithms;Feature extraction;sentiment classification;support vector machine;Radial Basis Function;Spark;big data},
doi={10.1109/BigDataCongress.2017.43},
ISSN={},
month={June},}

@inproceedings{Nodarakis2016LargeSS,
  title={Large Scale Sentiment Analysis on Twitter with Spark},
  author={Nikolaos Nodarakis and Spyros Sioutas and Athanasios K. Tsakalidis and Giannis Tzimas},
  booktitle={EDBT/ICDT Workshops},
  year={2016}
}

@inproceedings{baltas2016apache,
  title={An apache spark implementation for sentiment analysis on twitter data},
  author={Baltas, Alexandros and Kanavos, Andreas and Tsakalidis, Athanasios K},
  booktitle={International Workshop of Algorithmic Aspects of Cloud Computing},
  pages={15--25},
  year={2016},
  organization={Springer}
}

@article{khumoyun2016real,
  title={Real Time information Classification in Twitter using storm},
  author={Khumoyun, Akhmedov and Cui, Yun and Lee, Hanku},
  year={2016}
}

@inproceedings{svyatkovskiy2016large,
  title={Large-scale text processing pipeline with Apache Spark},
  author={Svyatkovskiy, Alexey and Imai, Kosuke and Kroeger, Mary and Shiraito, Yuki},
  booktitle={2016 IEEE International Conference on Big Data (Big Data)},
  pages={3928--3935},
  year={2016},
  organization={IEEE}
}

@article{tucker2003exploiting,
  title={Exploiting punctuation semantics in continuous data streams},
  author={Tucker, Peter A. and Maier, David and Sheard, Tim and Fegaras, Leonidas},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={15},
  number={3},
  pages={555--568},
  year={2003},
  publisher={IEEE}
}

@article{stonebraker20058,
  title={The 8 requirements of real-time stream processing},
  author={Stonebraker, Michael and {\c{C}}etintemel, Uǧur and Zdonik, Stan},
  journal={ACM Sigmod Record},
  volume={34},
  number={4},
  pages={42--47},
  year={2005},
  publisher={ACM}
}

@inproceedings{klinkenberg2000detecting,
  title={Detecting Concept Drift with Support Vector Machines.},
  author={Klinkenberg, Ralf and Joachims, Thorsten},
  booktitle={ICML},
  pages={487--494},
  year={2000}
}

@online{lentaru,
  month = feb,
  year = {2019},
  title = {Lenta.ru dataset},
  key = {Lenta},
  url = {https://github.com/yutkin/Lenta.Ru-News-Dataset}
}

@article{meng2016mllib,
  title={Mllib: Machine learning in apache spark},
  author={Meng, Xiangrui and Bradley, Joseph and Yavuz, Burak and Sparks, Evan and Venkataraman, Shivaram and Liu, Davies and Freeman, Jeremy and Tsai, DB and Amde, Manish and Owen, Sean and others},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={1235--1241},
  year={2016},
  publisher={JMLR. org}
}

@inproceedings{berral2015aloja,
  title={Aloja-ml: A framework for automating characterization and knowledge discovery in hadoop deployments},
  author={Berral, Josep Llu{\'\i}s and Poggi, Nicolas and Carrera, David and Call, Aaron and Reinauer, Rob and Green, Daron},
  booktitle={Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages={1701--1710},
  year={2015},
  organization={ACM}
}

@article{morales2015samoa,
  title={SAMOA: scalable advanced massive online analysis.},
  author={Morales, Gianmarco De Francisci and Bifet, Albert},
  journal={Journal of Machine Learning Research},
  volume={16},
  number={1},
  pages={149--153},
  year={2015}
}

@inproceedings{Xu:2013:MVS:2488222.2488275,
 author = {Xu, Cheng and Wedlund, Daniel and Helgoson, Martin and Risch, Tore},
 title = {Model-based Validation of Streaming Data: (Industry Article)},
 booktitle = {Proceedings of the 7th ACM International Conference on Distributed Event-based Systems},
 series = {DEBS '13},
 year = {2013},
 isbn = {978-1-4503-1758-0},
 location = {Arlington, Texas, USA},
 pages = {107--114},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/2488222.2488275},
 doi = {10.1145/2488222.2488275},
 acmid = {2488275},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {anomaly detection, data stream management system, data stream validation, equipment monitoring, parallelization},
} 

@inproceedings{frank2018semantic,
  title={Semantic Data Stream Mapping and Shape Constraint Validation Based on Collaboratively Created Annotations},
  author={Frank, Matthias T and Simko, Viliam},
  booktitle={International Conference on Web Engineering},
  pages={321--329},
  year={2018},
  organization={Springer}
}

@article{tran2014change,
  title={Change detection in streaming data in the era of big data: models and issues},
  author={Tran, Dang-Hoan and Gaber, Mohamed Medhat and Sattler, Kai-Uwe},
  journal={ACM SIGKDD Explorations Newsletter},
  volume={16},
  number={1},
  pages={30--38},
  year={2014},
  publisher={ACM}
}

@inproceedings{kifer2004detecting,
  title={Detecting change in data streams},
  author={Kifer, Daniel and Ben-David, Shai and Gehrke, Johannes},
  booktitle={Proceedings of the Thirtieth international conference on Very large data bases-Volume 30},
  pages={180--191},
  year={2004},
  organization={VLDB Endowment}
}

@inproceedings{lall2015data,
  title={Data streaming algorithms for the Kolmogorov-Smirnov test},
  author={Lall, Ashwin},
  booktitle={2015 IEEE International Conference on Big Data (Big Data)},
  pages={95--104},
  year={2015},
  organization={IEEE}
}

@article{tartakovsky2008asymptotically,
  title={Asymptotically optimal quickest change detection in distributed sensor systems},
  author={Tartakovsky, Alexander G and Veeravalli, Venugopal V},
  journal={Sequential Analysis},
  volume={27},
  number={4},
  pages={441--475},
  year={2008},
  publisher={Taylor \& Francis}
}

@article{tran2014change,
  title={Change detection in streaming data in the era of big data: models and issues},
  author={Tran, Dang-Hoan and Gaber, Mohamed Medhat and Sattler, Kai-Uwe},
  journal={ACM SIGKDD Explorations Newsletter},
  volume={16},
  number={1},
  pages={30--38},
  year={2014},
  publisher={ACM}
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@inproceedings{mcmahan2013ad,
  title={Ad click prediction: a view from the trenches},
  author={McMahan, H Brendan and Holt, Gary and Sculley, David and Young, Michael and Ebner, Dietmar and Grady, Julian and Nie, Lan and Phillips, Todd and Davydov, Eugene and Golovin, Daniel and others},
  booktitle={Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={1222--1230},
  year={2013},
  organization={ACM}
}

@article{qiu2016survey,
  title={A survey of machine learning for big data processing},
  author={Qiu, Junfei and Wu, Qihui and Ding, Guoru and Xu, Yuhua and Feng, Shuo},
  journal={EURASIP Journal on Advances in Signal Processing},
  volume={2016},
  number={1},
  pages={67},
  year={2016},
  publisher={Nature Publishing Group}
}

@inproceedings{Kuralenok:2018:CEV:3269206.3271789,
 author = {Kuralenok, Igor and Starikova, Natalia and Khvorov, Aleksandr and Serdyuk, Julian},
 title = {Construction of Efficient V-Gram Dictionary for Sequential Data Analysis},
 booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
 series = {CIKM '18},
 year = {2018},
 isbn = {978-1-4503-6014-2},
 location = {Torino, Italy},
 pages = {1343--1352},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/3269206.3271789},
 doi = {10.1145/3269206.3271789},
 acmid = {3271789},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {character-based n-gram, dictionary coder, minimum description length principle, text classification, text compression, unsupervised feature extraction, v-gram},
} 

@book{akidau2018streaming,
  title={Streaming Systems: The What, Where, When, and how of Large-scale Data Processing},
  author={Akidau, T. and Chernyak, S. and Lax, R.},
  isbn={9781491983874},
  lccn={2018277258},
  url={https://books.google.ru/books?id=48-BAQAACAAJ},
  year={2018},
  publisher={O'Reilly Media, Incorporated}
}

@inproceedings{hayashibara2002failure,
  title={Failure detectors for large-scale distributed systems},
  author={Hayashibara, Naohiro and Cherif, Adel and Katayama, Takuya},
  booktitle={21st IEEE Symposium on Reliable Distributed Systems, 2002. Proceedings.},
  pages={404--409},
  year={2002},
  organization={IEEE}
}



@book{DBLP:books/mk/WeikumV2002,
  author    = {Gerhard Weikum and
               Gottfried Vossen},
  title     = {Transactional Information Systems: Theory, Algorithms, and the Practice   of Concurrency Control and Recovery},
  publisher = {Morgan Kaufmann},
  year      = {2002},
  isbn      = {1-55860-508-8},
  timestamp = {Mon, 28 Aug 2006 07:47:06 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/books/mk/WeikumV2002},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@inproceedings{Begoli:2019:OSR:3299869.3314040,
 author = {Begoli, Edmon and Akidau, Tyler and Hueske, Fabian and Hyde, Julian and Knight, Kathryn and Knowles, Kenneth},
 title = {One SQL to Rule Them All - an Efficient and Syntactically Idiomatic Approach to Management of Streams and Tables},
 booktitle = {Proceedings of the 2019 International Conference on Management of Data},
 series = {SIGMOD '19},
 year = {2019},
 isbn = {978-1-4503-5643-5},
 location = {Amsterdam, Netherlands},
 pages = {1757--1772},
 numpages = {16},
 url = {http://doi.acm.org/10.1145/3299869.3314040},
 doi = {10.1145/3299869.3314040},
 acmid = {3314040},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data management, query processing, stream processing},
} 
Real-time data analysis and management are increasingly critical for today's businesses. SQL is the de facto lingua franca for these endeavors, yet support for robust streaming analysis and management with SQL remains limited. Many approaches restrict semantics to a reduced subset of features and/or require a suite of non-standard constructs. Additionally, use of event timestamps to provide native support for analyzing events according to when they actually occurred is not pervasive, and often comes with important limitations. We present a three-part proposal for integrating robust streaming into SQL, namely: (1) time-varying relations as a foundation for classical tables as well as streaming data, (2) event time semantics, (3) a limited set of optional keyword extensions to control the materialization of time-varying query results. We show how with these minimal additions it is possible to utilize the complete suite of standard SQL semantics to perform robust stream processing. We motivate and illustrate these concepts using examples and describe lessons learned from implementations in Apache Calcite, Apache Flink, and Apache Beam. We conclude with syntax and semantics of a concrete proposal for extensions of the SQL standard and note further areas of exploration.

@inproceedings{we2019debs,
 author = {Trofimov, Artem and Shavkunov, Mikhail and Reznick, Sergey and Sokolov, Nikita and Yutman, Mikhail and Kuralenok, Igor E. and Novikov, Boris},
 title = {Reproducible and Reliable Distributed Classification of Text Streams},
 booktitle = {Proceedings of the 13th ACM International Conference on Distributed and Event-based Systems},
 series = {DEBS '19},
 year = {2019},
 isbn = {978-1-4503-6794-3},
 location = {Darmstadt, Germany},
 pages = {264--265},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/3328905.3332514},
 doi = {10.1145/3328905.3332514},
 acmid = {3332514},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Data streams, exactly once, reproducibility, text classification},
} 

@misc{thepaper,
    title={Delivery, consistency, and determinism: rethinking guarantees in distributed stream processing},
    author={Artem Trofimov and Igor E. Kuralenok and Nikita Marshalkin and Boris Novikov},
    year={2019},
    eprint={1907.06250},
    archivePrefix={arXiv},
    primaryClass={cs.DB}
}



Stephanie Wang, John Liagouris, Robert Nishihara, Philipp Moritz, Ujval Misra, Alexey Tumanov, and Ion Stoica. 2019. Lineage stash: fault tolerance off the critical path. In Proceedings of the 27th ACM Symposium on Operating Systems Principles (SOSP '19). ACM, New York, NY, USA, 338-352. DOI: https://doi.org/10.1145/3341301.3359653

@inproceedings{Wang:2019:LSF:3341301.3359653,
 author = {Wang, Stephanie and Liagouris, John and Nishihara, Robert and Moritz, Philipp and Misra, Ujval and Tumanov, Alexey and Stoica, Ion},
 title = {Lineage Stash: Fault Tolerance off the Critical Path},
 booktitle = {Proceedings of the 27th ACM Symposium on Operating Systems Principles},
 series = {SOSP '19},
 year = {2019},
 isbn = {978-1-4503-6873-5},
 location = {Huntsville, Ontario, Canada},
 pages = {338--352},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/3341301.3359653},
 doi = {10.1145/3341301.3359653},
 acmid = {3359653},
 publisher = {ACM},
 address = {New York, NY, USA},
} 
As cluster computing frameworks such as Spark, Dryad, Flink, and Ray are being deployed in mission critical applications and on larger and larger clusters, their ability to tolerate failures is growing in importance. These frameworks employ two broad approaches for fault tolerance: checkpointing and lineage. Checkpointing exhibits low overhead during normal operation but high overhead during recovery, while lineage-based solutions make the opposite tradeoff.
We propose the lineage stash, a decentralized causal logging technique that significantly reduces the runtime overhead of lineage-based approaches without impacting recovery efficiency. With the lineage stash, instead of recording the task's information before the task is executed, we record it asynchronously and forward the lineage along with the task. This makes it possible to support large-scale, low-latency (millisecond-level) data processing applications with low runtime and recovery overheads. Experimental results for applications in distributed training and stream processing show that the lineage stash provides task execution latencies similar to checkpointing alone, while incurring a recovery overhead as low as traditional lineage-based approaches.

@inproceedings{webirte,
 author = {Trofimov, Artem and Sokolov, Nikita and Shavkunov, Mikhail and Kuralenok, Igor and Novikov, Boris},
 title = {Distributed Classification of Text Streams: Limitations, Challenges, and Solutions},
 booktitle = {Proceedings of Real-Time Business Intelligence and Analytics},
 series = {BIRTE 2019},
 year = {2019},
 isbn = {978-1-4503-7660-0},
 location = {Los Angeles, CA, USA},
 pages = {2:1--2:6},
 articleno = {2},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/3350489.3350491},
 doi = {10.1145/3350489.3350491},
 acmid = {3350491},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Data streams, exactly-once, reproducibility, text classification},
} 

@inproceedings{xu2016efficient,
  title={Efficient fault-tolerance for iterative graph processing on distributed dataflow systems},
  author={Xu, Chen and Holzemer, Markus and Kaul, Manohar and Markl, Volker},
  booktitle={2016 IEEE 32nd International Conference on Data Engineering (ICDE)},
  pages={613--624},
  year={2016},
  organization={IEEE}
}
Real-world graph processing applications often re- quire combining the graph data with tabular data. Moreover, graph processing usually is part of a larger analytics workflow consiting of data preparation, analysis and model building, and model application. General-purpose distributed dataflow frame- works execute all steps of such workflows holistically. This holistic view enables these systems to reason about and automatically optimize the processing. Most big graph processing algorithms are iterative and incur a long runtime, as they require multiple passes over the data until convergence. Thus, fault tolerance and quick recovery from any intermittent failure at any step of the workflow are crucial for effective and efficient analysis. In this work, we propose a novel fault-tolerance mechanism for iterative graph processing on distributed data-flow systems with the objective to reduce the checkpointing cost and failure recovery time. Rather than writing checkpoints that block downstream operators, our mechanism writes checkpoints in an unblocking manner, without breaking pipelined tasks. In contrast to the typical unblocking checkpointing approaches (i.e., managing checkpoints independently for immutable datasets), we inject the checkpoints of mutable datasets into the iterative dataflow itself. Hence, our mechanism is iteration-aware by design. This simplifies the system architecture and facilitates coordinating the checkpoint creation during iterative graph processing. We achieve speedier recovery, i.e., confined recovery, by using the local log files on each node to avoid a complete re-computation from scratch. Our theoretical studies as well as our experimental analysis on Flink give further insight into our fault-tolerance strategies and show that they are more efficient than blocking checkpointing and complete recovery for iterative graph processing on dataflow systems.

@inproceedings{zhang2017sub,
  title={Sub-millisecond stateful stream querying over fast-evolving linked data},
  author={Zhang, Yunhao and Chen, Rong and Chen, Haibo},
  booktitle={Proceedings of the 26th Symposium on Operating Systems Principles},
  pages={614--630},
  year={2017},
  organization={ACM}
}
Applications like social networking, urban monitoring and market feed processing require stateful stream query: a query consults not only streaming data but also stored data to extract timely information; useful information from streaming data also needs to be continuously and consistently integrated into stored data to serve inflight and future queries. However, prior streaming systems either focus on stream computation, or are not stateful, or cannot provide low latency and high throughput to handle the fast-evolving linked data and increasing concurrency of queries.
This paper presents Wukong+S, a distributed stream querying engine that provides sub-millisecond stateful query at millions of queries per-second over fast-evolving linked data. Wukong+S uses an integrated design that combines the stream processor and the persistent store with efficient state sharing, which avoids the cross-system cost and sub-optimal query plan in conventional composite designs (e.g., Storm/Heron+Wukong). Wukong+S uses a hybrid store to differentially manage timeless data and timing data accordingly and provides an efficient stream index with locality-aware partitioning to facilitate fast access to streaming data. Wukong+S further provides decentralized vector timestamps with bounded snapshot scalarization to scale with nodes and massive queries at efficient memory usage.
We have designed Wukong+S conforming to the RDF data model and Continuous SPARQL (C-SPARQL) query interface and have implemented Wukong+S by extending a state-of-the-art static RDF store (namely Wukong). Evaluation on an 8-node RDMA-capable cluster using LSBench and CityBench shows that Wukong+S significantly outperforms existing system designs (e.g., CSPARQL-engine, Storm/Heron+Wukong, and Spark Streaming/Structured Streaming) for both latency and throughput, usually at the scale of orders of magnitude.

@inproceedings{Toshniwal:2014:STO:2588555.2595641,
 author = {Toshniwal, Ankit and Taneja, Siddarth and Shukla, Amit and Ramasamy, Karthik and Patel, Jignesh M. and Kulkarni, Sanjeev and Jackson, Jason and Gade, Krishna and Fu, Maosong and Donham, Jake and Bhagat, Nikunj and Mittal, Sailesh and Ryaboy, Dmitriy},
 title = {Storm@Twitter},
 booktitle = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
 series = {SIGMOD '14},
 year = {2014},
 isbn = {978-1-4503-2376-5},
 location = {Snowbird, Utah, USA},
 pages = {147--156},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2588555.2595641},
 doi = {10.1145/2588555.2595641},
 acmid = {2595641},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {real-time query processing, stream data management},
} 
This paper describes the use of Storm at Twitter. Storm is a real-time fault-tolerant and distributed stream data processing system. Storm is currently being used to run various critical computations in Twitter at scale, and in real-time. This paper describes the architecture of Storm and its methods for distributed scale-out and fault-tolerance. This paper also describes how queries (aka. topologies) are executed in Storm, and presents some operational stories based on running Storm at Twitter. We also present results from an empirical evaluation demonstrating the resilience of Storm in dealing with machine failures. Storm is under active development at Twitter and we also present some potential directions for future work.

@phdthesis{carbone2018scalable,
  title={Scalable and Reliable Data Stream Processing},
  author={Carbone, Paris},
  year={2018},
  school={KTH Royal Institute of Technology}
}

@article{10.14778/3055330.3055335,
author = {Zamanian, Erfan and Binnig, Carsten and Harris, Tim and Kraska, Tim},
title = {The End of a Myth: Distributed Transactions Can Scale},
year = {2017},
issue_date = {February 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {6},
issn = {2150-8097},
url = {https://doi.org/10.14778/3055330.3055335},
doi = {10.14778/3055330.3055335},
journal = {Proc. VLDB Endow.},
month = feb,
pages = {685--696},
numpages = {12}
}

@article{liao2019efficient,
  title={Efficient Time-Evolving Stream Processing at Scale},
  author={Liao, Xiaofei and Huang, Yu and Zheng, Long and Jin, Hai},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume={30},
  number={10},
  pages={2165--2178},
  year={2019},
  publisher={IEEE}
}

@inproceedings{kwak2010twitter,
  title={What is Twitter, a social network or a news media?},
  author={Kwak, Haewoon and Lee, Changhyun and Park, Hosung and Moon, Sue},
  booktitle={Proceedings of the 19th international conference on World wide web},
  pages={591--600},
  year={2010}
}

@article{clauset2004finding,
  title={Finding community structure in very large networks.},
  author={Clauset, A and Newman, ME and Moore, C},
  journal={Physical review. E, Statistical, nonlinear, and soft matter physics},
  volume={70},
  number={6 Pt 2},
  pages={066111},
  year={2004}
}

@article{duch2005community,
  title={Community detection in complex networks using extremal optimization},
  author={Duch, Jordi and Arenas, Alex},
  journal={Physical review E},
  volume={72},
  number={2},
  pages={027104},
  year={2005},
  publisher={APS}
}

@inproceedings{sud2007real,
  title={Real-time path planning for virtual agents in dynamic environments},
  author={Sud, Avneesh and Andersen, Erik and Curtis, Sean and Lin, Ming and Manocha, Dinesh},
  booktitle={2007 IEEE Virtual Reality Conference},
  pages={91--98},
  year={2007},
  organization={IEEE}
}

@inproceedings{wang2011understanding,
  title={Understanding graph sampling algorithms for social network analysis},
  author={Wang, Tianyi and Chen, Yang and Zhang, Zengbin and Xu, Tianyin and Jin, Long and Hui, Pan and Deng, Beixing and Li, Xing},
  booktitle={2011 31st international conference on distributed computing systems workshops},
  pages={123--128},
  year={2011},
  organization={IEEE}
}

@techreport{tucker2008nexmark,
  title={NEXMark--A Benchmark for Queries over Data Streams (DRAFT)},
  author={Tucker, Pete and Tufte, Kristin and Papadimos, Vassilis and Maier, David},
  year={2008},
  institution={Technical report, OGI School of Science \& Engineering at OHSU, Septembers}
}

@inproceedings{schmidt2002xmark,
  title={XMark: A benchmark for XML data management},
  author={Schmidt, Albrecht and Waas, Florian and Kersten, Martin and Carey, Michael J and Manolescu, Ioana and Busse, Ralph},
  booktitle={VLDB'02: Proceedings of the 28th International Conference on Very Large Databases},
  pages={974--985},
  year={2002},
  organization={Elsevier}
}

@inproceedings{grulich2020grizzly,
author = {Grulich, Philipp M. and Sebastian, Bre\ss{} and Zeuch, Steffen and Traub, Jonas and Bleichert, Janis von and Chen, Zongxiong and Rabl, Tilmann and Markl, Volker},
title = {Grizzly: Efficient Stream Processing Through Adaptive Query Compilation},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Stream Processing Engines (SPEs) execute long-running queries on unbounded data streams. They follow an interpretation-based processing model and do not perform runtime optimizations. This limits the utilization of modern hardware and neglects changing data characteristics at runtime. In this paper, we present Grizzly, a novel adaptive query compilation-based SPE, to enable highly efficient query execution. We extend query compilation and task-based parallelization for the unique requirements of stream processing and apply adaptive compilation to enable runtime re-optimizations. The combination of light-weight statistic gathering with just-in-time compilation enables Grizzly to adjust to changing data-characteristics dynamically at runtime. Our experiments show that Grizzly outperforms state-of-the-art SPEs by up to an order of magnitude in throughput.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2487--2503},
numpages = {17},
keywords = {adaptive query processing, stream processing, mutli-core, code generation, query compilation, hardware},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@Inbook{Pitoura2018processing,
author="Pitoura, Evaggelia",
editor="Liu, Ling
and {\"O}zsu, M. Tamer",
title="Query Processing",
bookTitle="Encyclopedia of Database Systems",
year="2018",
publisher="Springer New York",
address="New York, NY",
pages="3026--3027",
isbn="978-1-4614-8265-9",
doi="10.1007/978-1-4614-8265-9{\_}860",
url="https://doi.org/10.1007/978-1-4614-8265-9{\_}860"
}

@Inbook{Pitoura2018rewriting,
author="Pitoura, Evaggelia",
editor="Liu, Ling
and {\"O}zsu, M. Tamer",
title="Query Rewriting",
bookTitle="Encyclopedia of Database Systems",
year="2018",
publisher="Springer New York",
address="New York, NY",
pages="3060--3060",
isbn="978-1-4614-8265-9",
doi="10.1007/978-1-4614-8265-9{\_}863",
url="https://doi.org/10.1007/978-1-4614-8265-9{\_}863"
}

@Inbook{Neumann2018optimization,
author="Neumann, Thomas",
editor="Liu, Ling
and {\"O}zsu, M. Tamer",
title="Query Optimization (in Relational Databases)",
bookTitle="Encyclopedia of Database Systems",
year="2018",
publisher="Springer New York",
address="New York, NY",
pages="3009--3015",
isbn="978-1-4614-8265-9",
doi="10.1007/978-1-4614-8265-9{\_}293",
url="https://doi.org/10.1007/978-1-4614-8265-9{\_}293"
}

@article{astrahan1976system,
  title={System R: Relational approach to database management},
  author={Astrahan, Morton M. and Blasgen, Mike W. and Chamberlin, Donald D. and Eswaran, Kapali P. and Gray, Jim N and Griffiths, Patricia P. and King, W Frank and Lorie, Raymond A. and McJones, Paul R. and Mehl, James W. and others},
  journal={ACM Transactions on Database Systems (TODS)},
  volume={1},
  number={2},
  pages={97--137},
  year={1976},
  publisher={ACM New York, NY, USA}
}

@inproceedings{haas1989extensible,
  title={Extensible query processing in Starburst},
  author={Haas, Laura M and Freytag, Johann Christoph and Lohman, Guy M and Pirahesh, Hamid},
  booktitle={Proceedings of the 1989 ACM SIGMOD international conference on Management of data},
  pages={377--388},
  year={1989}
}

@inproceedings{graefe1993volcano,
  title={The volcano optimizer generator: Extensibility and efficient search},
  author={Graefe, Goetz and McKenna, William J},
  booktitle={Proceedings of IEEE 9th International Conference on Data Engineering},
  pages={209--218},
  year={1993},
  organization={IEEE}
}

@inproceedings{gedik2009code,
  title={A code generation approach to optimizing high-performance distributed data stream processing},
  author={Gedik, Bu{\u{g}}ra and Andrade, Henrique and Wu, Kun-Lung},
  booktitle={Proceedings of the 18th ACM conference on Information and knowledge management},
  pages={847--856},
  year={2009}
}

@article{deshpande2007adaptive,
author = {Deshpande, Amol and Ives, Zachary and Raman, Vijayshankar},
title = {Adaptive Query Processing},
year = {2007},
issue_date = {January 2007},
publisher = {Now Publishers Inc.},
address = {Hanover, MA, USA},
volume = {1},
number = {1},
issn = {1931-7883},
abstract = {As the data management field has diversified to consider settings in which queries are increasingly complex, statistics are less available, or data is stored remotely, there has been an acknowledgment that the traditional optimize-then-execute paradigm is insufficient. This has led to a plethora of new techniques, generally placed under the common banner of adaptive query processing, that focus on using runtime feed-back to modify query processing in a way that provides better response time or more efficient CPU utilization.In this survey paper, we identify many of the common issues, themes, and approaches that pervade this work, and the settings in which each piece of work is most appropriate. Our goal with this paper is to be a "value-add" over the existing papers on the material, providing not only a brief overview of each technique, but also a basic framework for understanding the field of adaptive query processing in general. We focus primarily on intra-query adaptivity of long-running, but not full-fledged streaming, queries. We conclude with a discussion of open research problems that are of high importance.},
journal = {Found. Trends Databases},
month = jan,
pages = {1--140},
numpages = {140}
}

@inproceedings{zhu2004dynamic,
author = {Zhu, Yali and Rundensteiner, Elke A. and Heineman, George T.},
title = {Dynamic Plan Migration for Continuous Queries over Data Streams},
year = {2004},
isbn = {1581138598},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Dynamic plan migration is concerned with the on-the-fly transition from one continuous query plan to a semantically equivalent yet more efficient plan. Migration is important for stream monitoring systems where long-running queries may have to withstand fluctuations in stream workloads and data characteristics. Existing migration methods generally adopt a pause-drain-resume strategy that pauses the processing of new data, purges all old data in the existing plan, until finally the new plan can be plugged into the system. However, these existing strategies do not address the problem of migrating query plans that contain stateful operators, such as joins. We now develop solutions for online plan migration for continuous stateful plans. In particular, in this paper, we propose two alternative strategies, called the moving state strategy and the parallel track strategy, one exploiting reusability and the second employs parallelism to seamlessly migrate between continuous join plans without affecting the results of the query. We develop cost models for both migration strategies to analytically compare them. We embed these migration strategies into the CAPE [7], a prototype system of a stream query engine, and conduct a comparative experimental study to evaluate these two strategies for window-based join plans. Our experimental results illustrate that the two strategies can vary significantly in terms of output rates and intermediate storage spaces given distinct system configurations and stream workloads.},
booktitle = {Proceedings of the 2004 ACM SIGMOD International Conference on Management of Data},
pages = {431--442},
numpages = {12},
location = {Paris, France},
series = {SIGMOD '04}
}

@article{kossmann2000thestate,
author = {Kossmann, Donald},
title = {The State of the Art in Distributed Query Processing},
year = {2000},
issue_date = {Dec. 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/371578.371598},
doi = {10.1145/371578.371598},
abstract = {Distributed data processing is becoming a reality. Businesses want to do it for many reasons, and they often must do it in order to stay competitive. While much of the infrastructure for distributed data processing is already there (e.g., modern network technology), a number of issues make distributed data processing still a complex undertaking: (1) distributed systems can become very large, involving thousands of heterogeneous sites including PCs and mainframe server machines; (2) the state of a distributed system changes rapidly because the load of sites varies over time and new sites are added to the system; (3) legacy systems need to be integrated—such legacy systems usually have not been designed for distributed data processing and now need to interact with other (modern) systems in a distributed environment. This paper presents the state of the art of query processing for distributed database and information systems.  The paper presents the “textbook” architecture for distributed query processing and a series of techniques that are particularly useful for distributed database systems. These techniques include special join techniques, techniques to exploit intraquery paralleli sm, techniques to reduce communication costs, and techniques to exploit caching and replication of data. Furthermore, the paper discusses different kinds of distributed systems such as client-server, middleware (multitier), and heterogeneous database systems, and shows how query processing works in these systems.},
journal = {ACM Comput. Surv.},
month = dec,
pages = {422--469},
numpages = {48},
keywords = {dissemination-based information systems, query optimization, economic models for query processing, replication, client-server databases, query execution, multitier architectures, database application systems, wrappers, middleware, caching}
}

@inproceedings{liu2015cardinality,
author = {Liu, Henry and Xu, Mingbin and Yu, Ziting and Corvinelli, Vincent and Zuzarte, Calisto},
title = {Cardinality Estimation Using Neural Networks},
year = {2015},
publisher = {IBM Corp.},
address = {USA},
abstract = {Database query optimizers benefit greatly from accurate cardinality estimation; however, this is hard to achieve on tables with correlated and/or skewed columns. We present a novel approach using neural networks to learn and approximate selectivity functions that take a bounded range on each column as input, effectively estimating selectivities for all relational operators. Experimental results with a simplified prototype show a significant improvement over state-of-the-art cardinality estimators on constructed datasets in terms of accuracy, efficiency, and amount of user input required.},
booktitle = {Proceedings of the 25th Annual International Conference on Computer Science and Software Engineering},
pages = {53--59},
numpages = {7},
keywords = {cardinality estimation, neural network, relational database, machine learning},
location = {Markham, Canada},
series = {CASCON '15}
}

@article{kipf2018learned,
  title={Learned cardinalities: Estimating correlated joins with deep learning},
  author={Kipf, Andreas and Kipf, Thomas and Radke, Bernhard and Leis, Viktor and Boncz, Peter and Kemper, Alfons},
  journal={arXiv preprint arXiv:1809.00677},
  year={2018}
}

@article{ortiz2019empirical,
  title={An empirical analysis of deep learning for cardinality estimation},
  author={Ortiz, Jennifer and Balazinska, Magdalena and Gehrke, Johannes and Keerthi, S Sathiya},
  journal={arXiv preprint arXiv:1905.06425},
  year={2019}
}

@article{CHEN20211047,
title = {Join cardinality estimation by combining operator-level deep neural networks},
journal = {Information Sciences},
volume = {546},
pages = {1047-1062},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.09.065},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520309750},
author = {Ling Chen and Heqing Huang and Donghui Chen},
keywords = {Join cardinality estimation, Deep learning, Sampling},
abstract = {Join cardinality estimation is fundamental to cost-based query optimizers. The state-of-the-art deep learning based join cardinality estimation methods do not fully take the structure of a query plan into account and lack of data information about joins, causing significant performance degradation when the number of joins in a query increases. In this paper, we propose CAPE, a join cardinality estimation method combining operator-level deep neural networks. CAPE introduces two operator-level deep neural networks for selection operators and join operators, as well as an output deep neural network that maps the intermediate representations to join cardinality estimates. Given the query plan rooted at a join operator, CAPE generates a plan-level deep neural network by combining operator-level deep neural networks. In this way, CAPE can handle arbitrary query plans with simple operator-level deep neural networks rather than a single complicated model. In addition, we introduce join-crossing sampling information to detect join-crossing correlations. Experiments are conducted on a dataset constructed from the IMDb dataset, and the experimental results show that CAPE is significantly better than the state-of-the-art methods.}
}

@article{graefe1995cascades,
  title={The cascades framework for query optimization},
  author={Graefe, Goetz},
  journal={IEEE Data Eng. Bull.},
  volume={18},
  number={3},
  pages={19--29},
  year={1995}
}

@incollection{selinger1989access,
  title={Access path selection in a relational database management system},
  author={Selinger, P Griffiths and Astrahan, Morton M and Chamberlin, Donald D and Lorie, Raymond A and Price, Thomas G},
  booktitle={Readings in Artificial Intelligence and Databases},
  pages={511--522},
  year={1989},
  publisher={Elsevier}
}

@article{poosala1996improved,
  title={Improved histograms for selectivity estimation of range predicates},
  author={Poosala, Viswanath and Haas, Peter J and Ioannidis, Yannis E and Shekita, Eugene J},
  journal={ACM Sigmod Record},
  volume={25},
  number={2},
  pages={294--305},
  year={1996},
  publisher={ACM New York, NY, USA}
}

@article{gunopulos2005selectivity,
  title={Selectivity estimators for multidimensional range queries over real attributes},
  author={Gunopulos, Dimitrios and Kollios, George and Tsotras, Vassilis J and Domeniconi, Carlotta},
  journal={the VLDB Journal},
  volume={14},
  number={2},
  pages={137--154},
  year={2005},
  publisher={Springer}
}

@inproceedings{muralikrishna1988equidepth,
author = {Muralikrishna, M. and DeWitt, David J.},
title = {Equi-Depth Multidimensional Histograms},
year = {1988},
isbn = {0897912683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/50202.50205},
doi = {10.1145/50202.50205},
booktitle = {Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data},
pages = {28--36},
numpages = {9},
location = {Chicago, Illinois, USA},
series = {SIGMOD '88}
}

@article{yang2020neurocard,
  title={NeuroCard: one cardinality estimator for all tables},
  author={Yang, Zongheng and Kamsetty, Amog and Luan, Sifei and Liang, Eric and Duan, Yan and Chen, Xi and Stoica, Ion},
  journal={arXiv preprint arXiv:2006.08109},
  year={2020}
}

@article{hirzel2014catalog,
author = {Hirzel, Martin and Soul\'{e}, Robert and Schneider, Scott and Gedik, Bu\u{g}ra and Grimm, Robert},
title = {A Catalog of Stream Processing Optimizations},
year = {2014},
issue_date = {April 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/2528412},
doi = {10.1145/2528412},
abstract = {Various research communities have independently arrived at stream processing as a programming model for efficient and parallel computing. These communities include digital signal processing, databases, operating systems, and complex event processing. Since each community faces applications with challenging performance requirements, each of them has developed some of the same optimizations, but often with conflicting terminology and unstated assumptions. This article presents a survey of optimizations for stream processing. It is aimed both at users who need to understand and guide the system’s optimizer and at implementers who need to make engineering tradeoffs. To consolidate terminology, this article is organized as a catalog, in a style similar to catalogs of design patterns or refactorings. To make assumptions explicit and help understand tradeoffs, each optimization is presented with its safety constraints (when does it preserve correctness?) and a profitability experiment (when does it improve performance?). We hope that this survey will help future streaming system builders to stand on the shoulders of giants from not just their own community.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {46},
numpages = {34},
keywords = {optimizations, Stream processing}
}

@inproceedings{babu2004adaptive,
  title={Adaptive ordering of pipelined stream filters},
  author={Babu, Shivnath and Motwani, Rajeev and Munagala, Kamesh and Nishizawa, Itaru and Widom, Jennifer},
  booktitle={Proceedings of the 2004 ACM SIGMOD international conference on Management of data},
  pages={407--418},
  year={2004}
}

@inproceedings{selinger1979access,
author = {Selinger, P. Griffiths and Astrahan, M. M. and Chamberlin, D. D. and Lorie, R. A. and Price, T. G.},
title = {Access Path Selection in a Relational Database Management System},
year = {1979},
isbn = {089791001X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/582095.582099},
doi = {10.1145/582095.582099},
abstract = {In a high level query and data manipulation language such as SQL, requests are stated non-procedurally, without reference to access paths. This paper describes how System R chooses access paths for both simple (single relation) and complex queries (such as joins), given a user specification of desired data as a boolean expression of predicates. System R is an experimental database management system developed to carry out research on the relational model of data. System R was designed and built by members of the IBM San Jose Research Laboratory.},
booktitle = {Proceedings of the 1979 ACM SIGMOD International Conference on Management of Data},
pages = {23--34},
numpages = {12},
location = {Boston, Massachusetts},
series = {SIGMOD '79}
}



@article{krishnan2018learning,
  author    = {Sanjay Krishnan and
               Zongheng Yang and
               Ken Goldberg and
               Joseph M. Hellerstein and
               Ion Stoica},
  title     = {Learning to Optimize Join Queries With Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1808.03196},
  year      = {2018},
  url       = {http://arxiv.org/abs/1808.03196},
  archivePrefix = {arXiv},
  eprint    = {1808.03196},
  timestamp = {Sun, 02 Sep 2018 15:01:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1808-03196.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{marcus2019neo,
author = {Marcus, Ryan and Negi, Parimarjan and Mao, Hongzi and Zhang, Chi and Alizadeh, Mohammad and Kraska, Tim and Papaemmanouil, Olga and Tatbul, Nesime},
title = {Neo: A Learned Query Optimizer},
year = {2019},
issue_date = {July 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3342263.3342644},
doi = {10.14778/3342263.3342644},
abstract = {Query optimization is one of the most challenging problems in database systems. Despite the progress made over the past decades, query optimizers remain extremely complex components that require a great deal of hand-tuning for specific workloads and datasets. Motivated by this shortcoming and inspired by recent advances in applying machine learning to data management challenges, we introduce Neo (Neural Optimizer), a novel learning-based query optimizer that relies on deep neural networks to generate query executions plans. Neo bootstraps its query optimization model from existing optimizers and continues to learn from incoming queries, building upon its successes and learning from its failures. Furthermore, Neo naturally adapts to underlying data patterns and is robust to estimation errors. Experimental results demonstrate that Neo, even when bootstrapped from a simple optimizer like PostgreSQL, can learn a model that offers similar performance to state-of-the-art commercial optimizers, and in some cases even surpass them.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {1705--1718},
numpages = {14}
}
@inproceedings{street2001ensemble,
  author    = {W. Nick Street and
               YongSeog Kim},
  editor    = {Doheon Lee and
               Mario Schkolnick and
               Foster J. Provost and
               Ramakrishnan Srikant},
  title     = {A streaming ensemble algorithm {(SEA)} for large-scale classification},
  booktitle = {Proceedings of the seventh {ACM} {SIGKDD} international conference
               on Knowledge discovery and data mining, San Francisco, CA, USA, August
               26-29, 2001},
  pages     = {377--382},
  publisher = {{ACM}},
  year      = {2001},
  url       = {http://portal.acm.org/citation.cfm?id=502512.502568},
  timestamp = {Wed, 12 Dec 2012 15:08:19 +0100},
  biburl    = {https://dblp.org/rec/conf/kdd/StreetK01.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{hilprecht2020deepdb,
author = {Hilprecht, Benjamin and Schmidt, Andreas and Kulessa, Moritz and Molina, Alejandro and Kersting, Kristian and Binnig, Carsten},
title = {DeepDB: Learn from Data, Not from Queries!},
year = {2020},
issue_date = {March 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {7},
issn = {2150-8097},
url = {https://doi.org/10.14778/3384345.3384349},
doi = {10.14778/3384345.3384349},
abstract = {The typical approach for learned DBMS components is to capture the behavior by running a representative set of queries and use the observations to train a machine learning model. This workload-driven approach, however, has two major downsides. First, collecting the training data can be very expensive, since all queries need to be executed on potentially large databases. Second, training data has to be recollected when the workload or the database changes. To overcome these limitations, we take a different route and propose a new data-driven approach for learned DBMS components which directly supports changes of the workload and data without the need of retraining. Indeed, one may now expect that this comes at a price of lower accuracy since workload-driven approaches can make use of more information. However, this is not the case. The results of our empirical evaluation demonstrate that our data-driven approach not only provides better accuracy than state-ofthe- art learned components but also generalizes better to unseen queries.},
journal = {Proc. VLDB Endow.},
month = mar,
pages = {992--1005},
numpages = {14}
}

@inproceedings{armbrust2015spark,
  title={Spark sql: Relational data processing in spark},
  author={Armbrust, Michael and Xin, Reynold S and Lian, Cheng and Huai, Yin and Liu, Davies and Bradley, Joseph K and Meng, Xiangrui and Kaftan, Tomer and Franklin, Michael J and Ghodsi, Ali and others},
  booktitle={Proceedings of the 2015 ACM SIGMOD international conference on management of data},
  pages={1383--1394},
  year={2015}
}

@article{10.14778/3329772.3329777, author = {Hoffmann, Moritz and Lattuada, Andrea and McSherry, Frank}, title = {Megaphone: Latency-Conscious State Migration for Distributed Streaming Dataflows}, year = {2019}, issue_date = {May 2019}, publisher = {VLDB Endowment}, volume = {12}, number = {9}, issn = {2150-8097}, url = {https://doi.org/10.14778/3329772.3329777}, doi = {10.14778/3329772.3329777}, abstract = {We design and implement Megaphone, a data migration mechanism for stateful distributed dataflow engines with latency objectives. When compared to existing migration mechanisms, Megaphone has the following differentiating characteristics: (i) migrations can be subdivided to a configurable granularity to avoid latency spikes, and (ii) migrations can be prepared ahead of time to avoid runtime coordination. Megaphone is implemented as a library on an unmodified timely dataflow implementation, and provides an operator interface compatible with its existing APIs. We evaluate Megaphone on established benchmarks with varying amounts of state and observe that compared to na\"{\i}ve approaches Megaphone reduces service latencies during reconfiguration by orders of magnitude without significantly increasing steady-state overhead.}, journal = {Proc. VLDB Endow.}, month = may, pages = {1002--1015}, numpages = {14} }

@book{meyer2002design,
  title={Design by contract},
  author={Meyer, Bertrand},
  year={2002},
  publisher={Prentice Hall Upper Saddle River}
}

@inproceedings{schule2019mlearn,
  title={Mlearn: A declarative machine learning language for database systems},
  author={Sch{\"u}le, Maximilian E and Bungeroth, Matthias and Kemper, Alfons and G{\"u}nnemann, Stephan and Neumann, Thomas},
  booktitle={Proceedings of the 3rd International Workshop on Data Management for End-to-End Machine Learning},
  pages={1--4},
  year={2019}
}

@article{yarygina2014optimizing,
  title={Optimizing resource allocation for approximate real-time query processing},
  author={Yarygina, Anna and Novikov, Boris},
  journal={Computer Science and Information Systems},
  volume={11},
  number={1},
  pages={69--88},
  year={2014}
}

@inproceedings{kroll2019arc,
  title={Arc: an ir for batch and stream programming},
  author={Kroll, Lars and Segeljakt, Klas and Carbone, Paris and Schulte, Christian and Haridi, Seif},
  booktitle={Proceedings of the 17th ACM SIGPLAN International Symposium on Database Programming Languages},
  pages={53--58},
  year={2019}
}

@article{10.1145/335191.335420, author = {Avnur, Ron and Hellerstein, Joseph M.}, title = {Eddies: Continuously Adaptive Query Processing}, year = {2000}, issue_date = {June 2000}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {29}, number = {2}, issn = {0163-5808}, url = {https://doi.org/10.1145/335191.335420}, doi = {10.1145/335191.335420}, abstract = {In large federated and shared-nothing databases, resources can exhibit widely fluctuating characteristics. Assumptions made at the time a query is submitted will rarely hold throughout the duration of query processing. As a result, traditional static query optimization and execution techniques are ineffective in these environments.In this paper we introduce a query processing mechanism called an eddy, which continuously reorders operators in a query plan as it runs. We characterize the moments of symmetry during which pipelined joins can be easily reordered, and the synchronization barriers that require inputs from different sources to be coordinated. By combining eddies with appropriate join algorithms, we merge the optimization and execution phases of query processing, allowing each tuple to have a flexible ordering of the query operators. This flexibility is controlled by a combination of fluid dynamics and a simple learning algorithm. Our initial implementation demonstrates promising results, with eddies performing nearly as well as a static optimizer/executor in static scenarios, and providing dramatic improvements in dynamic execution environments.}, journal = {SIGMOD Rec.}, month = may, pages = {261–272}, numpages = {12} }

@inproceedings{10.1145/1007568.1007702, author = {Babu, Shivnath and Widom, Jennifer}, title = {StreaMon: An Adaptive Engine for Stream Query Processing}, year = {2004}, isbn = {1581138598}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1007568.1007702}, doi = {10.1145/1007568.1007702}, abstract = {StreaMon is the adaptive query processing engine of the STREAM prototype Data Stream Management System (DSMS) [4]. A fundamental challenge in many DSMS applications (e.g., network monitoring, financial monitoring over stock tickers, sensor processing) is that conditions may vary significantly over time. Since queries in these systems are usually long-running, or continuous [4], it is important to consider adaptive approaches to query processing. Without adaptivity, performance may drop drastically as stream data and arrival characteristics, query loads, and system conditions change over time.StreaMon uses several techniques to support adaptive query processing [1, 2, 3]; we demonstrate three of them:•Reducing run-time memory requirements for continuous queries by exploiting stream data and arrival patterns.•Adaptive join ordering for pipelined multiway stream joins, with strong quality guarantees.•Placing subresult caches adaptively in pipelined multiway stream joins to avoid recomputation of intermediate results.}, booktitle = {Proceedings of the 2004 ACM SIGMOD International Conference on Management of Data}, pages = {931–932}, numpages = {2}, location = {Paris, France}, series = {SIGMOD '04} }

@inproceedings{schneider2012auto,
  title={Auto-parallelizing stateful distributed streaming applications},
  author={Schneider, Scott and Hirzel, Martin and Gedik, Bugra and Wu, Kun-Lung},
  booktitle={Proceedings of the 21st international conference on Parallel architectures and compilation techniques},
  pages={53--64},
  year={2012}
}

@inproceedings{gedik2008spade,
  title={SPADE: The System S declarative stream processing engine},
  author={Gedik, Bugra and Andrade, Henrique and Wu, Kun-Lung and Yu, Philip S and Doo, Myungcheol},
  booktitle={Proceedings of the 2008 ACM SIGMOD international conference on Management of data},
  pages={1123--1134},
  year={2008}
}

@inproceedings{chang2014hawq,
  title={HAWQ: a massively parallel processing SQL engine in hadoop},
  author={Chang, Lei and Wang, Zhanwei and Ma, Tao and Jian, Lirong and Ma, Lili and Goldshuv, Alon and Lonergan, Luke and Cohen, Jeffrey and Welton, Caleb and Sherry, Gavin and others},
  booktitle={Proceedings of the 2014 ACM SIGMOD international conference on Management of data},
  pages={1223--1234},
  year={2014}
}

@inproceedings{sethi2019presto,
  title={Presto: Sql on everything},
  author={Sethi, Raghav and Traverso, Martin and Sundstrom, Dain and Phillips, David and Xie, Wenlei and Sun, Yutian and Yegitbasi, Nezih and Jin, Haozhun and Hwang, Eric and Shingte, Nileema and others},
  booktitle={2019 IEEE 35th International Conference on Data Engineering (ICDE)},
  pages={1802--1813},
  year={2019},
  organization={IEEE}
 }

@online{apachebeam,
  month = may,
  year = {2021},
  title={Apache Beam},
  key={Apache Beam},
  url = {http://beam.apache.org/}
}

@inproceedings{re2006complete,
  title={A complete and efficient algebraic compiler for XQuery},
  author={R{\'e}, Christopher and Sim{\'e}on, J{\'e}r{\^o}me and Fernandez, Mary},
  booktitle={22nd International Conference on Data Engineering (ICDE'06)},
  pages={14--14},
  year={2006},
  organization={IEEE}
}

@article{gyllstrom2006sase,
  title={SASE: Complex event processing over streams},
  author={Gyllstrom, Daniel and Wu, Eugene and Chae, Hee-Jin and Diao, Yanlei and Stahlberg, Patrick and Anderson, Gordon},
  journal={arXiv preprint cs/0612128},
  year={2006}
}

@article{hueske2012opening,
  title={Opening the black boxes in data flow optimization},
  author={Hueske, Fabian and Peters, Mathias and Sax, Matthias and Rheinl{\"a}nder, Astrid and Bergmann, Rico and Krettek, Aljoscha and Tzoumas, Kostas},
  journal={arXiv preprint arXiv:1208.0087},
  year={2012}
}

@inproceedings{schelter2016samsara,
  title={Samsara: Declarative machine learning on distributed dataflow systems},
  author={Schelter, Sebastian and Palumbo, Andrew and Quinn, Shannon and Marthi, Suneel and Musselman, Andrew},
  booktitle={NIPS Workshop MLSystems},
  year={2016}
}

@article{davidson2013optimizing,
  title={Optimizing shuffle performance in spark},
  author={Davidson, Aaron and Or, Andrew},
  journal={University of California, Berkeley-Department of Electrical Engineering and Computer Sciences, Tech. Rep},
  year={2013}
}

@inproceedings{bosagh2016matrix,
  title={Matrix computations and optimization in apache spark},
  author={Bosagh Zadeh, Reza and Meng, Xiangrui and Ulanov, Alexander and Yavuz, Burak and Pu, Li and Venkataraman, Shivaram and Sparks, Evan and Staple, Aaron and Zaharia, Matei},
  booktitle={Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages={31--38},
  year={2016}
}

@article{zhang2021research,
  title={Research on Optimal Checkpointing-Interval for Flink Stream Processing Applications},
  author={Zhang, Zhan and Li, Wenhao and Qing, Xiao and Liu, Xian and Liu, Hongwei},
  journal={Mobile Networks and Applications},
  pages={1--10},
  year={2021},
  publisher={Springer}
}

Computing aggregates over windows is at the core of virtually every stream processing job. Typical stream processing applications involve overlapping windows and, therefore, cause redundant computations. Several techniques prevent this redundancy by sharing partial aggregates among windows. However, these techniques do not support out-of-order processing and session windows. Out-of-order processing is a key requirement to deal with delayed tuples in case of source failures such as temporary sensor outages. Session windows are widely used to separate different periods of user activity from each other. In this paper, we present Scotty, a high throughput operator for window discretization and aggregation. Scotty splits streams into non-overlapping slices and computes partial aggregates per slice. These partial aggregates are shared among all concurrent queries with arbitrary combinations of tumbling, sliding, and session windows. Scotty introduces the first slicing technique which (1) enables stream slicing for session windows in addition to tumbling and sliding windows and (2) processes out-of-order tuples efficiently. Our technique is generally applicable to a broad group of dataflow systems which use a unified batch and stream processing model. Our experiments show that we achieve a throughput an order of magnitude higher than alternative state-of-the-art solutions.
@inproceedings{traub2018scotty,
  title={Scotty: Efficient window aggregation for out-of-order stream processing},
  author={Traub, Jonas and Grulich, Philipp Marian and Cuellar, Alejandro Rodriguez and Bre{\ss}, Sebastian and Katsifodimos, Asterios and Rabl, Tilmann and Markl, Volker},
  booktitle={2018 IEEE 34th International Conference on Data Engineering (ICDE)},
  pages={1300--1303},
  year={2018},
  organization={IEEE}
}

Despite the established scientific knowledge on efficient parallel and elastic data stream processing, it is challenging to combine generality and high level of abstraction (targeting ease of use) with fine-grained processing aspects (targeting efficiency) in stream processing frameworks. Towards this goal, we propose STRETCH, a framework that aims at guaranteeing (i) high efficiency in throughput and latency of stateful analysis and (ii) fast elastic reconfigurations (without requiring state transfer) for intra-node streaming applications. To achieve these, we introduce virtual shared-nothing parallelization and propose a scheme to implement it in STRETCH, enabling users to leverage parallelization techniques while also taking advantage of shared-memory synchronization, which has been proven to boost the scaling-up of streaming applications while supporting determinism. We provide a fully-implemented prototype and, together with a thorough evaluation, correctness proofs for its underlying claims supporting determinism and a model (also validated empirically) of virtual shared-nothing and pure shared-nothing scalability behavior. As we show, STRETCH can match the throughput and latency figures of the front of state-of-the-art solutions, while also achieving fast elastic reconfigurations (taking only a few milliseconds).
@inproceedings{najdataei2019stretch,
  title={Stretch: Scalable and elastic deterministic streaming analysis with virtual shared-nothing parallelism},
  author={Najdataei, Hannaneh and Nikolakopoulos, Yiannis and Papatriantafilou, Marina and Tsigas, Philippas and Gulisano, Vincenzo},
  booktitle={Proceedings of the 13th ACM International Conference on Distributed and Event-based Systems},
  pages={7--18},
  year={2019}
}

The inherently large and varying volumes of information generated in large scale systems demand near real-time processing of data streams. In this context, data streaming is imperative for data-intensive processing infrastructures. Stream joins, the streaming counterpart of database joins, compare tuples coming from different streams and constitute one of the most important and expensive data streaming operators. Algorithmic implementations of stream joins have to be capable of efficiently processing bursty and rate-varying data streams in a deterministic and skew-resilient fashion. To leverage the design of modern multicore architectures, scalability and parallelism need to be addressed also in the algorithmic design. In this paper we present ScaleJoin, an algorithmic construction for deterministic and parallel stream joins that guarantees all the above properties, thus filling in a gap in the existing state-of-theart. Key to the novelty of ScaleJoin is the ScaleGate data structure and its lock-free implementation. ScaleGate facilitates concurrent data exchange and balances independent actions among processing threads; enabling fine-grain parallelism and deterministic processing. It allows ScaleJoin to run on an arbitrary number of processing threads, evenly sharing the overall comparisons run in parallel and achieving disjoint and skew-resilient high processing throughput and low processing latency.
@article{gulisano2016scalejoin,
  title={Scalejoin: A deterministic, disjoint-parallel and skew-resilient stream join},
  author={Gulisano, Vincenzo and Nikolakopoulos, Yiannis and Papatriantafilou, Marina and Tsigas, Philippas},
  journal={IEEE Transactions on Big Data},
  year={2016},
  publisher={IEEE}
}

Event-time based stream processing is concerned with analyzing data with respect to its generation time. In most of the cases, data gets delayed during its journey from the source(s) to the stream processing engine. This is known as late data arrival. Among the different approaches for out-of-order stream processing, low watermarks are proposed to inject special records within data streams, i.e., watermarks. A watermark is a timestamp which indicates that no data with a timestamp older than the water- mark should be observed later on. Any element as such is consid- ered a late arrival. Watermark generation is usually periodic and heuristic-based. The limitation of such watermark generation strategy is its rigidness regarding the frequency of data arrival as well as the delay that data may encounter. In this paper, we propose an adaptive watermark generation strategy. Our strat- egy decides adaptively when to generate watermarks and with what timestamp without a priori adjustment. We treat changes in data arrival frequency and changes in delays as concept drifts in stream data mining. We use an Adaptive Window (ADWIN) as our concept drift sensor for the change in the distribution of arrival rate and delay. We have implemented our approach on top of Apache Flink. We compare our approach with periodic water- mark generation using two real-life data sets. Our results show that adaptive watermarks achieve a lower average latency by triggering windows earlier and a lower rate of dropped elements by delaying watermarks when out-of-order data is expected.
@inproceedings{awad2019adaptive,
  title={Adaptive Watermarks: A Concept Drift-based Approach for Predicting Event-Time Progress in Data Streams.},
  author={Awad, Ahmed and Traub, Jonas and Sakr, Sherif},
  booktitle={EDBT},
  pages={622--625},
  year={2019}
}

This paper introduces Trill – a new query processor for analytics. Trill fulfills a combination of three requirements for a query processor to serve the diverse big data analytics space: (1) Query Model: Trill is based on a tempo-relational model that enables it to handle streaming and relational queries with early results, across the latency spectrum from real-time to offline; (2) Fabric and Language Integration: Trill is architected as a high-level language library that supports rich data-types and user libraries, and integrates well with existing distribution fabrics and applications; and (3) Performance: Trill’s throughput is high across the latency spectrum. For streaming data, Trill’s throughput is 2-4 orders of magnitude higher than comparable streaming engines. For offline relational queries, Trill’s throughput is comparable to a major modern commercial columnar DBMS.
Trill uses a streaming batched-columnar data representation with a new dynamic compilation-based system architecture that addresses all these requirements. In this paper, we describe Trill’s new design and architecture, and report experimental results that demonstrate Trill’s high performance across diverse analytics scenarios. We also describe how Trill’s ability to support diverse analytics has resulted in its adoption across many usage scenarios at Microsoft.
@article{chandramouli2014trill,
  title={Trill: A high-performance incremental query processor for diverse analytics},
  author={Chandramouli, Badrish and Goldstein, Jonathan and Barnett, Mike and DeLine, Robert and Fisher, Danyel and Platt, John C and Terwilliger, James F and Wernsing, John},
  journal={Proceedings of the VLDB Endowment},
  volume={8},
  number={4},
  pages={401--412},
  year={2014},
  publisher={VLDB Endowment}
}

@article{xu2018fault,
  title={Fault-tolerance for distributed iterative dataflows in action},
  author={Xu, Chen and Lemaitre, Rudi Poepsel and Soto, Juan and Markl, Volker},
  journal={Proceedings of the VLDB Endowment},
  volume={11},
  number={12},
  pages={1990--1993},
  year={2018},
  publisher={VLDB Endowment}
}

@article{DBLP:journals/pvldb/BegoliACHKKMS21,
  author    = {Edmon Begoli and
               Tyler Akidau and
               Slava Chernyak and
               Fabian Hueske and
               Kathryn Knight and
               Kenneth Knowles and
               Daniel Mills and
               Dan Sotolongo},
  title     = {Watermarks in Stream Processing Systems: Semantics and Comparative
               Analysis of Apache Flink and Google Cloud Dataflow},
  journal   = {Proc. {VLDB} Endow.},
  volume    = {14},
  number    = {12},
  pages     = {3135--3147},
  year      = {2021},
  url       = {http://www.vldb.org/pvldb/vol14/p3135-begoli.pdf},
  timestamp = {Fri, 27 Aug 2021 17:02:27 +0200},
  biburl    = {https://dblp.org/rec/journals/pvldb/BegoliACHKKMS21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{pathak2016introduction,
  title={Introduction to real-time processing in Apache Apex},
  author={Pathak, Harsh and Rathi, Manas and Parekh, Aniket},
  journal={Int. J. Res. Advent Technol},
  pages={19},
  year={2016}
}

@inproceedings{10.1145/3524860.3539809, author = {Trofimov, Artem and Sokolov, Nikita and Marshalkin, Nikita and Kuralenok, Igor and Novikov, Boris}, title = {Substream Management in Distributed Streaming Dataflows}, year = {2022}, isbn = {9781450393089}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3524860.3539809}, doi = {10.1145/3524860.3539809}, abstract = {Most state-of-the-art SPEs use punctuations to divide a stream into bounded substreams of messages, such as epochs and windows. The punctuation approach is powerful but has limitations: it does not support cyclic dataflows, is poorly scalable in some cases due to intensive use of broadcasts, and becomes inefficient when the number of chunks or cluster size becomes significant. We introduce a new substream tracking technique called trAcker that overcomes the limits of punctuations. We experimentally evaluate the properties of trAcker in both synthetic and real-world environments. Experiments show that our technique outperforms punctuations for a large number of substreams and efficiently handles real-world cyclic dataflows.}, booktitle = {Proceedings of the 16th ACM International Conference on Distributed and Event-Based Systems}, pages = {55–66}, numpages = {12}, keywords = {data streams, state management, stream join, watermarks, punctuations, substreams}, location = {Copenhagen, Denmark}, series = {DEBS '22} }

@inproceedings{purtzel2022predicate,
  title={Predicate-based push-pull communication for distributed CEP},
  author={Purtzel, Steven and Akili, Samira and Weidlich, Matthias},
  booktitle={Proceedings of the 16th ACM International Conference on Distributed and Event-Based Systems},
  pages={31--42},
  year={2022}
}

@article{awad2022d2ia,
  title={D2IA: User-defined interval analytics on distributed streams},
  author={Awad, Ahmed and Tommasini, Riccardo and Langhi, Samuele and Kamel, Mahmoud and Della Valle, Emanuele and Sakr, Sherif},
  journal={Information Systems},
  volume={104},
  pages={101679},
  year={2022},
  publisher={Elsevier}
}

@inproceedings{hirzel2012partition,
  title={Partition and compose: Parallel complex event processing},
  author={Hirzel, Martin},
  booktitle={Proceedings of the 6th ACM International Conference on Distributed Event-Based Systems},
  pages={191--200},
  year={2012}
}