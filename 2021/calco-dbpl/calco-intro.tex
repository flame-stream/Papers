Nowadays different applications widely use frameworks for computations over unbounded and bounded data streams, such as Apache Flink, Apache Spark, etc.
All of them represent computations similarly, as a execution graph (directed acyclic graph).
Programmers are able to define graphs as is or to use SQL (that automatically translates to the execution graphs).

SQL is based on the relation algebra that has concrete number of operations and defines rules which describe permutation possibilities that can be used in optimization purposes.
However, this approach is not flexible enough in real-world dataflows specification.
It is needed to specify custom operations (maybe stateful) that accord to the problem domain.

Computations can be also specified directly by defining a concrete graph.
Nodes of the such graph are custom operations, which requirements and semantics cannot be automatically deduced (because of the halting problem) as well as theirs permutation rules.
So operations cannot be automatically permuted to achieve a better performance.
Thus, optimization of the concrete graphs is a programmer concern.
However, real-world graphs can be big enough that makes manual optimization to be a complex task.
And finally, most of the necessary information for the graph cost evaluation is available only in runtime (TODO duplicates abstract).

Many business tasks are computationally expensive, but have large common parts.
That makes programmers to write very big graphs that solve many tasks at once to maximally reuse computational results.
And it makes analytics to write complex multi-purpose SQL queries.
So cross-query optimization is needed, but it does not supported now.

Let's suppose that we have some number of binary modules, written in different programming languages.
And we want to integrate them in a complex cross-domain dataflow.
Today we have no means to optimize it.
