%  a Распределенная обработка и ее виды.
%    Есть MapReduce и батчи, есть потоки.
%  b Исторически, пайплайны распределенной обработки задавали графами исполнения (переход от Hadoop к Spark).
%    Графы исполнения популярны, т.к. их можно задать кодом и версионировать как код, у них есть переиспользуемые единицы (вершины), их можно визуализировать.
%    Однако, графы состоят из произвольных пользовательских операций, над которыми нет никакой заданной алгебры. Следовательно, графы можно оптимизировать очень ограниченно - если мы перестроим граф, то у нас нет возможности понять, остался ли он корректным.
%  c Еще один способ задания вычислений SQL - подход, основанный на реляционной алгебре, в котором через запрос задается результат, который необходимо получить. Пользуясь правилами реляционной алгебры можно получать гарантированно эквиваллентные графы исполнения из одного запроса и из них выбирать оптимальный. Однако, у SQL есть проблемы - через него нельзя выразить произвольное MapReduce преобразование, а user-defined функции не поддаются оптимизации, т.к. на них не распространяются правила реляционной алгебры.
%  d Наш подход - основан на задании алгебры над пользовательскими операциями через контракты. Таким образом, мы можем переставлять произвольные операции в соответсвии с правилами, которые задали пользователи. В частности, через наш подход выражается SQL - с соответствующими правилами в контрактах (пример?).
%  e Большое отличие распределенных от баз данных - обычно запросы выполняются долго и меняются редко, а данные меняются часто. Наш подход позволяет делать совместную оптимизацию запросов, заданных SQL и произвольными пользовательскими операциями через сведение к контрактам. Таким образом, открывается возможность переиспользовать уже запущенные операции, используя их в различных задачах.


% Distributed data processing has been extensively evolved in the past decades.
% There are two approaches in distributed processing: batch and streaming.
% Batch processing systems aim to handle large static datasets and maximize throughput while streaming frameworks deal with unbounded sequences of data and mostly optimize the latency of processing.
% One of the first batch processing models that have been widely adopted is MapReduce~\cite{Dean:2008:MSD:1327452.1327492}.
% It has a wide range of implementations, including Hadoop~\cite{hadoop2009hadoop}, Spark~\cite{Zaharia:2016:ASU:3013530.2934664}, Dryad~\cite{Isard:2007:DDD:1272996.1273005}.
% Popular streaming frameworks are Flink~\cite{carbone2015apache}, Storm~\cite{Toshniwal:2014:STO:2588555.2595641}, Spark Streaming~\cite{Zaharia:2012:DSE:2342763.2342773}, etc.

There are two common options to define computations in state-of-the-art distributed processing systems.
The first option is defining a logical {\em execution graph}.
An execution graph is a directed graph, where nodes represent operations and edges denote data flows.
This mechanism is robust and suitable for complex pipelines but has limited optimization abilities. 
A processing system can apply only local physical optimizations because it cannot ensure that the restructured graph is equivalent to the original one.

Another way is declarative: the user defines the result that she aims to obtain, and the execution graph is generated automatically by the processing system.
The declarative approach is commonly implemented using SQL. 
SQL is based on relational algebra that captures a set of operations and query transformation rules.
This way, a system can obtain multiple equivalent execution graphs and choose the most optimal one based on a {\em cost model}. 
Unfortunately, SQL is not rich enough to express some user-defined operations, e.g., complex machine learning pipelines~\cite{PROOF}.

In this work, we present a declarative framework called {\em Calco} to specify distributed dataflows. Our framework bases on the ideas from the programming languages axiomatic semantics and aim to solve the following problems:

{\bf Optimization of custom operations}: user can annotate a custom operation by a {\em contract} which captures operation properties and allows the system to apply global optimization, e.g., to permute such operations.

{\bf Cross-domain optimization}: SQL can be automatically translated into Calco contracts and optimized together with custom user-defined operations.

% Dataflows can be also specified directly by defining an execution graphs.
% Nodes of the such graphs are custom operations, so the transformation rules that produce equivalent graphs cannot be automatically deduced.

% SQL is based on relational algebra with a concrete number of operations and defines rules that describe permutation possibilities that can be used in query optimization.
% However, this approach is not flexible enough in real-world dataflows specifications, e.g., some map-reduce computations cannot be translated into SQL~\cite{PROOF}. Some frameworks for SQL over distributed systems, such as Beam~\cite{Begoli:2019:OSR:3299869.3314040}, allow registering user-defined operators in SQL to mitigate this problem. However, this approach significantly reduces the optimization feasibilities because relational algebra is not aware of such operators.

% because it is needed to specify custom operations (maybe stateful) that accord to the problem domain. (TODO proof). Also SQL is not a native way to define distributed computations, so its implementations can be not powerful enough.


% Dataflows can be also specified directly by defining an execution graphs.
% Nodes of the such graphs are custom operations, so the transformation rules that produce equivalent graphs cannot be automatically deduced.
% Thus, while physical optimizations such as operators fusion can be applied for an execution graph~\cite{grulich2020grizzly}, only manual optimization is possible on the logical level (TODO so what?).
% Real-world graphs can be rather complex to optimize them manually.
% Moreover, the runtime statistics can make graphs cost evaluation much more precise, and this information is not available during development.


% It should be possible to do automatic optimization.

% Having the runtime information can make graphs cost evaluation much more precise, and this information is not available for programmer during development.
% One graph can work for a long time on a changing data streams (as opposite to database queries), so dynamic optimization is a good idea (TODO).

% Many business tasks are computationally expensive, but have large common parts.
% That makes programmers to write very big graphs that solve many problems at once to maximally reuse computational results.
% Also it makes analytics to write complex multi-purpose SQL queries.
% So cross-query optimization is needed, but it does not supported now.

% Let's suppose that we have some number of binary modules, written in different programming languages.
% And we want to integrate them in a complex cross-domain dataflow.
% Nowadays we have no means to automatically optimize it.

% Data analytics scenarios may include a lot of concurrent SQL queries along with custom execution graphs. These pipelines can be large and involve many common parts. While these parts can be shared via user-defined SQL operators, the optimization of joint dataflows is limited. Such optimization requires deep interoperability between SQL and user-defined execution graphs based on common rules.

% We need more information about the properties of each user-defined operation to ensure such interoperability. One way is to specify the properties of custom operations that allow the system to permute them safely. In this work, we use similar ideas to the programming languages' axiomatic semantics, based on the Hoare triples. Hoare triple consists of the first predicate that should be satisfied for the program context before the statement execution, the statement, which semantics we describe, and the second predicate that should be satisfied for the program context after the statement execution.

% We propose a framework (TODO too proudly) called {\em Calco} to annotate each user-defined operation with an input contract that the input data should satisfy and an output contract that describes how this operation changes the input. Having all intermediate operations annotated with such contracts, we can specify a whole pipeline just as an expected input and output contract. Such representation allows us to generate multiple execution graphs which satisfy contracts. Hence, we obtain a space of possible execution graphs for choosing the most optimal one.

% TODO place upper
% Note that the optimization problem can be decomposed into two parts: generation of all possible graphs and cost evaluation of them.
% In this study, we consider only the first part of the problem, while the second one relates to our future work.
% In summary, our contributions are as follows: TODO

% The rest of the paper is organized as follows: TODO

% Also we want to use custom operations in the graph, because defining concrete set of operations with known permutation rules is not enough flexible approach.
% Thus we propose to specify permutation constraints of custom operations manually.
% We use similar idea to the axiomatic semantics of the programming languages, based on the Hoare triples.
% Hoare triple consists of the first predicate that should be satisfied for the program context before the statement execution, of statement, which semantics we describe, and of the second predicate that should be satisfied for the program context after the statement execution.
% We propose to annotate each graph operation with the input contract, that the input data stream should satisfy, and with the output contract that describes, how this operation changes the input data stream.
% So we can specify a computation as a set of annotated nodes, using this representation we can generate all concrete graphs with such nodes, which contracts are satisfied (hence, all operations work properly).
% Such novel specification approach we call CGraph.

% Our work has two general parts.
% Firstly, we investigate how to define and generate a set of the equivalent concrete execution graphs using the CGraph specification.
% Secondly, we need to build a concrete graphs cost evaluation function and develop instruments to gather runtime statistics and dynamically reconfigure execution graphs.
% In this paper we consider only the first part of the work and briefly outline challenges of the second.

% Let's consider CGraph and contracts in details.
