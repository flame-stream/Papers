Cost-based optimization requires statistical information on data in order to calculate cost function values for each plan. However, upon the start of a streaming query execution, no information about the data, such as its arrival rate, is available. Therefore, in order to properly apply cost-based optimization to streaming SQL queries, it is necessary to collect data statistics over the course of query execution. Moreover, since we possess no definitive knowledge about the arriving data, in order to utilize an optimal plan for the upcoming windows, we need to predict statistics for each next window based on statistics for previous windows. To this end, we identify two challenges in using statistics for streaming query optimization:

\begin{itemize}
    \item \textbf{Technical}:
    Statistical information on stream elements, such as their arrival rate, needs to be collected during execution at runtime without seriously affecting the performance of a distributed SPE. % which can present certain challenges since there is no centralized point at which to collect statistics, so we would need to aggregate it somehow, which can affect the performance of the system itself
    \item \textbf{Research}
    Next window statistics should be predicted using statistics collected for previous windows. We assume that previous window statistics would present a decent baseline; however, this assumption needs to be tested. It is expected that a simple baseline will be good enough in most simple cases, but in general, statistics prediction is not an easy task.
\end{itemize}

Popular SPEs do not offer any statistics fetching or predicting. For example, Apache Beam, which utilizes Apache Calcite, a dynamic data management framework, for its SQL processing functionality, passes constant values to the query planner instead of any actual data statistics.

% idk how to describe it. is selecting a good regressor a research challenge? should it be general or detailed? should we include other components of typical ML research?