\label {sec:fs-optimization-experiments}
In this section, we describe the preliminary experiments that demonstrate how the choice of a query plan affects the performance. We aim to show that a well-timed switch from an execution graph that is no longer optimal for the current data to a more optimal one would improve performance significantly. First, we present the experiment setup; then, we demonstrate our results. 
% мы хотим показать, что своевременное переключение между планами может улучшить перформанс; мы просто показываем, что разные планы дают разный перформанс, и поэтому переключение между ними дало бы улучшение
\subsection{Setup}

For our experiments, we used the query described in Section \ref{sec:fs-optimization-problem-statement} and the Apache Beam implementation of the NEXMark benchmark model. In this implementation, each entity (\texttt{Person}, \texttt{Auction}, or \texttt{Bid}) is represented via a subclass of the \texttt{Event} class. Each event is generated by an unbounded source in accordance with the provided configuration, which includes parameters such as the arrival rate for each event, the $|Person|:|Auction|:|Bid|$ ratio, the time-based window size, etc. 

First, we execute this query using the plan in which \texttt{Auction} and \texttt{Person} are joined first, and the result is joined with \texttt{Bid}; then, we use the plan in which \texttt{Person} and \texttt{Bid} are joined first (see Section  \ref{sec:fs-optimization-problem-statement}). For each run, we use a different $|Person|:|Auction|:|Bid|$ ratio.

To evaluate performance, we measure latency and throughput for each window. For a join result, we define \textit{latency} as the difference between the maximum arrival time of each of the rows making up the join result and the output time of the resulting row; then, we select the maximum out of the latency values of all the rows in a window. The throughput that we measure is \textit{sustainable throughput}, i.e., the maximum events arrival rate that a streaming system can handle without the continuous buildup of latency.
% sustainable throughput is the maximal rate at which the latency does not start increasing catastrophically
% benchmarking distributed processing systems <-- that is the article from which we use the definition

We have conducted our experiments on a single machine equipped with a 1.4 GHz Intel Core i5-8257U CPU (4 cores) and 8 GB of memory using the Apache Flink runner. Since Flink scales up \cite{karimov2018benchmarking}, we expect that our results for a single worker should scale up to multiple workers.

\subsection{Results}

In the subsequent text the plan which joins \texttt{Auction} and \texttt{Person} first is referred to as \textit{Plan 1}; the plan which joins \texttt{Person} and \texttt{Bid} first is \textit{Plan 2}.
\subsubsection{Latency}

We generated 1000000 events with the arrival rate of 10000 events per second and time-based windows of varying sizes.

Figure \ref{fig:latency_ratio} demonstrates how latency changes depending on data characteristics. The $|Person|:|Auction|:|Bid|$ ratio impacts arrival rate for each kind of entities, thereby influencing latency. As expected, the plan in which \texttt{Person} and \texttt{Auction} are joined first delivers better results when the arrival rate of \texttt{Bid} records significantly overwhelms the rates of \texttt{Person} and \texttt{Auction}, while the plan in which \texttt{Person} and \texttt{Bid} are joined first works best for cases where the rate of \texttt{Auction} records far exceeds those of \texttt{Person} and \texttt{Bid}. 

As Figures \ref{fig:latency_window_5590} and \ref{fig:latency_window_5905} demonstrate, the latency does not grow linearly as the window size increases. This is due to the fact that the join operator processes the records as they arrive instead of starting to process them only after the last record in the window has arrived; thus, the results are ready to emerge shortly thereafter the arrival of the last record in the window.

Figure \ref{fig:latency_diff_against_window_size} demonstrates how the difference in latency for the two execution plans changes with the window size. Since the difference grows as the window size increases, statistics-based optimization should provide an even bigger performance gain for larger windows.

\subsubsection{Throughput}

The parameters for throughput estimation were the same as for the latency estimation. As Figure \ref{fig:throughput_ratio} shows, Plan 2 delivers higher throughput in case of the arrival rate of \texttt{Auction} significantly exceeding that of \texttt{Person} and \texttt{Bid}, while Plan 2 performs better in case of the arrival rate of \texttt{Bid} being significantly higher. This corresponds with the latency measurements. Throughput decreases with the increase of window size, as shown in Figures \ref{fig:throughput_window_5590} and \ref{fig:throughput_window_5905}. Throughput difference does not depend on the window size, Figure \ref{fig:throughput_diff_against_window_size} demonstrates.

\begin{figure*}[t!]
    \begin{subfigure}[b]{0.43\textwidth}
            \include{plots/fs-optimization-latency-ratio}
            \captionsetup{justification=justified}
            \caption{Latency}
            \label{fig:latency_ratio}
    \end{subfigure}
    \hspace{5mm}
    \begin{subfigure}[b]{0.43\textwidth}
            \include{plots/fs-optimization-throughput-ratio}
            \captionsetup{justification=justified}
            \caption{Throughput}
            \label{fig:throughput_ratio}
    \end{subfigure}
    \caption{Latency and throughput for different ratios: out of 100 events, $|Person| = 5$, $|Auction|$ is the value on the $x$-axis, $|Bid| = 100 - |Person| - |Auction|$}
    \label{fig:ratio_plots}
\end{figure*}

\begin{figure*}[t!]
    \begin{subfigure}[b]{0.32\textwidth}
            \include{plots/fs-optimization-latency-window-5590}
            \captionsetup{justification=justified}
            \caption{$|Person|:|Auction|:|Bid|$ = 5:5:90}
            \label{fig:latency_window_5590}
    \end{subfigure}
    \hspace{2mm}
    \begin{subfigure}[b]{0.32\textwidth}
            \include{plots/fs-optimization-latency-window-5905}
            \captionsetup{justification=justified}
            \caption{$|Person|:|Auction|:|Bid|$ = 5:90:5}
            \label{fig:latency_window_5905}
    \end{subfigure}
    \hspace{2mm}
    \begin{subfigure}[b]{0.32\textwidth}
            \include{plots/fs-optimization-latency-difference-window}
            \captionsetup{justification=justified}
            \caption{Latency difference for Plans 1 and 2}
            \label{fig:latency_diff_against_window_size}
    \end{subfigure}
    \caption{Latency for different window sizes and $|Person|:|Auction|:|Bid|$ ratios}
    \label{fig:latency_plots}
\end{figure*}

\begin{figure*}[t!]
    \begin{subfigure}[b]{0.32\textwidth}
            \include{plots/fs-optimization-throughput-window-5590}
            \captionsetup{justification=justified}
            \caption{$|Person|:|Auction|:|Bid|$ = 5:5:90}
            \label{fig:throughput_window_5590}
    \end{subfigure}
    \hspace{2mm}
    \begin{subfigure}[b]{0.32\textwidth}
            \include{plots/fs-optimization-throughput-window-5905}
            \captionsetup{justification=justified}
            \caption{$|Person|:|Auction|:|Bid|$ = 5:90:5}
            \label{fig:throughput_window_5905}
    \end{subfigure}
    \hspace{2mm}
    \begin{subfigure}[b]{0.32\textwidth}
            \include{plots/fs-optimization-throughput-difference-window}
            \captionsetup{justification=justified}
            \caption{Throughput difference for Plans 1 and 2}
            \label{fig:throughput_diff_against_window_size}
    \end{subfigure}
    \caption{Throughput for different window sizes and $|Person|:|Auction|:|Bid|$ ratios}
    \label{fig:throughput_plots}
\end{figure*}


\subsection{Discussion}

Our experiments demonstrate that streaming query execution performance depends on the plan used for the execution, and the optimality of the plan depends on the data characteristics, which proves the necessity of adaptive optimization of streaming queries. Particularly, the first steps towards adaptive optimization should be predicting statistics for each window and performing runtime graph migration, since the results of the experiments show that even the current planners, such as the Volcano query planner \cite{graefe1993volcano} used in Apache Calcite, which are unaware of whether the data comes from a stream or a table, could use those statistics to produce a better execution plan. These two challenges will be the focus of our future work. After that, the planner can be enhanced by introducing distributed streaming systems-specific operators and their costs.
