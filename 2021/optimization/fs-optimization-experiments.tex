\label {sec:fs-optimization-experiments}
In this section, we describe the preliminary experiments that we conducted in order to show how the choice of a query plan affects the performance. First, we present the experiment setup and the configuration used; then, we demonstrate our results.

\subsection{Setup}

For our experiments, we used the same query that was described in Section \ref{sec:fs-optimization-problem-statement} and the Apache Beam implementation of the NEXMark benchmark model. In this implementation, each entity (\texttt{Person}, \texttt{Auction}, or \texttt{Bid}) is represented via a subclass of the \texttt{Event} class. Each event is generated by an unbounded source in accordance to the provided configuration, which includes parameters such as the arrival rate for each event, the $|Person|:|Auction|:|Bid|$ ratio, the time-based window size, etc. 

First, we execute this query using the plan in which \texttt{Auction} and \texttt{Person} are joined first, and the result is joined with \texttt{Bid}; then, we use the plan in which \texttt{Person} and \texttt{Bid} are joined first (see Section  \ref{sec:fs-optimization-problem-statement}). For each run, we use a different $|Person|:|Auction|:|Bid|$ ratio.

To evaluate performance, we measure the the latency and the throughput for each window. For a join result, we consider the \textit{latency} to be the difference between the maximum arrival time of each of the rows making up the join result and the output time of the resulting row; then, we select the maximum out of the latency values of all the rows in each window. We calculate \textit{throughput} as the ratio of execution time to the number of the elements. Additionally, we measure the correspondence between window size and latency.

\subsection{Running configuration}
We have conducted our experiments on a local machine equipped with a 1.4 GHz Intel Core i5-8257U CPU (4 cores) and 8 GB of memory using the Apache Flink runner. We plan to repeat the same experiments on a distributed system.

\subsection{Results}

In the subsequent text the plan which joins \texttt{Auction} and \texttt{Person} first is referred to as \textit{Plan 1}; the plan which joins \texttt{Person} and \texttt{Bid} first is \textit{Plan 2}.
\subsubsection{Latency}

We generated 1000000 events with the arrival rate of 10000 events per second and time-based windows of varying sizes.

As Figure \ref{fig:latency_against_window_size} demonstrates, the latency does not grow linearly as the window size increases. This is due to the fact that the join operator processes the records as they arrive instead of starting to process them only after the last record in the window has arrived, and thus the results are ready to emerge shortly thereafter the arrival of the last record in the window.

Figure \ref{fig:latency_diff_against_window_size} demonstrates how the difference in latency for the two execution plans changes with the window size. Since the difference grows as the window size increases, statistics-based optimization should provide an even bigger performance gain for larger windows.

% TODO rewrite this. first reread this then rewrite this.
Figure \ref{fig:latency_ratio} demonstrates how query execution time changes depending on data characteristics. The $|Person|:|Auction|:|Bid|$ ratio impacts arrival rate for each kind of entities, thereby influencing join execution time. As expected, the plan in which \texttt{Person} and \texttt{Auction} are joined first delivers better results when the arrival rate of \texttt{Bid} records significantly overwhelms the rates of \texttt{Person} and \texttt{Auction} (the 5:5:90 ratio is an example of such a case), while the plan in which \texttt{Person} and \texttt{Bid} are joined first works best for cases where the rate of \texttt{Auction} records far exceeds those of \texttt{Person} and \texttt{Bid}. 


\subsubsection{Throughput}

The parameters for throughput estimation were the same as for the latency estimation. As Figure \ref{fig:throughput_against_window_size} shows, 


\begin{figure}
\centering
\include{plots/fs-optimization-latency-ratio}
\caption{Latency plotted against different $|Person|:|Auction|:|Bid|$ ratios, with $|Person|$ fixed at 5, values of $|Auction|$ on the $x$ axis, $|Bid| = 95 - |Auction|$.} 
\label{fig:latency_ratio}
\end{figure}


\begin{figure}
\centering
\include{plots/fs-optimization-latency-window}
\caption{Latency plotted against window size for different $|Person|:|Auction|:|Bid|$ ratios and execution plans.}
\label{fig:latency_against_window_size}
\end{figure}


\begin{figure}
\centering
\include{plots/fs-optimization-latency-difference-window}
\caption{The difference in latency for the two execution plans plotted against window size.}

\label{fig:latency_diff_against_window_size}
\end{figure}

\begin{figure}
\centering
\include{plots/fs-optimization-throughput-window}
\caption{Throughput plotted against window size for different $|Person|:|Auction|:|Bid|$ ratios and execution plans.}
\label{fig:throughput_against_window_size}
\end{figure}


\subsection{Discussion}

The results of our experiments demonstrate that streaming query execution performance depends on the plan used for the execution, and the optimality of the plan depends on the data characteristics, which proves the necessity of adaptive optimization of streaming queries. Particularly, the first steps towards adaptive optimization should be predicting statistics for each window and performing runtime graph migration, since the results of the experiments show that even the current planners, such as the Volcano query planner \cite{graefe1993volcano} used in Apache Calcite, which are unaware of whether the data comes from a stream or a table, could use those statistics to produce a better execution plan. These two challenges will be the focus of our future work. After that, the planner can be enhanced by introducing distributed streaming systems-specific operators and their costs.



% запрос выполняется многократно на каждом окне заново и поэтому легче собирать статистику и по ней учиться