\label {sec:fs-optimization-related-work}

We structure the related work into three areas: database query optimization, adaptive optimization of streaming queries, and predicting data statistics for queries using machine learning techniques.

\subsubsection*{Database query optimization}
Early efforts in database query optimization include the System R optimizer \cite{selinger1979access}, which introduced a dynamic programming algorithm for finding an optimal execution plan; the Starburst optimizer \cite{haas1989extensible}, which proposed new rule-based optimization techniques; and others: a survey chapter on query optimization in relational databases can be found in \cite{Neumann2018optimization}. Our particular interest lies in the areas of query optimization in distributed database systems and adaptive query optimization. The former has been described in \cite{kossmann2000thestate}; a detailed survey \cite{deshpande2007adaptive} is a good reference for the latter. Adaptive optimization of streaming SQL queries is different from database optimization: while in databases the execution plan is modified during the execution of a query, in streaming systems the plan should be changed after a window has been processed but before the next window processing has started; moreover, database data is static while streaming data is continuously updated.

\subsubsection*{Streaming query adaptive optimization}
An overview of streaming query optimizations can be found in \cite{hirzel2014catalog}: most of these optimizations can be classified as rule-based and are applied statically at compilation time, although dynamic versions are listed for several of these; we are interested in adaptive optimization at runtime. Various works explore adaptive optimization of streaming queries: \cite{babu2004adaptive} focuses on finding the optimal order of pipelined filter operators, with possible reordering at runtime; it uses the current known data statistics while we are interested in predicting the next window statistics and using those. Other works focus on physical level adaptive optimizations: one such study can be found in \cite{grulich2020grizzly}; we are interested in logical level optimization instead.

\subsubsection*{Predicting data statistics}
Some studies (\cite{krishnan2018learning, marcus2019neo}) use machine learning to predict optimal execution plans, while others explore join cardinality prediction. Database cardinality prediction techniques can be categorized as either query-driven or data-driven. Query-driven prediction models learn on sets of queries; among studies employing this approach are \cite{liu2015cardinality, CHEN20211047, kipf2018learned}, which use neural networks, and \cite{ortiz2019empirical}, which explores deep learning methods. Data-driven prediction models, described in \cite{hilprecht2020deepdb} and \cite{yang2020neurocard}, are trained on data without queries and attempt to learn characteristics such as distribution of single attributes as well as joint distributions of multiple attributes. Neither of these approaches fit the streaming queries scenario: SPEs are executing the same query on each subsequent window, rendering query-driven approaches unusable, and the data is continuously updating, which means data-driven approaches are not employable either. Additionally, neural networks might not be the best choice of a learning model for a small amount of data contained in a single window; instead, it would potentially be more beneficial to use a model similar to \cite{street2001ensemble}, a fixed-size ensemble of heuristically replaced classifiers.

% cost-based database optimization

% physical-level/non-specific to sql database optimizations

% predicting cardinality using ML in databases or in streams even if that exists

% in-flight graph migration, possibly included in sections about optimizations above

% TODO move this into the machine learning section

% On the topic of predicting data statistics, it must be noted that while there have been various attempts at predicting cardinality in databases (\cite{liu2015cardinality, kipf2018learned, ortiz2019empirical, CHEN20211047}), predicting statistics for streaming queries might yield better results since the query is being run on different subsequent windows for an extended period of time, unlike in databases, where each new query is run separately from its predecessors. This could be advantageous in predicting statistics not only for the next window but for intermediate execution results as well.


% The
% methods presented in this paper take advantage of plentiful
% data, building separate classifiers on sequential chunks of

% training points. These classifiers are combined into a fixed-
% size ensemble using a heuristic replacement strategy.