\label {fs-acker-intro}

Distributed stream processing engines (SPEs), such as Flink~\cite{carbone2015apache}, Heron~\cite{Kulkarni:2015:THS:2723372.2742788}, or MillWheel~\cite{Akidau:2013:MFS:2536222.2536229}, aim at handling potentially unbounded sequences of data elements. These systems receive input elements one-by-one, process them, update the internal in-memory state, and eventually release results. 

However, the processing of unbounded data sequences has at least two difficulties. First, SPEs should be aware of resource consumption. For instance, SPE can maintain an in-memory state with no upper bound in size and run out of memory. Second, SPEs observe elements sequentially, but traditional relational operators assume finite data sources with random access. Therefore, to produce correct results, there is a need to determine the ``bounds'' of the query.

In practice, infinite streams are often considered as a mixture of possibly finite substreams to deal with these difficulties. The examples of such scenarios are listed below.
\begin{enumerate}
    \item {\bf State pruning}. Most SPEs aggregate state by processing keys that are used for data partitioning. For example, if the task is to count the number of outgoing links from web pages, the page address would likely be the key. The problem here is that the state can grow unlimitedly~\cite{Tucker:2003:EPS:776752.776780}. To prevent overflow, SPE can remove state for outdated keys, e.g., for deleted web pages. In this case, we can consider the stream as a mixture of substreams, each of them can contain information about only some specific webpages.  
    \item {\bf Time windowed aggregations}. One way to produce results for a relational query is to process elements within event generation time windows. The time window can be considered as a substream. To release output, an SPE needs a guarantee that all data within a time window are received. This method is widely applied in online analytics~\cite{traub2018scotty} and is used for streaming SQL implementation~\cite{Begoli:2019:OSR:3299869.3314040}.
    \item {\bf State snapshotting}. Epoch-based state snapshotting is a popular recovery technique applied in Flink~\cite{Carbone:2017:SMA:3137765.3137777}, Storm~\cite{Toshniwal:2014:STO:2588555.2595641}, Samza~\cite{Noghabi:2017:SSS:3137765.3137770}, IBM Streams~\cite{jacques2016consistent}. Within this method, all input elements are divided into special substreams called {\em epochs}. An SPE takes a state snapshot when a regular epoch is {\em atomically} processed. In case of failures, SPE can consistently recover state from the snapshot~\cite{2015arXiv150608603C}. 
\end{enumerate}

Each of these scenarios can be considered as a special case of {\em substreams management problem}. This problem is to monitor for substreams emergence and termination. More formally, a substream is a part of the stream such that all its elements satisfy some predicate. For example, in the case of state pruning, the predicate is {\em [a data element may contain key $X$]}, while for time window aggregations, the predicate is {\em [a data element may be generated with timestamp less than $T$]}. 

Substreams can be finite or infinite. In this paper, we focus on substreams lifespan monitoring, so we consider finite substreams only. Note that various problems may require different properties of substream termination events:
\begin{itemize}
    \item Time windowed aggregations include some special scenarios, e.g., deterministic windowed join, that require an order of signals to be synchronized with the order of input elements (termination events from data producers)~\cite{najdataei2019stretch, gulisano2016scalejoin}.
    \item Epoch is a substream that an SPE should atomically process. Therefore, the event that an epoch ends should be received before any elements from the next epoch~\cite{2015arXiv150608603C}. 
    \item State pruning problem does not require any specific termination events properties. However, high latency between the actual substream termination (the event from data producer) and termination event receiving may reduce the efficiency of the technique.
\end{itemize}

A popular substreams management method is punctuations framework~\cite{tucker2003exploiting}. The main idea behind the punctuations framework is to divide the stream into substreams by injection of particular elements that define the ``border'' of a substream. While the punctuations approach is robust and easy-to-implement, it has two limitations. First, it does not support cyclic dataflows in general~\cite{carbone2018scalable}. Second, it has high network traffic overhead so that punctuations can reduce the throughput of an SPE for small substreams~\cite{Li:2008:OPN:1453856.1453890}. 

As we can see, substreams management problem appears in many practical scenarios. Moreover, the features of some substreams impose requirements on the substream termination events. In this work, we formalize the substream management problem and its properties to compare the capabilities of various techniques. We introduce a new substream management approach that supports all features required by practical scenarios and provides the lowest possible network overhead on regular processing. Our main contributions are the following:


% State-of-the-art distributed stream processing systems such as Flink~\cite{Carbone:2017:SMA:3137765.3137777}, Heron~\cite{Kulkarni:2015:THS:2723372.2742788}, or MillWheel~\cite{Akidau:2013:MFS:2536222.2536229} are able to execute complex dataflows consisted of multiple operators that can be partitioned among workers. Each operator may execute almost arbitrary user-defined code that transforms elements into other ones or filters them out. After multiple transformations, it may not be clear which ingested by a system input element spawned an output. 

% To satisfy {\em consistency} and {\em fault tolerance} requirements, modern streaming engines support {\em dependency tracking} between input and output elements. Tracking mechanisms usually provide notifications that some set of ingested elements has already been transformed into outputs by the whole dataflow or its part. Such notifications are required for a plenty of problems: consistent {\bf state snapshotting}~\cite{Akidau:2013:MFS:2536222.2536229, 2015arXiv150608603C} to provide for correct failure-recovery, {\bf transactional processing} to ensure {\em delivery guarantees}~\cite{thepaper, Carbone:2017:SMA:3137765.3137777}, {\bf data producer cleanup}~\cite{Noghabi:2017:SSS:3137765.3137770} to support recovery while persistently storing only a part of input records, and so on. 

% Although most of the mentioned problems have been extensively studied for databases~\cite{DBLP:books/mk/WeikumV2002}, classical databases do not commonly face the problem of matching input and output because they mostly work under less strict latency requirements and persistently save information about all applied transformations. For instance, transactional processing methods employed in databases cannot be used as-is in distributed streaming systems without an adaptation of some dependency tracking technique~\cite{2015arXiv150608603C}.

% Most streaming systems apply one of the following approaches for dependency tracking: {\em micro-batching} and {\em markers}. In micro-batching, the output element may be originated only by an input one from the same micro-batch. Therefore, the completeness of a micro-batch indicates that all input elements within this batch are transformed into output elements. Markers approach bases on an injection of special input elements into a dataflow. These elements are broadcasted to each partition of the next operator and play the role of separators between ordinary records. Receiving such element from all partitions indicates that all upstream operators have entirely processed a particular set of input elements. 

% Both micro-batching and marker approaches have limitations and can induce high overhead on regular processing. Micro-batching has well-known issues with latency-conscious dataflows~\cite{S7530084}. It is hard to track individual records due to the ineffectiveness of very small micro-batches~\cite{Zaharia:2012:DSE:2342763.2342773}. The application of marker-based methods is limited for cyclic dataflows~\cite{Carbone:2017:SMA:3137765.3137777}. Besides, as we show further, markers may induce significant overhead on throughput due to a huge amount of extra network traffic.

% In this work, we present \tracker , a dependency tracking technique that provides for individual elements monitoring, while inducing a small overhead on regular processing. On the one hand, \tracker\ is suitable for a precise element-at-time streaming model without the need of micro-batching. On the other hand, it supports cyclic dataflows and requires a small amount of service traffic. \tracker\ can also monitor streaming elements within parts of dataflow as well as marker-based techniques. \tracker\ is deployed as an external agent that can scale out to sustain a high rate of input records. 

% This paper complements preliminary publications~\cite{we2018beyondmr, we2018adbis, thepaper}, which describe a stream processing system that uses \tracker\ as a component. In this paper, we detail a formal concept of \tracker , propose its distributed implementation, and evaluate the~\tracker\ performance in a more narrowly focused way. Our main contributions are:
% \begin{itemize}
%     \item Design of general formal concepts for a dependency tracking mechanism
%     \item Transformation of these concepts into the~\tracker\ using ideas from Apache Storm {\em \acker}~\cite{Toshniwal:2014:STO:2588555.2595641}
%     \item Demonstration of the \tracker\ practical feasibility and performance insights
% \end{itemize}

% The rest of the paper is organized as follows: Section~\ref{fs-acker-motivation} gives an overview of existing dependency tracking solutions and practical tasks that require a tracking mechanism. In Section~\ref{fs-acker-design}, we design formal concepts for dependency tracking methods and turn these concepts into the \tracker\ mechanism. Section~\ref{fs-acker-impl} summarizes the implementation of \tracker\ for both centralized and distributed setups with optimizations that can reduce the amount of extra traffic. In Section~\ref{fs-acker-experiments}, we show that the proposed technique is scalable, and can outperform alternatives employed in state-of-the-art stream processing engines. Finally, we discuss our conclusions in Section~\ref{fs-acker-conclusion}.