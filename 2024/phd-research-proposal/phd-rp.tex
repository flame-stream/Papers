\documentclass[runningheads]{llncs}
% packages should be added as needed 
\usepackage{graphicx}

\usepackage{algorithm} % for algorithms
%\usepackage{mathtools}

\usepackage{algpseudocode}
\algblockdefx[Process]{Process}{EndProcess}[2][Unknown]{{\bf Process} {\it #2}}{}
\algblockdefx[Event]{Event}{EndEvent}[2][Unknown]{{\bf upon event #2 do}}{}

\usepackage{booktabs} % For formal tables
\usepackage{cite} %for multiple refs

\usepackage{amssymb} %for nice emptyset

\usepackage[T1]{fontenc}
\usepackage{inconsolata}

\pagestyle{plain} % removes running headers

\newcommand{\PicScale}{0.5}
\newcommand {\FlameStream} {FlameStream}

\begin{document}

\title {Research Proposal: Consistent Scalable Processing of Data Streams in  a Distributed Environment
}
\author{Artem Trofimov}
\institute{Constructor University}

\maketitle

\begin{abstract}
Batch and stream systems cover two various data processing scenarios. In the first scenario, data is preliminarily collected and stored, and a system should produce the final processing result. In the second scenario, data is represented as a discrete and potentially infinite stream of items, and there is a need to continuously produce a stream of freshly updated results. This distinction implies major problems in distributed stream processing. Unlike batch processing, it is not possible to restart processing from the very beginning. Hence, a stream processing engine (SPE) needs sophisticated fault tolerance mechanisms to ensure data consistency in case of failures. If failures do not affect results, one states that an SPE provides an exact-once guarantee. The second problem is the need to continuously produce results stream during processing. It may be non-trivial to detect the exact moment when the resulting item is ready to be released. State-of-the-art approaches for both fault tolerance and results updating problems imply significant latency overhead. Latency is one of the key metrics in stream processing since the freshness of results is usually valuable. In this proposal, we overview state-of-the-art techniques that solve the mentioned issues. We offer a technique to achieve the exactly-once guarantee with low latency overhead based on ensuring deterministic processing in a distributed environment. We also propose a theoretically more efficient approach to form a result stream even in the case of high granularity of results. We formulate research questions that should be answered to confirm our ideas. 
\end{abstract}

\section {Introduction}

\section {Review and literature}
\input{2024/phd-research-proposal/phd-related-work}

\section {Discussion and observations}

\section {Research questions}

In this work, we aim to answer the following research questions:
\begin{enumerate}
    \item What are the reasons behind the high latency overhead on exactly-once guarantee in state-of-the-art SPEs?
    \item Can deterministic stream processing systems theoretically be more efficient regarding latency than non-deterministic? 
    \item What is the theoretical lower bound estimation of required network traffic to detect that the result update is ready in a distributed environment? 
    \item Is it possible to provide a technique that achieves this lower bound, and will it be more practically efficient than state-of-the-art? 
\end{enumerate}

\section {Current Progress}

\section {Conclusion}
\input{2024/phd-research-proposal/phd-conclusion}

\bibliographystyle{splncs04}
\bibliography{bibliography/flame-stream}

\end {document}

\endinput
