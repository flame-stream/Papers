\subsection{Consistency in stream processing}
\label{consistency_overview}

In this section, we discuss several types of consistency in distributed stream processing. We start with delivery guarantees in Section~\ref{delivery_guarantees}: exactly-once, at-least-once, and at-most-once. Delivery guarantees specify the rules for how a system should behave in case of node failures to maintain a consistent state and prevent data loss or duplication.

Section~\ref{completeness} covers the completeness of results.  In a distributed system, especially in streaming, data can arrive out of order, be delayed, or even temporarily lost. Completeness guarantees that the system's output reflects all the data that should have been processed within a given window or time frame, preventing partial or misleading results.

Eventually, in Section~\ref{transactional} we discuss transactional stream processing (TSP). TSP integrates the low latency processing capabilities of stream processing with the reliability and consistency guarantees of traditional transactional systems. TSP systems achieve consistency by incorporating the ACID (Atomicity, Consistency, Isolation, Durability) properties typically found in traditional database transactions.

% Eventually, in Section~\ref{statistical} we touch on the topic of statistical consistency in stream processing. Statistical consistency refers to the accuracy and reliability of statistical estimates and summaries derived from continuous data streams. This type of consistency ensures that the statistical measures calculated over the data stream are accurate and close to the expected values. Statistical consistency is close to anomaly detection because anomalies, such as sudden spikes or unusual data patterns, can be detected based on deviations from these consistent statistical measures.

\subsubsection{Delivery guarantees}
\label{delivery_guarantees}

\paragraph{Types of delivery guarantees}\mbox{} \\

Delivery guarantees, such as at-most-once, at-least-once, and exactly-once, define how and when data is delivered and processed in the system~\cite{fragkoulis2024survey, carbone2018scalable, Akidau:2013:MFS:2536222.2536229}:
\begin{itemize}
    \item At-most-once is a delivery guarantee in system processing where an input element is processed atomically (with all its derivatives) or not processed at all. This means that the system ensures that each input element is processed only one time at maximum, but there's a possibility that some input elements may not get processed.
    \item At-least-once is a type of delivery guarantee where the system ensures that every input element is processed one or more times. This could lead to potential duplication if an input contains duplicated items. This method ensures no data is lost, but it does not prevent possible repetitions.
    \item Exactly-once is a system processing term that guarantees each input element or transaction is processed exactly one time. This means that it ensures there is no data loss and no duplicate processing. It combines both the prevention of input data losses and the avoidance of repeated delivery of results.
\end{itemize}

The type of delivery guarantee chosen for a system directly influences how that system handles failures~\cite{zhang2024survey, silvestre2021clonos, wang2021consistency}. In at-most-once delivery, if a failure occurs during processing, the system may not attempt to reprocess the input, leading to potential data loss. In at-least-once delivery, the system ensures no data is lost by reprocessing inputs in case of failures. However, this can lead to duplicate processing and results if the system doesn't track what has already been processed. In exactly-once delivery, the system ensures that every input is processed exactly one time, even in the case of failures. This is the most complex to implement as it needs to ensure atomicity between reading input data, processing, and delivering results, and it requires the system to be able to recover to a consistent state after a failure~\cite{Carbone:2017:SMA:3137765.3137777}.

\paragraph{State recovery and consistency}\mbox{} \\

State recovery is crucial for consistency because it allows a system to return to a known, correct state after a failure. Without state recovery, a failure can leave the system in an inconsistent state, where the results of some operations are lost or incorrect~\cite{Carbone:2017:SMA:3137765.3137777, Akidau:2013:MFS:2536222.2536229}. State recovery also allows the system to resume processing from a recent checkpoint instead of starting over from the beginning, saving both time and computational resources.

In the context of data processing systems, state recovery is particularly important for handling transient failures. For instance, if a node in a distributed system fails and then restarts, it needs to know what data it has already processed to avoid duplicating work or missing some data. By restoring its state from before the failure, the node can ensure it processes each piece of data exactly once, maintaining the consistency of the system's output~\cite{silvestre2021clonos, Carbone:2017:SMA:3137765.3137777, wang2021consistency}.

The two main ways to recover a consistent state are~\cite{fragkoulis2024survey, zhang2024survey}:

\begin{enumerate}
    \item {\em Global State Snapshotting} involves taking snapshots of the entire system's state at given points in time. The snapshot includes the state of all nodes in the system and any in-transit messages. This approach provides a consistent view of the system, but it may lead to high overhead on recovery because there is a need to re-process all data items since the previous snapshot is taken.
    \item {\em Record-Level Logging} involves saving data elements, state, or causal logs after processing each record or a batch of records. This allows for very fine-grained recovery, as the system can resume processing from the last processed record. However, this approach can introduce significant overhead, as the system needs to save the state frequently.
\end{enumerate}

We will dive into the details of the specific state recovery techniques in Section~\ref{phd-related-fault-tolerance}.

\paragraph{Suitable consistency levels for various types of problems}\mbox{} \\

Distributed stream processing is used in many areas~\cite{fragkoulis2024survey} including network monitoring, IoT, financial processing, machine learning, etc. There are many applications, such as financial systems, where the integrity and correctness of the data are crucial~\cite{zhang2024survey}. Therefore, such applications usually require exactly-once delivery guarantee. In applications like fraud detection, IoT, earthquakes detection, or network security, where each piece of data needs to be analyzed to detect potential anomalies or threats, exactly-once processing is also critical. Missing a data point or processing it multiple times could lead to failure in detecting an anomaly or false alarms~\cite{zhou2019scalable, diro2024anomaly, 10.1093/gji/ggac355, geldenhuys2021dependable}.

On the other hand, the need for exactly-once processing guarantees in machine learning applications depends on the specific requirements of the problem being addressed. While exactly-once processing is critical in some scenarios to ensure data integrity and correctness, it might be less crucial in others~\cite{boden2017distributed, webirte}:
\begin{itemize}
    \item {\em Training Data Preparation}: In the preparation phase of training data, ensuring that each data sample is included exactly once is important for the accuracy of the model. Duplicate data can skew the distribution of the training set, leading to biased models. However, many machine learning algorithms are robust to small inconsistencies, so exactly-once processing might not always be strictly required.
    \item {\em Model Training}: During the training process, especially in distributed systems, exactly-once processing might be less critical depending on the training algorithm. For example, stochastic gradient descent, commonly used in training neural networks, can inherently tolerate some level of noise and inconsistency in data updates, as it is designed to converge despite the random nature of the input data batches.
    \item {\em Real-time Predictions}: For real-time machine learning systems, such as those used in recommendation engines or dynamic pricing models, exactly-once processing can be crucial. Ensuring that each event or piece of data affects the system once prevents the model from making decisions based on duplicated data, which could mislead the prediction outcomes.
    \item {\em Online Learning}: In online learning, where the model continuously updates itself based on incoming data streams, exactly-once processing is beneficial to maintain the correctness of the model updates. Duplicate or missed data points can significantly affect the accuracy and reliability.
\end{itemize}

In summary, whether exactly-once or at-least-once processing is required primarily depends on the tolerance of the specific algorithm to inconsistencies and the impact of data duplication or loss on the application effectiveness. In cases where data integrity directly influences decision-making processes, exactly-once processing becomes more crucial.

\subsubsection{Completeness of results}\mbox{} \\
\label{completeness}

In static batch data processing, an operator can read an entire input and produce an output. However, with unbounded streams, there is no defined end to the data, which means that the traditional approach of processing all the data before generating an output is not feasible. Therefore, another important aspect of consistency in distributed stream processing is the {\em completeness} of results~\cite{Tucker:2003:EPS:776752.776780}. Completeness of results in stream processing refers to the extent to which the results generated from processing a data stream fully and accurately reflect all the relevant data for a given query or operation. In other words, it measures whether all the necessary data points have been included in the computation to produce a correct and comprehensive result~\cite{akidau2015streaming}.

\paragraph{Substreams}\mbox{} \\

Substreams in stream processing refer to smaller, finite segments or partitions of a larger, potentially infinite data stream~\cite{Tucker:2003:EPS:776752.776780}. Substreams are created to manage and process data more efficiently by breaking down the continuous data flow into more manageable chunks. A substream can be expressed as a predicate on stream elements. All elements satisfying the predicate form the substream.

For example, in a stream of web server logs, one could create substreams for each user session, where each substream contains all events (page views, clicks, etc.) that occurred within a specific session. Another example is the system that analyzes customers' activity within hourly windows. It needs to ensure that all orders within that hour are included in the analysis. In this case, the substream is formed by all events from that window.

Substreams allow outputting results before all input is processed. By dividing the continuous data stream into smaller, manageable segments, an SPE can produce intermediate results as each substream is processed. In terms of substreams, the problem of result completeness comes down to ensuring that all substream elements are processed before outputting the corresponding result. 

\paragraph{Detecting substreams termination}\mbox{} \\

Solving the problem of detecting that all substream elements have been processed is the prerequisite for ensuring that system results are complete. The strict substream termination guarantee consists of two parts: the source must promise that no more messages from the substream may emerge, and the system must ensure it contains no substream messages. 

The first task requires a contract with a particular data source and is discussed further in this section. Promises, that are also called {\em watermarks}, can be generated periodically or heuristic-based~\cite{Akidau:2013:MFS:2536222.2536229, akidau2015streaming}. There are adaptive approaches to generating watermarks. The first one treats change in the skewness between the event time and processing time of stream elements as a concept drift~\cite{awad2019adaptive}. It utilizes the adaptive window (ADWIN) drift detection technique~\cite{bifet2007learning, grulich2018scalable} to determine when and with what value to generate a new watermark. Another adaptive watermark generation mechanism leverages a time series prediction model. This mechanism dynamically adjusts the frequency and timing of watermark distribution based on the ratio of disordered data and other lateness properties of the data stream~\cite{song2021adaptive}.

The second task is also challenging due to the distributed nature of the system and the absence of a standard message lifetime limit. Hence, there is a need for a more complex mechanism than checking that all elements arrived in a strict order~\cite{Li:2008:OPN:1453856.1453890}. This difficulty increases with the introduction of cycles into dataflow. Punctuations framework was firstly introduced in~\cite{Tucker:2003:EPS:776752.776780} and then developed to a distributed setup in~\cite{Akidau:2015:DMP:2824032.2824076} and~\cite{Carbone:2017:SMA:3137765.3137777}. According to this method, special data elements, called punctuations, are injected into the data stream. These punctuations serve as markers that define the boundaries of substreams. They indicate that all elements following punctuation will not satisfy a specified predicate, effectively marking the end of the corresponding substream. While the punctuations framework is widely adopted in major SPEs~\cite{carbone2015apache, Noghabi:2017:SSS:3137765.3137770, Kulkarni:2015:THS:2723372.2742788}, it has several pitfalls. It is not applicable for execution graphs with cycles~\cite{carbone2018scalable}. Additionally, the punctuations framework generates extra service traffic that can degrade the latency~\cite{DBLP:journals/pvldb/BegoliACHKKMS21} and throughput~\cite{Li:2008:OPN:1453856.1453890} of an SPE.

\paragraph{Window aggregations} \mbox{} \\

A window is a particular case of substreams with time-related criteria to segment the continuous data stream. Several types of windows are widely adopted in modern distributed stream processing engines: tumbling windows, sliding windows, and session windows~\cite{verwiebe2023survey}.

Tumbling windows group events into fixed-size, non-overlapping intervals. Each event from the stream can belong to exactly one tumbling window~\cite{carbone2019stream}. A key characteristic of tumbling windows is their fixed duration~\cite{patroumpas2006window}. Each window has a predefined, constant length, such as 1 minute, 5 seconds, or 1 hour. The next window starts immediately after the previous one. For example, if there is a stream of events with timestamps and one defines a tumbling window of 1 minute, events from 12:00:00 to 12:00:59 belong to the first window, events from 12:01:00 to 12:01:59 belong to the second window, and so on. Tumbling windows are commonly used for aggregation, such as calculating metrics like sum, average, and count over fixed intervals.

A sliding window is defined by two parameters: the window size and the slide interval. The window size specifies the duration of each window, while the slide interval determines how frequently a new window starts~\cite{traub2019efficient}. This means that multiple windows can contain the same event if they overlap. For example, if there is a sliding window with a size of 1 minute and a sliding interval of 30 seconds, a new window starts every 30 seconds, and each window includes events from the past 1 minute. This overlapping nature makes sliding windows particularly useful for continuous monitoring and real-time analytics where it's important to have up-to-date calculations over recent events. For instance, if one wants to calculate a moving average of temperature readings from sensors, a sliding window can provide an ongoing, updated average as new readings come in, rather than waiting for a fixed interval to complete~\cite{ma2017correction}. However, the increased overlap and frequency of updates also come with higher computational costs. Since each event might be processed multiple times (once for each window it falls into), the system needs to handle a greater load, which can affect performance and resource usage~\cite{traub2019efficient, carbone2019stream}.

Session windows are dynamic and determined by the occurrence of events. A session window starts when an event arrives and accumulates events until a predefined period of inactivity, known as the gap, is detected~\cite{Akidau:2015:DMP:2824032.2824076}. Once this gap duration is met without any new events, the session window closes, and any subsequent events will start a new session window~\cite{traub2019efficient}. Gap duration defines how long the system should wait before considering the session ended. Session windows are useful in scenarios where events are sporadic and bursts of activity are separated by idle periods. For instance, in web analytics, user activity on a website can be tracked using session windows, where a session begins when a user starts interacting with the site and ends after a period of inactivity, indicating the user has likely left. However, the dynamic nature of session windows also poses challenges. They require the system to continuously monitor for inactivity gaps, which can be computationally intensive~\cite{traub2019efficient}.

Another important aspect of window aggregations is timing. In stream processing, there are two concepts of time: event time and processing time~\cite{Akidau:2015:DMP:2824032.2824076, carbone2015apache}. Event time refers to the time at which an event actually occurred outside an SPE. This timestamp is usually embedded within the event data itself. Event time is essential for applications that require precise temporal analysis, such as user behavior analysis, billing systems, or anomaly detection. It allows for constructing a timeline of events, even if those events are ingested into the system out of order. Processing time is when an event is observed and processed by the stream processing system. This is typically the system clock time when the event is processed. Processing time is often simpler to manage and can be sufficient for use cases where exact temporal accuracy is not critical. However, relying solely on processing time can lead to inaccuracies if events are delayed or arrive out-of-order~\cite{Akidau:2015:DMP:2824032.2824076, carbone2015apache}.

\subsubsection{Transactional guarantees}\mbox{} \\
\label{transactional}

Transactional stream processing (TSP) systems have emerged as a solution that combines data stream management with ACID (atomicity, consistency, isolation, durability) guarantees~\cite{zhang2024survey, affetti2020tspoon, botan2012transactional}. Unlike traditional databases, TSP systems initiate transactions through streaming events, which can be triggered individually or in batches. Transactions can modify the system's internal state, including data aggregates, intermediate results, or configurations~\cite{zhang2024survey}.

\paragraph{ACID properties in streaming} \mbox{} \\

{\em Atomicity} means that all operations within a transaction are either successfully processed together or not processed at all, thereby preventing partial updates that could lead to an inconsistent state~\cite{zhang2024survey}. The level of atomicity in TSP can vary depending on the transaction model. For instance, traditional commit protocols like two-phase commit (2PC) ensure atomicity by coordinating commit or abort decisions among distributed participants. Conversely, some models allow the exposure of intermediate uncommitted states and require developers to define compensating actions for each operation, providing a more flexible way to handle atomicity at the expense of strong isolation guarantees~\cite{10.1145/38713.38742}.

{\em Consistency} requires the system to transit from one consistent state to another~\cite{zhang2024survey}. This involves the preservation of integrity constraints, which are rules defining the valid states of the data and events within the stream processing system. In TSP systems, consistency also includes the processing and updating of data following a specified consistency model, like strong or eventual consistency. This is crucial in stream processing, where real-time data interaction is involved~\cite{affetti2017flowdb, zhang2020towards}.

{\em Isolation} ensures that concurrent transactions do not interfere with each other~\cite{zhang2024survey}. In TSP systems, isolation is crucial for maintaining consistent output data, even when transactions are executed concurrently due to simultaneous input events. TSP systems can offer various isolation levels, including serializability, snapshot isolation, or read committed. Some TSP systems provide configurable isolation levels, allowing developers to adjust the isolation guarantees to meet the specific needs of their applications~\cite{affetti2017flowdb}.

{\em Durability} in TSP systems ensures that once a transaction is committed, its changes are permanently stored. Unlike traditional transactional systems, TSP's recovery mechanism may involve replaying input streams to rebuild the state, which might not always result in an identical state due to concurrent processing, asynchronous network channels, and timing differences~\cite{thepaper}. TSP systems must ensure properties like input preservation, state maintenance, and output persistence. Achieving these properties often involves strategies such as replication, logging, or checkpointing, which must balance trade-offs between performance, availability, and the application's specific requirements~\cite{zhang2024survey}.

\paragraph{State management and delivery guarantees} \mbox{} \\

Preserving nearly every ACID property necessitates effective state management and recovery mechanisms in case of failures. Atomicity requires the state to be rolled back to its condition before the transaction started if there is a transaction or system failure. Isolation ensures that the state is separated between transactions, preventing uncommitted changes in one transaction from being visible to others. Durability mandates that the state be persistently stored and recoverable in the event of failures.

Therefore, delivery guarantees are tightly connected to transactional stream processing as well. Exactly-once processing is highly desirable for TSP systems because it guarantees that each event is processed precisely once, regardless of any issues that may arise during processing~\cite{zhang2024survey}. Moreover, TSP systems may encounter unique challenges that necessitate stricter delivery guarantees than the typical exact-once assurance found in many stream processing engines (SPEs). Specifically, TSP systems must replay failed tuples in the exact timestamp sequence of their triggering input events and prevent duplicate message processing. This is crucial because the results depend on the local state of an operator and the time ordering of input streams~\cite{zhang2024survey}. Hence, the property of determinism~\cite{thepaper, Zacheilas:2017:MDS:3093742.3093921, palyvos2022research} can be important for TSP systems.

% \subsubsection{Statistical consistency and anomalies}
% \label{statistical}
% \cite{tellis2018detecting}

\subsection{Fault tolerance mechanisms in stream processing}
\label{phd-related-fault-tolerance}

As discussed in Section~\ref{consistency_overview}, consistency is closely linked with fault tolerance in stream processing. State recovery is essential for achieving delivery guarantees. Completeness and transactional guarantees depend on these delivery guarantees. Consequently, a fundamental mechanism for maintaining consistency in distributed stream processing is recovery after failures, ensuring the necessary level of delivery guarantee: at-most-once, at-least-once, or exactly-once.

In Section~\ref{phd-related-global-state-checkpointing}, we explore two primary mechanisms for global state checkpointing and rollback utilized by major distributed stream processing engines: {\em micro-batching} (or {\em discretized streams}) used in Apache Spark~\cite{Zaharia:2012:DSE:2342763.2342773}, and {\em global asynchronous snapshots} used in Apache Flink~\cite{2015arXiv150608603C, Carbone:2017:SMA:3137765.3137777}. We examine the similarities and differences between these methods, highlighting their respective advantages and limitations.

In Section~\ref{phd-related-critical-path-recovery}, we examine techniques for ensuring fault tolerance without requiring global state recovery. These techniques propose protocols to localize failure and recover only the state of failed nodes, possibly including dependent nodes. We begin with the local recovery method used in MillWheel distributed stream processing~\cite{Akidau:2013:MFS:2536222.2536229}, which involves each operator writing its state and intermediate records to persistent storage. Following this, we discuss two mechanisms for causal recovery: the first, called {\em lineage stash}, extends the micro-batching model~\cite{Wang:2019:LSF:3341301.3359653}, while the second, {\em Clonos}, applies the continuous stream model~\cite{silvestre2021clonos}.

\subsubsection{Global state checkpointing}
\label{phd-related-global-state-checkpointing}

\paragraph{Micro-batching}

\paragraph{Asynchronous snapshots}

\subsubsection{Critical path recovery}
\label{phd-related-critical-path-recovery}

\paragraph{Local recovery}

\paragraph{Lineage stash: causal logging recovery for micro-batching}

\paragraph{Clonos: causal logging recovery for continuous streams}

\subsection{Summary}
\label{review_summary}