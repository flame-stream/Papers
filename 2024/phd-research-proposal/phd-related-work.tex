\subsection{Consistency in stream processing}

\subsubsection{Delivery guarantees}

\paragraph{Types of delivery guarantees}\mbox{} \\

Delivery guarantees, such as at-most-once, at-least-once, and exactly-once, define how and when data is delivered and processed in the system~\cite{fragkoulis2024survey, carbone2018scalable, Akidau:2013:MFS:2536222.2536229}:
\begin{itemize}
    \item At-most-once is a delivery guarantee in system processing where an input element is processed atomically (with all its derivatives) or not processed at all. This means that the system ensures that each input element is processed only one time at maximum, but there's a possibility that some input elements may not get processed.
    \item At-least-once is a type of delivery guarantee where the system ensures that every input element is processed one or more times. This could lead to potential duplication if an input contains duplicated items. This method ensures no data is lost, but it does not prevent possible repetitions.
    \item Exactly-once is a system processing term that guarantees each input element or transaction is processed exactly one time. This means that it ensures there is no data loss and no duplicate processing. It combines both the prevention of input data losses and the avoidance of repeated delivery of results.
\end{itemize}

The type of delivery guarantee chosen for a system directly influences how that system handles failures~\cite{zhang2024survey, silvestre2021clonos, wang2021consistency}. In at-most-once delivery, if a failure occurs during processing, the system may not attempt to reprocess the input, leading to potential data loss. In at-least-once delivery, the system ensures no data is lost by reprocessing inputs in case of failures. However, this can lead to duplicate processing and results if the system doesn't track what has already been processed. In exactly-once delivery, the system ensures that every input is processed exactly one time, even in the case of failures. This is the most complex to implement as it needs to ensure atomicity between reading input data, processing, and delivering results, and it requires the system to be able to recover to a consistent state after a failure~\cite{Carbone:2017:SMA:3137765.3137777}.

\paragraph{State recovery and consistency}\mbox{} \\

State recovery is crucial for consistency because it allows a system to return to a known, correct state after a failure. Without state recovery, a failure can leave the system in an inconsistent state, where the results of some operations are lost or incorrect~\cite{Carbone:2017:SMA:3137765.3137777, Akidau:2013:MFS:2536222.2536229}. State recovery also allows the system to resume processing from a recent checkpoint instead of starting over from the beginning, saving both time and computational resources.

In the context of data processing systems, state recovery is particularly important for handling transient failures. For instance, if a node in a distributed system fails and then restarts, it needs to know what data it has already processed to avoid duplicating work or missing some data. By restoring its state from before the failure, the node can ensure it processes each piece of data exactly once, maintaining the consistency of the system's output~\cite{silvestre2021clonos, Carbone:2017:SMA:3137765.3137777, wang2021consistency}.

The two main ways to recover a consistent state are~\cite{fragkoulis2024survey, zhang2024survey}:

\begin{enumerate}
    \item {\em Global State Snapshotting} involves taking snapshots of the entire system's state at given points in time. The snapshot includes the state of all nodes in the system and any in-transit messages. This approach provides a consistent view of the system, but it may lead to high overhead on recovery because there is a need to re-process all data items since the previous snapshot is taken.
    \item {\em Record-Level Logging} involves saving data elements, state, or causal logs after processing each record or a batch of records. This allows for very fine-grained recovery, as the system can resume processing from the last processed record. However, this approach can introduce significant overhead, as the system needs to save the state frequently.
\end{enumerate}

We will dive into the details of the specific state recovery techniques in Section~\ref{phd-related-fault-tolerance}.

\paragraph{Suitable consistency levels for various types of problems}\mbox{} \\

Distributed stream processing is used in many areas~\cite{fragkoulis2024survey} including network monitoring, IoT, financial processing, machine learning, etc. There are many applications, such as financial systems, where the integrity and correctness of the data are crucial~\cite{zhang2024survey}. Therefore, such applications usually require exactly-once delivery guarantee. In applications like fraud detection, IoT, earthquakes detection, or network security, where each piece of data needs to be analyzed to detect potential anomalies or threats, exactly-once processing is also critical. Missing a data point or processing it multiple times could lead to failure in detecting an anomaly or false alarms~\cite{zhou2019scalable, diro2024anomaly, 10.1093/gji/ggac355, geldenhuys2021dependable}.

On the other hand, the need for exactly-once processing guarantees in machine learning applications depends on the specific requirements of the problem being addressed. While exactly-once processing is critical in some scenarios to ensure data integrity and correctness, it might be less crucial in others~\cite{boden2017distributed, webirte}:
\begin{itemize}
    \item {\em Training Data Preparation}: In the preparation phase of training data, ensuring that each data sample is included exactly once is important for the accuracy of the model. Duplicate data can skew the distribution of the training set, leading to biased models. However, many machine learning algorithms are robust to small inconsistencies, so exactly-once processing might not always be strictly required.
    \item {\em Model Training}: During the training process, especially in distributed systems, exactly-once processing might be less critical depending on the training algorithm. For example, stochastic gradient descent, commonly used in training neural networks, can inherently tolerate some level of noise and inconsistency in data updates, as it is designed to converge despite the random nature of the input data batches.
    \item {\em Real-time Predictions}: For real-time machine learning systems, such as those used in recommendation engines or dynamic pricing models, exactly-once processing can be crucial. Ensuring that each event or piece of data affects the system once prevents the model from making decisions based on duplicated data, which could mislead the prediction outcomes.
    \item {\em Online Learning}: In online learning, where the model continuously updates itself based on incoming data streams, exactly-once processing is beneficial to maintain the correctness of the model updates. Duplicate or missed data points can significantly affect the accuracy and reliability.
\end{itemize}

In summary, whether exactly-once or at-least-once processing is required primarily depends on the tolerance of the specific algorithm to inconsistencies and the impact of data duplication or loss on the application's effectiveness. In cases where data integrity directly influences decision-making processes, exactly-once processing becomes more crucial.

\subsubsection{Completeness of results}

\paragraph{Substreams}
\cite{Tucker:2003:EPS:776752.776780}

\paragraph{Window aggregations}
\cite{verwiebe2023survey}

\paragraph{Handling out-of-order events}
\cite{traub2018scotty, wang2021consistency}

\subsubsection{Statistical consistency and anomalies}
\cite{tellis2018detecting}

\subsection{Fault tolerance mechanisms in stream processing}
\label{phd-related-fault-tolerance}

\subsubsection{Global state checkpointing}

\paragraph{Micro-batching}
\cite{Zaharia:2012:DSE:2342763.2342773}

\paragraph{Asynchronous snapshots}
\cite{Carbone:2017:SMA:3137765.3137777}

\subsubsection{Critical path recovery}

\paragraph{Lineage stash}
\cite{Wang:2019:LSF:3341301.3359653}

\paragraph{Effective determinism}
\cite{Akidau:2013:MFS:2536222.2536229, silvestre2021clonos}
