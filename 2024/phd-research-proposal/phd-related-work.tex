\subsection{Consistency in stream processing}

In this section, we discuss several types of consistency in distributed stream processing. We start with delivery guarantees in Section~\ref{delivery_guarantees}: exactly-once, at-least-once, and at-most-once. Delivery guarantees specify the rules for how a system should behave in case of node failures to maintain a consistent state and prevent data loss or duplication.

Section~\ref{completeness} covers the completeness of results.  In a distributed system, especially in streaming or real-time processing, data can arrive out of order, be delayed, or even temporarily lost. Completeness guarantees that the system's output reflects all the data that should have been processed within a given window or time frame, preventing partial or misleading results.

In Section~\ref{transactional} we discuss transactional stream processing (TSP). TSP integrates the real-time data processing capabilities of stream processing with the reliability and consistency guarantees of traditional transactional systems. TSP systems achieve consistency by incorporating the ACID (Atomicity, Consistency, Isolation, Durability) properties typically found in traditional database transactions.

Eventually, in Section~\ref{statistical} we touch on the topic of statistical consistency in stream processing. Statistical consistency refers to the accuracy and reliability of statistical estimates and summaries derived from continuous data streams. This type of consistency ensures that the statistical measures calculated over the data stream are accurate and close to the expected values. Statistical consistency is close to anomaly detection because anomalies, such as sudden spikes or unusual data patterns, can be detected based on deviations from these consistent statistical measures.

\subsubsection{Delivery guarantees}
\label{delivery_guarantees}

\paragraph{Types of delivery guarantees}\mbox{} \\

Delivery guarantees, such as at-most-once, at-least-once, and exactly-once, define how and when data is delivered and processed in the system~\cite{fragkoulis2024survey, carbone2018scalable, Akidau:2013:MFS:2536222.2536229}:
\begin{itemize}
    \item At-most-once is a delivery guarantee in system processing where an input element is processed atomically (with all its derivatives) or not processed at all. This means that the system ensures that each input element is processed only one time at maximum, but there's a possibility that some input elements may not get processed.
    \item At-least-once is a type of delivery guarantee where the system ensures that every input element is processed one or more times. This could lead to potential duplication if an input contains duplicated items. This method ensures no data is lost, but it does not prevent possible repetitions.
    \item Exactly-once is a system processing term that guarantees each input element or transaction is processed exactly one time. This means that it ensures there is no data loss and no duplicate processing. It combines both the prevention of input data losses and the avoidance of repeated delivery of results.
\end{itemize}

The type of delivery guarantee chosen for a system directly influences how that system handles failures~\cite{zhang2024survey, silvestre2021clonos, wang2021consistency}. In at-most-once delivery, if a failure occurs during processing, the system may not attempt to reprocess the input, leading to potential data loss. In at-least-once delivery, the system ensures no data is lost by reprocessing inputs in case of failures. However, this can lead to duplicate processing and results if the system doesn't track what has already been processed. In exactly-once delivery, the system ensures that every input is processed exactly one time, even in the case of failures. This is the most complex to implement as it needs to ensure atomicity between reading input data, processing, and delivering results, and it requires the system to be able to recover to a consistent state after a failure~\cite{Carbone:2017:SMA:3137765.3137777}.

\paragraph{State recovery and consistency}\mbox{} \\

State recovery is crucial for consistency because it allows a system to return to a known, correct state after a failure. Without state recovery, a failure can leave the system in an inconsistent state, where the results of some operations are lost or incorrect~\cite{Carbone:2017:SMA:3137765.3137777, Akidau:2013:MFS:2536222.2536229}. State recovery also allows the system to resume processing from a recent checkpoint instead of starting over from the beginning, saving both time and computational resources.

In the context of data processing systems, state recovery is particularly important for handling transient failures. For instance, if a node in a distributed system fails and then restarts, it needs to know what data it has already processed to avoid duplicating work or missing some data. By restoring its state from before the failure, the node can ensure it processes each piece of data exactly once, maintaining the consistency of the system's output~\cite{silvestre2021clonos, Carbone:2017:SMA:3137765.3137777, wang2021consistency}.

The two main ways to recover a consistent state are~\cite{fragkoulis2024survey, zhang2024survey}:

\begin{enumerate}
    \item {\em Global State Snapshotting} involves taking snapshots of the entire system's state at given points in time. The snapshot includes the state of all nodes in the system and any in-transit messages. This approach provides a consistent view of the system, but it may lead to high overhead on recovery because there is a need to re-process all data items since the previous snapshot is taken.
    \item {\em Record-Level Logging} involves saving data elements, state, or causal logs after processing each record or a batch of records. This allows for very fine-grained recovery, as the system can resume processing from the last processed record. However, this approach can introduce significant overhead, as the system needs to save the state frequently.
\end{enumerate}

We will dive into the details of the specific state recovery techniques in Section~\ref{phd-related-fault-tolerance}.

\paragraph{Suitable consistency levels for various types of problems}\mbox{} \\

Distributed stream processing is used in many areas~\cite{fragkoulis2024survey} including network monitoring, IoT, financial processing, machine learning, etc. There are many applications, such as financial systems, where the integrity and correctness of the data are crucial~\cite{zhang2024survey}. Therefore, such applications usually require exactly-once delivery guarantee. In applications like fraud detection, IoT, earthquakes detection, or network security, where each piece of data needs to be analyzed to detect potential anomalies or threats, exactly-once processing is also critical. Missing a data point or processing it multiple times could lead to failure in detecting an anomaly or false alarms~\cite{zhou2019scalable, diro2024anomaly, 10.1093/gji/ggac355, geldenhuys2021dependable}.

On the other hand, the need for exactly-once processing guarantees in machine learning applications depends on the specific requirements of the problem being addressed. While exactly-once processing is critical in some scenarios to ensure data integrity and correctness, it might be less crucial in others~\cite{boden2017distributed, webirte}:
\begin{itemize}
    \item {\em Training Data Preparation}: In the preparation phase of training data, ensuring that each data sample is included exactly once is important for the accuracy of the model. Duplicate data can skew the distribution of the training set, leading to biased models. However, many machine learning algorithms are robust to small inconsistencies, so exactly-once processing might not always be strictly required.
    \item {\em Model Training}: During the training process, especially in distributed systems, exactly-once processing might be less critical depending on the training algorithm. For example, stochastic gradient descent, commonly used in training neural networks, can inherently tolerate some level of noise and inconsistency in data updates, as it is designed to converge despite the random nature of the input data batches.
    \item {\em Real-time Predictions}: For real-time machine learning systems, such as those used in recommendation engines or dynamic pricing models, exactly-once processing can be crucial. Ensuring that each event or piece of data affects the system once prevents the model from making decisions based on duplicated data, which could mislead the prediction outcomes.
    \item {\em Online Learning}: In online learning, where the model continuously updates itself based on incoming data streams, exactly-once processing is beneficial to maintain the correctness of the model updates. Duplicate or missed data points can significantly affect the accuracy and reliability.
\end{itemize}

In summary, whether exactly-once or at-least-once processing is required primarily depends on the tolerance of the specific algorithm to inconsistencies and the impact of data duplication or loss on the application effectiveness. In cases where data integrity directly influences decision-making processes, exactly-once processing becomes more crucial.

\subsubsection{Completeness of results}\mbox{} \\
\label{completeness}

In static batch data processing, an operator can read an entire input and produce an output. However, with unbounded streams, there is no defined end to the data, which means that the traditional approach of processing all the data before generating an output is not feasible. Therefore, another important aspect of consistency in distributed stream processing is the {\em completeness} of results~\cite{Tucker:2003:EPS:776752.776780}. Completeness of results in stream processing refers to the extent to which the results generated from processing a data stream fully and accurately reflect all the relevant data for a given query or operation. In other words, it measures whether all the necessary data points have been included in the computation to produce a correct and comprehensive result~\cite{akidau2015streaming}.

\paragraph{Substreams and punctuations framework}\mbox{} \\

Substreams in stream processing refer to smaller, finite segments or partitions of a larger, potentially infinite data stream~\cite{Tucker:2003:EPS:776752.776780}. Substreams are created to manage and process data more efficiently by breaking down the continuous data flow into more manageable chunks. A substream can be expressed as a predicate on stream elements. All elements satisfying the predicate form the substream.

For example, in a stream of web server logs, one could create substreams for each user session, where each substream contains all events (page views, clicks, etc.) that occurred within a specific session. Another example is the system that analyzes customers' activity within hourly windows. It needs to ensure that all orders within that hour are included in the analysis. In this case, the substream is formed by all events from that window.

Substreams allow outputting results before all input is processed. By dividing the continuous data stream into smaller, manageable segments, an SPE can produce intermediate results as each substream is processed. In terms of substreams, the problem of result completeness comes down to ensuring that all substream elements are processed before outputting the corresponding result. 

Solving the problem of detecting that all substream elements have been processed is the prerequisite for ensuring that system results are complete. Such detection is especially complex in distributed environment where elements can be easily reordered due to asynchronous network channels. Hence, there is a need for more complex mechanism than checking that all elements arrived in a strict order~\cite{Li:2008:OPN:1453856.1453890}.

Punctuations framework was firstly introduced in~\cite{Tucker:2003:EPS:776752.776780} and then developed to a distributed setup in~\cite{Akidau:2015:DMP:2824032.2824076} and~\cite{Carbone:2017:SMA:3137765.3137777}. According to this method, special data elements, called punctuations, are injected into the data stream. These punctuations serve as markers that define the boundaries of substreams. They indicate that all elements following punctuation will not satisfy a specified predicate, effectively indicating the end of the corresponding substream.

While the punctuations framework is widely adopted in major SPEs~\cite{carbone2015apache, Noghabi:2017:SSS:3137765.3137770, Kulkarni:2015:THS:2723372.2742788}, it has several pitfalls. It is not applicable for execution graphs with cycles~\cite{carbone2018scalable}. Additionally, the punctuations framework generates extra service traffic that can degrade the latency~\cite{DBLP:journals/pvldb/BegoliACHKKMS21} and throughput~\cite{Li:2008:OPN:1453856.1453890} of an SPE.

\paragraph{Window aggregations}
\cite{verwiebe2023survey}

\paragraph{Handling out-of-order events}
\cite{traub2018scotty, wang2021consistency}

\subsubsection{Transactional guarantees}
\label{transactional}

\subsubsection{Statistical consistency and anomalies}
\label{statistical}
\cite{tellis2018detecting}

\subsection{Fault tolerance mechanisms in stream processing}
\label{phd-related-fault-tolerance}

\subsubsection{Global state checkpointing}

\paragraph{Micro-batching}
\cite{Zaharia:2012:DSE:2342763.2342773}

\paragraph{Asynchronous snapshots}
\cite{Carbone:2017:SMA:3137765.3137777}

\subsubsection{Critical path recovery}

\paragraph{Lineage stash}
\cite{Wang:2019:LSF:3341301.3359653}

\paragraph{Effective determinism}
\cite{Akidau:2013:MFS:2536222.2536229, silvestre2021clonos}
