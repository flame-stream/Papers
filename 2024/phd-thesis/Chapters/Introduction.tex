\chapter{Introduction}

Distributed batch and stream systems address two distinct data processing scenarios. Batch processing deals with pre-collected and stored static datasets, with latency spanning hours or even days~\cite{carbone2015apache, chang2014hawq, sun2023survey}. Conversely, applications requiring short-term personalization, IoT, or network monitoring demand results with high freshness. In these cases, data appears as a continuous, discrete, and potentially infinite stream of items, necessitating the continuous production of freshly updated results~\cite{fragkoulis2024survey, diro2024anomaly}.

This distinction highlights significant challenges in distributed stream processing, particularly the low latency requirement, which brings forth two major issues. Firstly, a stream processing engine (SPE) requires advanced fault tolerance mechanisms to ensure quick recovery and data consistency during failures~\cite{Wang:2019:LSF:3341301.3359653, akidau2015streaming}. Secondly, there is the challenge of continuously producing a results stream during processing. Determining the precise moment when a resulting item is ready for release in a distributed environment can be complex~\cite{Tucker:2003:EPS:776752.776780, DBLP:journals/pvldb/BegoliACHKKMS21}.

While partial solutions have been proposed for these problems, in this thesis, we: 
\begin{itemize}
    \item Design formal models describing the limitations of existing approaches
    \item Search for more efficient techniques based on the obtained theoretical insights
\end{itemize}

\section{Open Challenges Related to Consistency}
\label{thesis-intro-challenges}

We identify two key challenges in achieving consistency in distributed stream processing, which are critical yet difficult to address in both academic and industrial contexts: the lack of formalization and the high overhead of consistency mechanisms. Specifically, we discuss how these challenges arise in the two main types of consistency covered in this thesis: delivery guarantees and substreams consistency.

\subsection{Lack of formalization}

\subsubsection{Delivery guarantees}

Delivery guarantees, such as at-most-once, at-least-once, and exactly-once, define how and when data is delivered and processed in the system~\cite{fragkoulis2024survey, carbone2018scalable, Akidau:2013:MFS:2536222.2536229}. The type of delivery guarantee chosen for a system directly influences how that system handles failures~\cite{zhang2024survey, silvestre2021clonos, wang2021consistency}. In at-most-once, if a failure occurs during processing, the system may not attempt to reprocess the input, leading to potential data loss. In at-least-once, the system ensures no data is lost by reprocessing inputs in case of failures. However, this can lead to duplicate processing and results if the system doesn't track what has already been processed. In exactly-once, the system ensures that every input is processed exactly one time, even in the case of failures.

In state-of-the-art stream processing systems like Flink~\cite{Carbone:2017:SMA:3137765.3137777} and Spark~\cite{Zaharia:2012:DSE:2342763.2342773}, delivery guarantees are primarily defined through each system's internal failure recovery mechanisms. However, a significant limitation of this approach is the variation in features provided by different recovery mechanisms, complicating the assurance of delivery guarantees across systems. For instance, in Flink with declared at-least-once guarantee, if an operation within the execution graph is non-commutative, this can result not only in duplicated outputs but also in outputs that are inconsistent with previously released data, as we will further demonstrate in this thesis. Consequently, one of the critical issues we have identified is the poor formalization of delivery guarantees. A deeper understanding of the expected outputs following a system failure is essential for developing reliable, production-ready systems based on distributed stream processing.

\subsubsection{Substreams consistency}

There are plenty of data processing scenarios where results are most valuable at the time of data arrival, for example, IoT, news processing, financial analysis, fraud detection, and network monitoring. However, regular blocking data processing operators read an entire input before producing an output and can never release a result in the case of a stream. To handle this problem, one can divide the whole unbounded, potentially infinite data stream into bounded, possibly overlapping substreamsËœ\cite{tucker2003exploiting}. In this case, an operator can produce an output when a corresponding substream terminates.

Generating a substream termination event is a challenging task that often depends on the specific properties of practical problems. For instance, a deterministic windowed join operation\footnote{given the identical sequences of input tuples, the identical output tuples will be produced} requires that the order of termination signals mirrors the order of input elements (termination events from data producers)~\cite{najdataei2019stretch, gulisano2016scalejoin}. Another example is an epoch: a substream that an SPE should process atomically. A termination event for an epoch must occur before any elements of the subsequent epoch arrive. These examples demonstrate that it is crucial to formally define the properties of substream management techniques to determine their suitability for specific scenarios.

\subsection{High overhead on consistency mechanisms that leads to high processing latency}

\subsubsection{Delivery guarantees}

One of the most challenging tasks for streaming systems is to design and implement delivery guarantees. 
Streaming systems must release output elements before processing has finished because the input data is assumed to be unbounded. Exactly-once is the strongest and the most valuable guarantee from the user perspective as it ensures that input elements are processed atomically and are not lost. These notions are seemingly simple but shadow the dependency of an output item on the {\em system state} as well as on the input item. 
Streaming systems face the need to recover computations consistently with previous input data, current system state, and already delivered elements.
This requirement makes failure recovery mechanisms somewhat complicated. 

This complication is resolved by most existing stream processing engines. 
Flink ensures the atomicity of state updates and delivery using a protocol based on distributed transactions. 
Google MillWheel~\cite{Akidau:2013:MFS:2536222.2536229} enforces consistency between state and output elements by writing the results of each operation to persistent external storage. 
Micro-batching engines like Storm Trident~\cite{apache:storm:trident} and Spark Streaming~\cite{Zaharia:2012:DSE:2342763.2342773} process data in small-sized blocks. 
Each block is atomically processed at each stage of a data flow, providing properties similar to batch processing. 
The price for exactly-once delivery is a high latency observed in these implementations (e.g., ~\cite{7530084, 7474816}). It is unclear if it is possible to mitigate the latency overhead on exactly-once enforcement.

\subsubsection{Substreams consistency}

A popular method for the generation of substream termination events is the punctuation framework~\cite{tucker2003exploiting} applied in many production-scale SPEs such as Flink~\cite{carbone2015apache}, Heron~\cite{Kulkarni:2015:THS:2723372.2742788}, Samza~\cite{Noghabi:2017:SSS:3137765.3137770}, IBM Streams~\cite{jacques2016consistent}, Apex~\cite{pathak2016introduction}. The main idea behind this framework is to divide the stream by injecting special elements called {\em punctuations} that define substreams ``borders''. An SPE propagates these special elements via the same network channels as data elements. The high network overhead is one of the major limitations of the punctuation framework. Network traffic complexity for this method is $O(K|\Pi|^2)$, where $|\Pi|$ is the number of processes and $K$ is the number of substreams because each process should propagate punctuations to all output channels. The above formula estimates the number of punctuation messages needed in the worst case of fully interconnected processes. 

We argue that the worst case can appear on any execution graph that contains a re-partitioning operator. Indeed, SPEs try to distribute workload evenly between processes~\cite{carbone2015apache, Kulkarni:2015:THS:2723372.2742788, Akidau:2013:MFS:2536222.2536229}, so elements of a substream can be evenly distributed among processes as well. When a process reaches the end of a substream, it must broadcast the punctuation because the items of the part of a sub-stream handled by this process are re-distributed evenly for subsequent processing. Substreams can be {\em fine-grained}: for example, each user session defines a substream. If there are a lot of small substreams, an inefficient substreaming system can degrade the latency~\cite{DBLP:journals/pvldb/BegoliACHKKMS21} and the throughput of an SPE~\cite{Li:2008:OPN:1453856.1453890} or affect the performance of state checkpointing~\cite{zhang2021research}. Therefore, reducing an overhead on substreams management is an important open challenge in distributed stream processing.

\section{Primary Contributions}

We highlight the main results of this work in the following contributions, which address the challenges detailed in Section~\ref{thesis-intro-challenges}.

\subsection{Delivery guarantees formalization}

We introduced a formal framework for modeling consistency properties for any stream processing system~\cite{thepaper}. It was shown that the property of determinism is tightly connected with the concept of exactly-once. We proved that non-deterministic systems must persistently save a state of non-commutative operations before output delivery in order to achieve exactly-once.

We demonstrated that most of the state-of-the-art stream processing systems~\cite{Carbone:2017:SMA:3137765.3137777, Zaharia:2012:DSE:2342763.2342773, Akidau:2013:MFS:2536222.2536229, apache:storm:trident} use one of the following approaches to overcome this problem: 

\begin{itemize}
    \item Inherit exactly-once from batch processing using small-sized batches (micro-batching)
    \item Apply distributed transaction control protocols which guarantee that states are saved before delivery of elements affected by these states
    \item Write results of an operation to external storage on each input element
\end{itemize}

All these methods experience difficulties with working under low-latency requirements (less than a second). In the first case, latency cannot be lower than the batching period, in the second case, the distributed two-phase commit may result in a significant increase of latency, while in the third case latency is bounded below by the duration of external writes.

\subsection{Exactly-once based on lightweight determinism}

Using our formal inference, we designed the \textit{drifting state} technique~\cite{we2018adbis}, a lightweight optimistic approach for ensuring deterministic processing. As demonstrated earlier, deterministic systems can theoretically be more efficient for exactly-once processing in terms of latency, so that we adapted drifting state to achieve exactly-once guarantee~\cite{thepaper}. Our protocols offer several important features:

\begin{itemize}
    \item Elements are processed in a pure streaming fashion, without the need for input buffering.
    \item Business-logic computations, state snapshotting, and the delivery of output items operate asynchronously and independently.
    \item Exactly-once processing semantics are maintained.
\end{itemize}

We implemented the prototype of the proposed technique to examine its performance~\cite{we2018adbis, we2018seim, thepaper}. Our experiments demonstrated that the introduced protocols for fault tolerance are scalable and provide remarkably low overhead within different computational layouts. The comparison with the industrial stream processing solution indicated that our prototype could provide lower latency under exactly-once requirement.

\subsection{Formalization of substreams consistency}

We formalized the problem of substreams management and highlighted its main features~\cite{10.1145/3524860.3539809, trofimov2023bounding}. Particularly, we introduced the notions of {\em soft bound} and {\em firm bound} required by various stream processing scenarios. Some applications that apply substream management systems do not require any particular properties of termination events. In this case, we denote the guarantee provided by such events as {\em soft bound} because termination events indicate that the substream ended some time ago. However, other problems require a {\em firm bound}: guarantee that the substream ends {\em exactly} after the termination event. For example, a commonly used snapshotting protocol~\cite{2015arXiv150608603C, jacques2016consistent} relies on an {\em epoch}. An epoch is a special substream that must be processed atomically. Therefore, the SPE requires the termination event for a given epoch to occur immediately after the last processing event belonging to that epoch. Otherwise, the snapshot can be inconsistent, capturing elements from multiple epochs.

We also provided several insights into the performance of substream management systems. We showed that the network overhead induced by a substream management system cannot be lower than linear in terms of the number of computational nodes. We demonstrated that the state-of-the-art punctuations framework, as applied in many production-scale SPEs~\cite{tucker2003exploiting}, incurs quadratic network overhead with respect to the number of computational nodes, indicating that it is far from the lower bound.

\subsection{Substream management approach reaching lower bound of network traffic overhead}

We designed and implemented a new substreams management technique called \tracker~\cite{10.1145/3524860.3539809, trofimov2023bounding}. Our approach has the following features:

\begin{itemize}
    \item Efficiency due to the lower bound of service traffic overhead
    \item Suitability for problems that require non-linear executions: graph traversing, iterative algorithms, etc.
    \item Suitability for substreams consisting of a few elements
    \item Scalability due to the distribution of extra network traffic from operators between multiple nodes
\end{itemize}

These properties of the \tracker\ framework create a possibility to apply substream management in new applications; this includes smart caching of operator state and latency-conscious windowed joins.

\section{Research Methodology and Novelty}

\section{Published Work}

\section{Dissertation Outline}
