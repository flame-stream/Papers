Distributed batch processing systems, such as Google MapReduce~\cite{Dean:2008:MSD:1327452.1327492}, Apache Hadoop~\cite{hadoop2009hadoop}, and Apache Spark~\cite{Zaharia:2016:ASU:3013530.2934664}, address the need to process large amounts of data (e.g., Internet-scale). The basic idea behind them is to independently process large data blocks (batches) that are collected from static datasets. The main advantages of these systems are the fault tolerance and scalability~\cite{borthakur2011apache} of massively parallel computations on commodity hardware. Batch processing ensures latency in terms of hours or even days~\cite{carbone2015apache, chang2014hawq, sun2023survey}. 

Batch processing is suitable for scenarios where data freshness is not a critical factor, such as in the collection of large datasets, exract-transform-load (ETL) problems, non-real-time data analysis, large-scale scientific computing, and log processing. In recent years, standalone batch processing systems are being replaced by batch processing subsystems within cloud-based data lakes and data warehouses. They may provide better performance due to transformations applied to data within the storage system itself (ELT)~\cite{stonebraker2024goes, akidau2024continuous}. However, these subsystems are still focused on scenarios with no strong requirements on latency. 

Distributed stream processing engines (SPEs) handle analytical, computational, and monitoring scenarios where data freshness is a strong requirement. These scenarios include online analytics (OLAP), short-term personalization, IoT, network monitoring. In these cases, data appear as a continuous, discrete, and potentially infinite stream of items, necessitating the continuous production of freshly updated results~\cite{fragkoulis2024survey, diro2024anomaly}. SPEs can be both standalone, such as Flink~\cite{carbone2015apache}, Spark Streaming~\cite{Zaharia:2012:DSE:2342763.2342773}, or Samza~\cite{Noghabi:2017:SSS:3137765.3137770}, and integrated into data lakes, such as Snowflake stream processing~\cite{akidau2024continuous}.

The low latency requirement raises two major issues related to data consistency. Firstly, an SPE requires advanced fault tolerance mechanisms to ensure quick recovery and data consistency during failures~\cite{Wang:2019:LSF:3341301.3359653, akidau2015streaming}. Secondly, there is the challenge of continuously producing a consistent resulting stream during processing. Determining the precise moment when a data item is ready for release in a distributed environment can be complex~\cite{Tucker:2003:EPS:776752.776780, DBLP:journals/pvldb/BegoliACHKKMS21}.

We acknowledge that, although there have been attempts to address these challenges, there is still room for improvement. In this thesis, we develop formal models that describe the shortcomings and limitations of current approaches. Additionally, we design more efficient techniques based on the theoretical insights gained from our analysis and modeling of existing methods. We demonstrate that the proposed methods are not only theoretically more efficient, but also outperforms state-of-the-art industrial alternatives.

\section{Research Methodology}
\input{Chapters/Introduction/intro-research-methodology}

\section{Novelty}
\input{Chapters/Introduction/intro-novelty}

\section{Open Challenges}
\label{thesis-intro-challenges}
\input{Chapters/Introduction/intro-open-challenges}

\section{Primary Contributions}
\label{thesis-intro-contributions}
\input{Chapters/Introduction/intro-contributions}

\section{Published Work}
\input{Chapters/Introduction/intro-published-work}

\section{Dissertation Outline}
\input{Chapters/Introduction/intro-outline}