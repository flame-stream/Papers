As discussed in Section~\ref{consistency_overview}, consistency is closely linked with fault tolerance in stream processing. State recovery is essential for achieving delivery guarantees. Completeness and transactional guarantees depend on these delivery guarantees. Consequently, a fundamental mechanism for maintaining consistency in distributed stream processing is recovery after failures, ensuring the necessary level of delivery guarantee: at-most-once, at-least-once, or exactly-once.

In Section~\ref{phd-related-global-state-checkpointing}, we explore two primary mechanisms for global state checkpointing and rollback utilized by major distributed stream processing engines: {\em micro-batching} (or {\em discretized streams}) used in Apache Spark~\cite{Zaharia:2012:DSE:2342763.2342773}, and {\em global asynchronous snapshots} used in Apache Flink~\cite{2015arXiv150608603C, Carbone:2017:SMA:3137765.3137777}. We examine the similarities and differences between these methods, highlighting their respective advantages and limitations.

In Section~\ref{phd-related-critical-path-recovery}, we examine techniques for ensuring fault tolerance without requiring global state recovery. These techniques propose protocols to localize failure and recover only the state of failed nodes, possibly including dependent nodes. We begin with the local recovery method used in MillWheel distributed stream processing~\cite{Akidau:2013:MFS:2536222.2536229}, which involves each operator writing its state and intermediate records to persistent storage. Following this, we discuss two mechanisms for causal recovery: the first, called {\em lineage stash}, extends the micro-batching model~\cite{Wang:2019:LSF:3341301.3359653}, while the second, {\em Clonos}, applies the continuous stream model~\cite{silvestre2021clonos}.

We use the notion of a {\em snapshot} or {\em checkpoint} throughout this section. Snapshots capture the state of a system at a particular point in time, including the intermediate state of computational nodes. They provide a consistent state from which the system can recover in case of failure. If a node or the entire system crashes, the system can be restored to the state captured in the snapshot, minimizing data loss and downtime. This ensures that the system can quickly resume normal operations without reprocessing all data from the beginning. Snapshots can be {\em global} or {\em local}. Global snapshots consist of the states of all nodes in a distributed system. In contrast, local snapshots represent the state of a single node only.

\subsection{Global State Checkpointing}
\label{phd-related-global-state-checkpointing}

\subsubsection{Global Synchronous Snapshots: Micro-Batching}

Micro-batching is a stream processing technique that combines elements of both stream and batch processing~\cite{Zaharia:2012:DSE:2342763.2342773, zaharia2010spark}. In this approach, incoming data streams are divided into small, time-bounded batches, which are then processed sequentially. This hybrid method leverages the low-latency benefits of stream processing while simplifying state management and error recovery by treating each micro-batch as a small batch job~\cite{garcia2023micro}. Micro-batching involves a central coordinating mechanism that regularly segments the data stream into batches and assigns a corresponding job (a process graph of short-lived tasks) to each batch~\cite{Zaharia:2012:DSE:2342763.2342773}.

In micro-batching, a global snapshot is periodically saved to persistent storage to prevent recomputation of all input data. To take a snapshot, the central coordinating mechanism initiates a synchronous protocol similar to the two-phase commit protocol: in the first stage, it sends a prepare snapshot request, and when all nodes have saved their state, it sends a commit request~\cite{carbone2018scalable}. This protocol can be initiated only after the current batch is finished. Note that the system cannot output results before the corresponding snapshot is taken because recomputed output data can be inconsistent with already delivered results~\cite{carbone2018scalable, thepaper}. When a failure occurs, each node first recovers its state from the last snapshot. After that, all batches processed after the snapshot was taken are reprocessed from the beginning~\cite{Zaharia:2012:DSE:2342763.2342773}.

This approach is widely adopted in practice and ensures the system achieves exactly-once delivery guarantee. However, it has a significant drawback: it requires a blocking synchronous coordination protocol to take snapshots. The primary issue with this protocol is that most tasks must remain idle until all other tasks have completed their computations and committed their state to persistent storage~\cite{carbone2018scalable, thepaper}. This problem could be mitigated by increasing the interval between snapshots. However, this can lead to high processing latency, as output data cannot be delivered until the corresponding snapshot is taken, as mentioned earlier. Consequently, this approach may be unsuitable for tasks with stringent low-latency requirements~\cite{carbone2018scalable}.

\subsubsection{Global Asynchronous Snapshots}

Global asynchronous snapshots in distributed stream processing are a method used to achieve consistent state recovery and fault tolerance with less impacting the system's performance than synchronous protocols used in micro-batching. In synchronous snapshots, all parts of the system stop processing data to take a consistent snapshot. As we mentioned above, this method can cause significant delays and reduce the system's throughput since processing halts during the snapshot process. In contrast, asynchronous snapshots allow the system to continue processing data while the snapshot is being taken, aiming to minimize disruptions to stream processing and allowing for higher throughput and lower latency~\cite{2015arXiv150608603C, Carbone:2017:SMA:3137765.3137777}.

The asynchronous snapshot process begins with a global coordinator initiating it and informing all nodes to start capturing their state. To do that, the coordinator inserts a special {\em marker} or {\em barrier} into the data stream to demarcate the snapshot boundary~\cite{2015arXiv150608603C, Carbone:2017:SMA:3137765.3137777}. When a node processes the barrier, it knows that all preceding data is already captured in the state. Hence, it is safe to take a local state snapshot. While some nodes process the barrier, others continue to process new incoming data, ensuring the system remains responsive and maintains high throughput. The local state snapshots are saved to reliable storage, which is often distributed and fault-tolerant, to ensure the snapshots themselves are not lost in case of failure. 

In the event of a failure, the system can be restored to the last consistent snapshot, with nodes reloading their states from the snapshot and resuming processing from the point where the barrier was inserted~\cite{2015arXiv150608603C, Carbone:2017:SMA:3137765.3137777}. To achieve exactly-once guarantee, the system may not output data before the corresponding snapshot is taken~\cite{silvestre2021clonos, thepaper}. Otherwise, there is no guarantee that the recomputed output will be consistent with the already released output elements. For at-least-once guarantee, this requirement can be relaxed if the computational graph does not contain non-idempotent operations~\cite{thepaper}.

The benefits of asynchronous snapshots include less disruption to the system, resulting in lower latency compared to the micro-batching approach. This method is also suitable for large-scale distributed systems where stopping all nodes simultaneously for a snapshot would be impractical~\cite{carbone2018scalable}. Global asynchronous snapshots are widely adopted in popular SPEs such as Flink~\cite{carbone2015apache}, Samza~\cite{Noghabi:2017:SSS:3137765.3137770}, and Storm Trident~\cite{apache:storm:trident}. However, sending barriers through network channels used for processing can limit the system's throughput, especially in the case of frequent snapshots~\cite{DBLP:journals/pvldb/BegoliACHKKMS21, trofimov2023bounding}. On the other hand, as mentioned above, to achieve exactly-once guarantee, the system can only output data after the corresponding snapshot has been taken. Therefore, it is important to carefully select the period of taking snapshots~\cite{thepaper}.

\subsection{Critical Path Recovery}
\label{phd-related-critical-path-recovery}

\subsubsection{Causal Logging Recovery for Micro-Batching: Lineage Stash}

The {\em Lineage Stash} mechanism presented in~\cite{Wang:2019:LSF:3341301.3359653} aims to make the recovery mechanism for micro-batching more efficient. The main idea of the lineage stash is to record information during process execution to minimize the amount of work that needs to be redone in case of a failure. Instead of completely recomputing all elements since the previous global snapshot, the lineage stash allows the system to recompute only the necessary data, thereby reducing the overhead on recovery. This approach uses causal logging for recording and replaying computations~\cite{elnozahy1994manetho, alvisi1998message}. Each process keeps a buffer of all nondeterministic events that have affected its current state, and these volatile records are attached to messages sent to other processes. If a process fails, it recovers by retrieving logs from the other processes. This technique ensures global consistency by allowing all nondeterministic events from the initial execution to be replayed exactly as they occurred.

The key challenge of the lineage stash mechanism is to identify and efficiently capture the sources of nondeterminism that occur in data processing applications. The key optimization here is leveraging the deterministic nature of data processing outputs. Instead of recording raw data, lineage stash records the lineage, which includes a pointer to the application data (an object) and a description of the computation (a task). Each task uses the process's local state and one or more objects as inputs and can produce new objects and tasks. The lineage of an object consists of the task that created it and the lineage of the task's inputs. Because object values are deterministic, it is possible to cache multiple immutable copies across multiple nodes. If all copies are lost, objects can be recomputed during recovery. For smaller objects, the data can be directly included in the task specification~\cite{Wang:2019:LSF:3341301.3359653}.

One limitation of the lineage stash is its lack of support for certain important nondeterministic functions in stream processing. For instance, it does not support timer-based services used for processing time windows, nor does it handle punctuations needed for progress tracking and managing out-of-order events. Additionally, the use of a micro-batching architecture further limits the applicability of lineage stash~\cite{silvestre2021clonos} due to higher latency of processing in comparison with continuous stream processing systems.

\subsubsection{Local Recovery: MillWheel}

MillWheel is a distributed stream processing system and model presented in~\cite{Akidau:2013:MFS:2536222.2536229}. Its approach to state snapshotting and recovery is significantly different from both micro-batching (synchronous global snapshots) and asynchronous global snapshots. Instead of taking global snapshots and recomputing parts of the input stream after a failure, each computational node in MillWheel persistently stores all input, corresponding output elements, and state changes. As a result, in case of a failure, each node can lose the processing results of at most a single element. This design allows each node to recover locally without the need for a global coordinator. This mechanism is called {\em strong productions}.

Diving into more details, the strong productions mechanism works as follows. When a computational node receives an input element, it first checks for duplicates using deduplication data from previous deliveries and discards any duplicates. The node then processes the input element, producing pending changes: output elements and a new state. Next, the node saves the input element along with the pending changes in a single transaction. Finally, the sender computational node is notified that the input element has been processed, and the output elements are sent downstream. Upon recovery, each node re-sends all output elements for which it has not received acknowledgments~\cite{Akidau:2013:MFS:2536222.2536229}.

MillWheel ensures an exactly-once guarantee by making all operations idempotent and filtering duplicates. It requires all streaming operations to be pure functions, meaning they must operate without any side-effects. This behavior is referred to as effective determinism \cite{akidau2018streaming} because computations remain deterministic until the persistent storage is cleared. The strong production method ensures that, even though many possible results of a non-commutative operation can occur due to race conditions, only one result is computed and saved. The cost of this exactly-once enforcement is an overhead on external writes for each transformation. Implementing this approach requires high-performance distributed storage that supports blind writes, such as BigTable \cite{chang2008bigtable}. The lack of efficient storage for blind writes may lead to high latency overhead on processing.

\subsubsection{Causal Logging Recovery for Continuous Streams: Clonos}

Clonos~\cite{silvestre2021clonos} is a framework for optimizing recovery time in the continuous stream model. It addresses two main issues of the MillWheel approach. Firstly, it tracks and logs many sources of non-determinism, allowing streaming operations to be impure functions in some cases. Secondly, it does not require high-performance distributed storage that supports blind writes, such as BigTable \cite{chang2008bigtable}.

The first limitation is overcome by using causal logging to handle the following sources of non-determinism: windowing and time-sensitive computations, event-time windows, timers, user-defined functions, external calls, random number generation, and the order of data element arrival. Processing becomes effectively deterministic by saving the exact states of these sources, making them idempotent. To simplify causal logging and recovery for users writing user-defined functions, operators have access to {\em causal services} that abstract away the complexity. For example, when a user-defined code calls the Timestamp service, it returns timestamps. Under normal operation, the service generates a nondeterministic timestamp and appends it to the causal log. During recovery, when the code requests a timestamp from the Timestamp service, the service will return a timestamp read from the causal log~\cite{silvestre2021clonos}.

The second limitation is addressed by using asynchronous global state snapshots. These snapshots allow the system to save only the causal log between snapshots, which significantly reduces its size and storage requirements. As previously mentioned, systems that utilize global asynchronous snapshots typically can only output data after the snapshot has been completed to ensure an exactly-once guarantee. However, Clonos is not subject to this limitation because it processes data deterministically. This means that if there is a failure, Clonos can recompute the same output elements, which can then be easily deduplicated. Nevertheless, there are some limitations with the Clonos approach. One issue is that it requires blocking of streaming operators while snapshots are being taken that may affect the latency of processing. Another limitation is that it does not take advantage of the fact that streaming computations are often naturally deterministic~\cite{Wang:2019:LSF:3341301.3359653, silvestre2021clonos} and may have unnecessary latency overhead on causal logs saving.

% \subsection{Summary}
% \label{review_summary}