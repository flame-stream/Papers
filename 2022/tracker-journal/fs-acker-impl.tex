\label {fs-acker-impl}

In the previous section, we introduced a general schema of the \tracker\ framework. In this section, we deepen into its implementation details. We describe and explore the properties of the tracking agent that produces the substream termination notifications (NEOSS). After that, the technique to achieve consistent termination events order is detailed. Distributed implementation of the tracking agent concludes this part.

\subsection{Bound guarantees}

The tracking agent splits the workflow graph into partially ordered segments and tracks them separately. For each process $p$, we can generate a list of preceding segments, including a set of incoming message generators $W_p$. As soon as all these segments contain no elements of a substream, the agent sends to a process {\em NEOSS}.

To track the message path through segments, the agent receives SND/RCV reports containing a segment identifier and a list of predicates the message satisfies. The agent aggregates this information into the table illustrated in Table~\ref{tracker-table-simple}.

\begin{table}[t]
\caption{\tracker\ table: a general example}
  \label{tracker-table-simple}
  \centering
  \footnotesize
  \begin{tabular}{|c|c|c|>{\bfseries}c|} 
    \hline
    Notified & Predicate & Segment & Substream elements  \\ \hline \hline
    \multirow{2}{*}{\checkmark} & \multirow{2}{*}{h(x)} & A & No \\ \cline{3-4}
    & & B & No \\ \hline
    \multirow{2}{*}{} & \multirow{2}{*}{q(x)} & A & No \\ \cline{3-4}
    & & B & Yes \\ \hline
    \multirow{2}{*}{\checkmark} & \multirow{2}{*}{z(x)} & A & No \\ \cline{3-4}
    & & B & No \\ \hline
  \end{tabular}
\end{table}

A graph shown in Figure~\ref{fig:tracker-acker-comparison} illustrates the notion of segments. This graph has two segments: $A$ and $B$. According to Table~\ref{tracker-table-simple}, the NEOSS for predicate $q(x)$ can be sent for a segment $A$, but not for a segment $B$. This behavior is similar to punctuations: NEOSS can be spawned earlier for the upstream processes.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.3\textwidth]{pics/segments-example.pdf}
  \caption{Graph segmentation example}
  \label{fig:tracker-acker-comparison}
\end{figure}

Several possible methods exist to build the indicator that the segment contains elements from a substream using the reports from processes. Our implementation uses the trick applied in Apache Storm to monitor the completeness of processing~\cite{apache:storm:acker}. 

Each report is labeled by a random number $X$, the same for the send and the corresponding receive actions. This trick makes it easy to check if the segment contains a complete set of $SND$/$RCV$ pairs for a message: XOR operation for all numbers received from the chain will turn into 0. The result of the XOR operation can accidentally become zero. However, the probability of this event is controlled by the random number $X$ length so that one can neglect it in practice~\cite{apache:storm:acker}.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.3\textwidth]{pics/tracker-segments-example.pdf}
  \caption{Reports example}
  \label{fig:tracker-reports}
\end{figure}

Figure~\ref{fig:tracker-reports} illustrates the reports with random numbers. The elements satisfying a predicate $h(x)$ flow through a dataflow. The first process generates an output and sends the SND report with $X=5$. The corresponding RCV report by the second process also has $X=5$ because this process receives the element from the first one. The second process sends a new element that satisfies $h(x)$ to the following process and produces the new SND report with $X=12$. The third operator receives this element and produces an RCV report with $X=12$. If there are no more elements such that $h(x)=1$, then we can send NEOSS for the substream defined by $h(x)$ ends because {\em 5 XOR 5 XOR 12 XOR 12 = 0}. Table~\ref{tracker-table-xor} illustrates the actual \tracker\ table for the mentioned technique.

\begin{table}[t]
\caption{\tracker\ table: XORing technique example}
  \label{tracker-table-xor}
  \centering
  \footnotesize
  \begin{tabular}{|c|c|c|>{\bfseries}c|>{\bfseries}c|} 
    \hline
    Notified & Predicate & Segment & Segm. XOR & XOR  \\ \hline \hline
    \multirow{2}{*}{\checkmark} & \multirow{2}{*}{h(x)} & A & 000 & \multirow{2}{*}{000} \\ \cline{3-4}
    & & B & 000 & \\ \hline
    \multirow{2}{*}{} & \multirow{2}{*}{q(x)} & A & 000 & \multirow{2}{*}{110} \\ \cline{4-4}
    & & B & 110 & \\ \hline
    \multirow{2}{*}{\checkmark} & \multirow{2}{*}{z(x)} & A & 000 & \multirow{2}{*}{000} \\ \cline{3-4}
    & & B & 000 & \\ \hline
  \end{tabular}
\end{table}

\subsection{Implementation properties}

In the \tracker\ framework, there is no need to inject service items into a dataflow. Thus, \tracker\ can handle cyclic execution graphs.

\begin{lemma}
If an execution graph is cyclic, the \tracker\ framework can determine substream termination.
\end{lemma}
\begin{proof}
Suppose an execution graph contains a cycle and no in-flight elements in the system satisfying a predicate $pred$, but \tracker\ did not provide NEOSS for the substream. It means that the corresponding XOR value is not 0. Therefore, there is a process that sends an element satisfying $pred$ (possibly through the cycle), but the following process has not received and processed it yet. We obtain a contradiction because we supposed that the execution graph does not contain an in-flight element satisfying the predicate $pred$.
\end{proof}

Another property that became reachable due to avoid of injecting service items into a dataflow is low network traffic overhead. All traffic goes between the tracking agent and processes without broadcasting service items among processes.

Due to the associativity of XOR, we can optimize tracking agent incoming traffic by aggregating reports locally within the processes. For each process, we introduce a {\em local tracking agent} component. It serves as a mediator between the process and the global agent, buffering the outgoing reports and flushing them periodically. 

\begin{lemma}
Network traffic complexity for the \tracker\ framework is $O(K|\Pi|)$, where $|\Pi|$ is the number of processes and $K$ is the number of substreams.
\end{lemma}
\begin{proof}
Substreams last a finite time period by definition so that each process sends aggregated reports a constant number of times that does not depend on the number of substreams and processes. Therefore, the amount of extra network traffic for the reports is $O(|\Pi|)$, so the total estimation with the overhead on the notifications is $O(K|\Pi|)$ because \tracker\ must inform all processes about substream termination (send NEOSS).
\end{proof}

The flushing window is the parameter that allows us to balance the service traffic and latency between the actual substream termination (the event from the data producer) and the termination event. Although this optimization does not reduce the theoretical estimation of network traffic complexity, it can significantly reduce substream termination latency, as demonstrated in Section~\ref{fs-experiments}.

\subsection{Consistent termination events order}
\label{termination_order_impl}

If the order of NEOSS is consistent, the order of termination events will also be consistent. To achieve consistent order of NEOSS, we need to define $t(M)$ such that the order on $t(M)$ respects the order of input elements. All reports that processes send to the tracking agent should be labeled with $t(M)$. The tracking agent sends the NEOSS elements according to the order on $t(<)$.

Table~\ref{tracker-table-oder} illustrates the \tracker\ table in case of consistent NEOSS order. Column {\em min t(x)} indicates the minimal $t(x)$ among the elements that satisfy the corresponding predicate. The tracking agent sends notifications for the substream if the $XOR$ value is 0 and all substreams that contain elements with less {\em min t(x)} have finished (notifications have been produced). Therefore, NEOSS for the substream defined by the predicate $h(x)$ is not sent until the NEOSS for the predicate $q(x)$ is generated. 

\begin{table}[t]
\caption{\tracker\ table: consistent NEOSS order example}
  \label{tracker-table-oder}
  \centering
  \footnotesize
  \begin{tabular}{|c|c|>{\bfseries}c|c|} 
    \hline
    Notified & Predicate & min t(x) &  XOR  \\ \hline \hline
    \multirow{2}{*}{waits for q(x) finish} & \multirow{2}{*}{h(x)} & \multirow{2}{*}{5} & \multirow{2}{*}{000} \\
    & & & \\ \hline
    \multirow{2}{*}{} & \multirow{2}{*}{q(x)} & \multirow{2}{*}{4} & \multirow{2}{*}{110} \\
    & & & \\ \hline
    \multirow{2}{*}{\checkmark} & \multirow{2}{*}{z(x)} & \multirow{2}{*}{1} & \multirow{2}{*}{000} \\
    & & & \\ \hline
  \end{tabular}
\end{table}

If input elements arrive through a single node, $t(x)$ can denote the monotonic system time of the element $x$ arrival. If there are multiple source processes, one can use time oracle agent~\cite{10.14778/3055330.3055335} as a service for the generation of a monotonic sequence of unique timestamps. However, in this case, there is a need to manage one more subsystem.

A simple technique to build $t(x)$ without extra agents is based on systematic synchronization of the system clocks. We call this method and associated labels {\em coarse time}. Assume that clock differences are no more than some fixed $\delta$, which we reference as synchronization slack. Let $\tau(x)$ be a precise physical time of input data item $x$ arrival, and $s(x)$ be the local system time of the source node where $x$ arrived. The valid order of events $\tau(d_1) > \tau(d_2)$ coming from different sources can sometimes be restored by their system timestamps $s(d_1)$ and $s(d_2)$. If these timestamps differ more than time synchronization slack, then the order is clear: $s(d_1) > s(d_2) + \delta \Rightarrow \tau(d_1) > \tau(d_2)$.

This fact allows us to define $t(x)$ such that $t(x) = [s(x) / \delta]$. This way, we make $t(x)$ less precise, but this trick allows us to compare global time assigned by different source nodes. If $t(x_1)$ is greater than the $(t(x_2) + 1)$, then their order is defined even if they arrived from different source nodes:  $t(x_1) > t(x_2) + 1 \Rightarrow \tau(x_1) > \tau(x_2)$. Therefore, the order on $t(x)$ coincides with the order of input elements, so it is suitable for the defined problem. Here is a summary of the mechanism that ensures consistent termination events order:
\begin{enumerate}
    \item On input item arrival, the source node gets the system timestamp
    \item The system timestamp is shrunk up to synchronization slack (practically, we achieve 10ms slack)
    \item Each report for the tracking agent is labeled by the result of $t(x)$
    \item Tracking agent sends NEOSS according to the order on $t(x)$
    \item Termination events are generated according to the order of NEOSS arriving
\end{enumerate}

\subsection{Distributed tracking agent}

The tracking agent accumulates all the service traffic from the entire system. To ensure scalability, we introduce a distributed version of the tracking agent.

A basic approach is to partition predicates between the shards of the tracking agent. One shard can handle all reports for the predicate $h(x)$, while the second all reports for the predicate $q(x)$. The network traffic complexity remains the same $O(K|\Pi|)$ because each process sends each report to only a single shard, depending on the predicate.

The main problem regarding this approach is enforcing the consistent NEOSS order. A centralized agent sends NEOSS through the FIFO network channels, so the NEOSS elements for the same processes cannot be reordered. There can be a race between NEOSS from various shards due to asynchronous network channels in the distributed case.

A simple solution for this issue is for each NEOSS from a shard to wait for NEOSS from all other shards but with greater $t(x)$. For example, if a process receives NEOSS for some predicate with $\min t(x) = 3$, then it needs to wait until receiving NEOSS with $t(x) > 3$ from all other shards to produce the termination event. However, this method can increase the latency between the substream end and the termination event.

We employ the vector clock algorithm to mitigate latency overhead. Each process either periodically sends its system time to all tracking agent shards. Tracking agents also periodically send the minimal vector among the in-flight elements. Therefore, if a process receives NEOSS for some predicate with $\min t(x) = 3, vec=[0,4,1]$, then it needs to wait until receiving vectors component-wise greater than $[0,4,1]$ from all shards to produce the termination event.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.3\textwidth]{pics/distributed-tracker.pdf}
  \caption{Example of a NEOSS and a vector report}
  \label{fig:distributed-tracker}
\end{figure}

In many cases, a process should not wait for NEOSS from other tracking agent shards because it can be confident that other shards do not have stale NEOSS due to periodic observing of their vectors. Figure~\ref{fig:distributed-tracker} illustrates the example of reports from the tracking agents. The NEOSS from the first tracking agent can be accepted if the second tracking agent has already sent a vector report that is component-wise greater than the vector from the NEOSS.

The vector clock introduction increases the service traffic but allows us to eliminate the bottleneck from the system. The service traffic complexity remains unchanged, but the $O$ factor increases. In the experimental section, we will study how significant this increase is.

Despite the fact that distributed tracking agent makes the whole system more scalable, it may affect the performance as we show further. We also demonstrate in the experimental section that it is hard to overload the centralized tracking agent. Therefore, we recommend using the centralized agent unless the cluster configuration is enormous (more than 100 nodes).
